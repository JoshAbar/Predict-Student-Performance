{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7615d1",
   "metadata": {},
   "source": [
    "### **BÁO CÁO ĐỒ ÁN CUỐI KỲ**\n",
    "# DỮ LIỆU LỚN - IS405.O21.HTCL\n",
    "# ***ĐỀ TÀI: DỰ ĐOÁN NĂNG LỰC HỌC TẬP VÀ ĐIỂM THI CUỐI NĂM CỦA HỌC SINH***\n",
    "# Danh sách thành viên:\n",
    "1. Đỗ Huỳnh Mỹ Tâm - 20520746\n",
    "2. Đinh Thị Tú Uyên - 20522139\n",
    "3. Nguyễn Công Thành - 20521918\n",
    "4. Nguyễn Phạm Thanh Phong - 21522458\n",
    "\n",
    "\n",
    "### Data Source: https://www.kaggle.com/datasets/dillonmyrick/high-school-student-performance-and-demographics?resource=download\n",
    "\n",
    "###  Thuật toán k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7698a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d295aa2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.AbstractConnector - Stopped Spark@70ba3c8b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "[Thread-3] INFO org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.24:4040\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared\n",
      "[Thread-3] INFO org.apache.spark.storage.BlockManager - BlockManager stopped\n",
      "[Thread-3] INFO org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Successfully stopped SparkContext\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Running Spark version 3.5.1\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - OS info Linux, 6.5.0-35-generic, amd64\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Java version 11.0.22\n",
      "[Thread-3] INFO org.apache.spark.resource.ResourceUtils - ==============================================================\n",
      "[Thread-3] INFO org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.\n",
      "[Thread-3] INFO org.apache.spark.resource.ResourceUtils - ==============================================================\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Submitted application: PROJECT\n",
      "[Thread-3] INFO org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "[Thread-3] INFO org.apache.spark.resource.ResourceProfile - Limiting resource is cpu\n",
      "[Thread-3] INFO org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0\n",
      "[Thread-3] INFO org.apache.spark.SecurityManager - Changing view acls to: mtam\n",
      "[Thread-3] INFO org.apache.spark.SecurityManager - Changing modify acls to: mtam\n",
      "[Thread-3] INFO org.apache.spark.SecurityManager - Changing view acls groups to: \n",
      "[Thread-3] INFO org.apache.spark.SecurityManager - Changing modify acls groups to: \n",
      "[Thread-3] INFO org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: mtam; groups with view permissions: EMPTY; users with modify permissions: mtam; groups with modify permissions: EMPTY\n",
      "[Thread-3] INFO org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34337.\n",
      "[Thread-3] INFO org.apache.spark.SparkEnv - Registering MapOutputTracker\n",
      "[Thread-3] INFO org.apache.spark.SparkEnv - Registering BlockManagerMaster\n",
      "[Thread-3] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "[Thread-3] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up\n",
      "[Thread-3] INFO org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat\n",
      "[Thread-3] INFO org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3a5a7e57-f2a7-49b4-a435-cf80bee230c9\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 434.4 MiB\n",
      "[Thread-3] INFO org.apache.spark.SparkEnv - Registering OutputCommitCoordinator\n",
      "[Thread-3] INFO org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 11.0.22+7-post-Ubuntu-0ubuntu222.04.1\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.Server - Started @3166ms\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@1523fbbb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "[Thread-3] INFO org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14b99c5c{/,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.24\n",
      "[Thread-3] INFO org.apache.spark.executor.Executor - OS info Linux, 6.5.0-35-generic, amd64\n",
      "[Thread-3] INFO org.apache.spark.executor.Executor - Java version 11.0.22\n",
      "[Thread-3] INFO org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "[Thread-3] INFO org.apache.spark.executor.Executor - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@605646a0 for default.\n",
      "[Thread-3] INFO org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39111.\n",
      "[Thread-3] INFO org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.24:39111\n",
      "[Thread-3] INFO org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "[Thread-3] INFO org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.24, 39111, None)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.24:39111 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.1.24, 39111, None)\n",
      "[Thread-3] INFO org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.24, 39111, None)\n",
      "[Thread-3] INFO org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.24, 39111, None)\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@14b99c5c{/,null,STOPPED,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f6bafee{/jobs,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38f0c4e4{/jobs/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57a236d1{/jobs/job,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@417f1645{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55f1cbc0{/stages,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2208f0cf{/stages/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71491322{/stages/stage,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fa3a816{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17a5cf76{/stages/pool,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a4b6260{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d62a347{/storage,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@200e7b02{/storage/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24819ce9{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c91b9bd{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@539cd921{/environment,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d438924{/environment/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70bab5e5{/executors,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9ddc061{/executors/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38ee04ee{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@549db6ba{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75d2d6d3{/executors/heapHistogram,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a429326{/executors/heapHistogram/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fd76d6e{/static,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c60848e{/,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7615a9d9{/api,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2963fd50{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@354b8bb6{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65dc243d{/metrics/json,null,AVAILABLE,@Spark}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Tạo Spark session và cấu hình log4j\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PROJECT\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "     #.master('spark://192.168.100.119:7077') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedc3c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "[Thread-3] INFO org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/mtam/Spark/spark-3.5.1-bin-hadoop3/spark-warehouse'.\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c119b8b{/SQL,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ffd78f2{/SQL/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@caf951d{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b05a77c{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "[Thread-3] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cb898f5{/static/sql,null,AVAILABLE,@Spark}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PROJECT</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x777f50bf2ad0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859b595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae7d1d",
   "metadata": {},
   "source": [
    "# Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ab06dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtam/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "# Pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from operator import add, getitem\n",
    "from collections import Counter\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a382fbe",
   "metadata": {},
   "source": [
    "# Đọc dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c733a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 49 ms to list leaf files for 1 paths.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 246.210746 ms\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.24:39111 (size: 34.5 KiB, free: 434.4 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Created broadcast 0 from load at NativeMethodAccessorImpl.java:0\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.24:39111 (size: 6.4 KiB, free: 434.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8261 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)\n",
      "[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.654753 ms\n",
      "[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_math_clean.csv, range: 0-77747, partition values: [empty row]\n",
      "[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.456391 ms\n",
      "[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1960 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 512 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 0.661 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 0.724862 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.975375 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.24:39111 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Created broadcast 2 from load at NativeMethodAccessorImpl.java:0\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.1.24:39111 in memory (size: 6.4 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 1 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 28.2 KiB, free 433.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 433.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.1.24:39111 (size: 13.0 KiB, free: 434.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8261 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 67.811524 ms\n",
      "[Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_math_clean.csv, range: 0-77747, partition values: [empty row]\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.1.24:39111 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1817 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 401 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 0.544571 s\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "[Thread-3] WARN org.apache.spark.sql.catalyst.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[student_id: int, school: string, sex: string, age: int, address_type: string, family_size: string, parent_status: string, mother_education: string, father_education: string, mother_job: string, father_job: string, school_choice_reason: string, guardian: string, travel_time: string, study_time: string, class_failures: int, school_support: string, family_support: string, extra_paid_classes: string, activities: string, nursery_school: string, higher_ed: string, internet_access: string, romantic_relationship: string, family_relationship: int, free_time: int, social: int, weekday_alcohol: int, weekend_alcohol: int, health: int, absences: int, grade_1: int, grade_2: int, final_grade: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_math = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"na_values\", [\"\", \" \", \"N/A\", \"NA\"])\\\n",
    "    .load(\"Data_project/student_math_clean.csv\")\n",
    "\n",
    "df_math.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c74286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 1 ms to list leaf files for 1 paths.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#255, None)) > 0)\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 200.1 KiB, free 433.9 MiB)\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.1.24:39111 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Created broadcast 4 from load at NativeMethodAccessorImpl.java:0\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 2 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (load at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[13] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 13.5 KiB, free 433.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.1.24:39111 (size: 6.4 KiB, free: 434.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8267 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)\n",
      "[Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_portuguese_clean.csv, range: 0-128248, partition values: [empty row]\n",
      "[Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1917 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 39 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (load at NativeMethodAccessorImpl.java:0) finished in 0.054 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 2 finished: load at NativeMethodAccessorImpl.java:0, took 0.068360 s\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 200.1 KiB, free 433.7 MiB)\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.1.24:39111 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Created broadcast 6 from load at NativeMethodAccessorImpl.java:0\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 3 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (load at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[19] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 28.2 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.1.24:39111 (size: 13.0 KiB, free: 434.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8267 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)\n",
      "[Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_portuguese_clean.csv, range: 0-128248, partition values: [empty row]\n",
      "[Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 1731 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 103 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (load at NativeMethodAccessorImpl.java:0) finished in 0.115 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 3 finished: load at NativeMethodAccessorImpl.java:0, took 0.121290 s\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "[Thread-3] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[student_id: int, school: string, sex: string, age: int, address_type: string, family_size: string, parent_status: string, mother_education: string, father_education: string, mother_job: string, father_job: string, school_choice_reason: string, guardian: string, travel_time: string, study_time: string, class_failures: int, school_support: string, family_support: string, extra_paid_classes: string, activities: string, nursery_school: string, higher_ed: string, internet_access: string, romantic_relationship: string, family_relationship: int, free_time: int, social: int, weekday_alcohol: int, weekend_alcohol: int, health: int, absences: int, grade_1: int, grade_2: int, final_grade: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_portuguese = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"na_values\", [\"\", \" \", \"N/A\", \"NA\"])\\\n",
    "    .load(\"Data_project/student_portuguese_clean.csv\")\n",
    "\n",
    "df_portuguese.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6cad2f",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09536500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 200.0 KiB, free 433.4 MiB)\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.1.24:39111 (size: 34.4 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Created broadcast 8 from count at NativeMethodAccessorImpl.java:0\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (FileScan csv [student_id#272,school#273,sex#274,age#275,address_type#276,family_size#277,parent_status#278,mother_education#279,father_education#280,mother_job#281,father_job#282,school_choice_reason#283,guardian#284,travel_time#285,study_time#286,class_failures#287,school_support#288,family_support#289,extra_paid_classes#290,activities#291,nursery_school#292,higher_ed#293,internet_access#294,romantic_relationship#295,... 10 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_por..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<student_id:int,school:string,sex:string,age:int,address_type:string,family_size:string,par...\n",
      " MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 19.7 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.1.24:39111 (size: 9.1 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.1.24:39111 in memory (size: 13.0 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (FileScan csv [student_id#272,school#273,sex#274,age#275,address_type#276,family_size#277,parent_status#278,mother_education#279,father_education#280,mother_job#281,father_job#282,school_choice_reason#283,guardian#284,travel_time#285,study_time#286,class_failures#287,school_support#288,family_support#289,extra_paid_classes#290,activities#291,nursery_school#292,higher_ed#293,internet_access#294,romantic_relationship#295,... 10 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_por..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<student_id:int,school:string,sex:string,age:int,address_type:string,family_size:string,par...\n",
      " MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8267 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.1.24:39111 in memory (size: 13.0 KiB, free: 434.3 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_portuguese_clean.csv, range: 0-128248, partition values: [empty row]\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.1.24:39111 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.1.24:39111 in memory (size: 6.4 KiB, free: 434.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.1.24:39111 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 130.799771 ms\n",
      "[Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO org.apache.spark.storage.memory.MemoryStore - Block rdd_23_0 stored as values in memory (estimated size 39.1 KiB, free 433.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_23_0 in memory on 192.168.1.24:39111 (size: 39.1 KiB, free: 434.3 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 4.0 (TID 4)\n",
      "[rdd_23_0]\n",
      "[Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 1330 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 510 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.545 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 44.693154 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 28 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[28] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 27.8 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.1.24:39111 (size: 12.4 KiB, free: 434.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[28] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8256 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)\n",
      "[Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.327538 ms\n",
      "[Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 44.984877 ms\n",
      "[Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.836381 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 2156 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 191 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.235 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.072236 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[31] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 12.5 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 6) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 6)\n",
      "[Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 14 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số dòng: 649\n",
      "Số cột: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 32.467535 ms\n",
      "[Executor task launch worker for task 0.0 in stage 7.0 (TID 6)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 6). 3995 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 6) in 97 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.120 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 6 finished: count at NativeMethodAccessorImpl.java:0, took 0.134602 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[student_id: int, school: string, sex: string, age: int, address_type: string, family_size: string, parent_status: string, mother_education: string, father_education: string, mother_job: string, father_job: string, school_choice_reason: string, guardian: string, travel_time: string, study_time: string, class_failures: int, school_support: string, family_support: string, extra_paid_classes: string, activities: string, nursery_school: string, higher_ed: string, internet_access: string, romantic_relationship: string, family_relationship: int, free_time: int, social: int, weekday_alcohol: int, weekend_alcohol: int, health: int, absences: int, grade_1: int, grade_2: int, final_grade: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- school: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- address_type: string (nullable = true)\n",
      " |-- family_size: string (nullable = true)\n",
      " |-- parent_status: string (nullable = true)\n",
      " |-- mother_education: string (nullable = true)\n",
      " |-- father_education: string (nullable = true)\n",
      " |-- mother_job: string (nullable = true)\n",
      " |-- father_job: string (nullable = true)\n",
      " |-- school_choice_reason: string (nullable = true)\n",
      " |-- guardian: string (nullable = true)\n",
      " |-- travel_time: string (nullable = true)\n",
      " |-- study_time: string (nullable = true)\n",
      " |-- class_failures: integer (nullable = true)\n",
      " |-- school_support: string (nullable = true)\n",
      " |-- family_support: string (nullable = true)\n",
      " |-- extra_paid_classes: string (nullable = true)\n",
      " |-- activities: string (nullable = true)\n",
      " |-- nursery_school: string (nullable = true)\n",
      " |-- higher_ed: string (nullable = true)\n",
      " |-- internet_access: string (nullable = true)\n",
      " |-- romantic_relationship: string (nullable = true)\n",
      " |-- family_relationship: integer (nullable = true)\n",
      " |-- free_time: integer (nullable = true)\n",
      " |-- social: integer (nullable = true)\n",
      " |-- weekday_alcohol: integer (nullable = true)\n",
      " |-- weekend_alcohol: integer (nullable = true)\n",
      " |-- health: integer (nullable = true)\n",
      " |-- absences: integer (nullable = true)\n",
      " |-- grade_1: integer (nullable = true)\n",
      " |-- grade_2: integer (nullable = true)\n",
      " |-- final_grade: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị thông tin dữ liệu\n",
    "print('Portuguese')\n",
    "print(f\"Số dòng: {df_portuguese.count()}\")\n",
    "print(f\"Số cột: {len(df_portuguese.columns)}\")\n",
    "\n",
    "display(df_portuguese)\n",
    "df_portuguese.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee073d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 200.0 KiB, free 433.6 MiB)\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.1.24:39111 (size: 34.4 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Created broadcast 12 from count at NativeMethodAccessorImpl.java:0\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (FileScan csv [student_id#17,school#18,sex#19,age#20,address_type#21,family_size#22,parent_status#23,mother_education#24,father_education#25,mother_job#26,father_job#27,school_choice_reason#28,guardian#29,travel_time#30,study_time#31,class_failures#32,school_support#33,family_support#34,extra_paid_classes#35,activities#36,nursery_school#37,higher_ed#38,internet_access#39,romantic_relationship#40,... 10 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_mat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<student_id:int,school:string,sex:string,age:int,address_type:string,family_size:string,par...\n",
      " MapPartitionsRDD[35] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 19.7 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 192.168.1.24:39111 (size: 9.1 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (FileScan csv [student_id#17,school#18,sex#19,age#20,address_type#21,family_size#22,parent_status#23,mother_education#24,father_education#25,mother_job#26,father_job#27,school_choice_reason#28,guardian#29,travel_time#30,study_time#31,class_failures#32,school_support#33,family_support#34,extra_paid_classes#35,activities#36,nursery_school#37,higher_ed#38,internet_access#39,romantic_relationship#40,... 10 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_mat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<student_id:int,school:string,sex:string,age:int,address_type:string,family_size:string,par...\n",
      " MapPartitionsRDD[35] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 7) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8261 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 7)\n",
      "[Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/mtam/Spark/spark-3.5.1-bin-hadoop3/Data_project/student_math_clean.csv, range: 0-77747, partition values: [empty row]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO org.apache.spark.storage.memory.MemoryStore - Block rdd_35_0 stored as values in memory (estimated size 26.5 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_35_0 in memory on 192.168.1.24:39111 (size: 26.5 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 8.0 (TID 7)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 8.0 (TID 7)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 7). 1330 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 7) in 88 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.102 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 8: Stage finished\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 40 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (MapPartitionsRDD[40] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 27.7 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 192.168.1.24:39111 (size: 12.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[40] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 8) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8250 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 9.0 (TID 8)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 8)\n",
      "[Executor task launch worker for task 0.0 in stage 9.0 (TID 8)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.1.24:39111 in memory (size: 34.5 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 9.0 (TID 8)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 8). 2113 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 8) in 24 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.102 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 192.168.1.24:39111 in memory (size: 12.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[43] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 12.5 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.1.24:39111 in memory (size: 9.1 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[43] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 9) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 11.0 (TID 9)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 9)\n",
      "[Executor task launch worker for task 0.0 in stage 11.0 (TID 9)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 11.0 (TID 9)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 11.0 (TID 9)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 9). 3995 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 9) in 22 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.033 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 11: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.056667 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số dòng: 395\n",
      "Số cột: 34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[student_id: int, school: string, sex: string, age: int, address_type: string, family_size: string, parent_status: string, mother_education: string, father_education: string, mother_job: string, father_job: string, school_choice_reason: string, guardian: string, travel_time: string, study_time: string, class_failures: int, school_support: string, family_support: string, extra_paid_classes: string, activities: string, nursery_school: string, higher_ed: string, internet_access: string, romantic_relationship: string, family_relationship: int, free_time: int, social: int, weekday_alcohol: int, weekend_alcohol: int, health: int, absences: int, grade_1: int, grade_2: int, final_grade: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- school: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- address_type: string (nullable = true)\n",
      " |-- family_size: string (nullable = true)\n",
      " |-- parent_status: string (nullable = true)\n",
      " |-- mother_education: string (nullable = true)\n",
      " |-- father_education: string (nullable = true)\n",
      " |-- mother_job: string (nullable = true)\n",
      " |-- father_job: string (nullable = true)\n",
      " |-- school_choice_reason: string (nullable = true)\n",
      " |-- guardian: string (nullable = true)\n",
      " |-- travel_time: string (nullable = true)\n",
      " |-- study_time: string (nullable = true)\n",
      " |-- class_failures: integer (nullable = true)\n",
      " |-- school_support: string (nullable = true)\n",
      " |-- family_support: string (nullable = true)\n",
      " |-- extra_paid_classes: string (nullable = true)\n",
      " |-- activities: string (nullable = true)\n",
      " |-- nursery_school: string (nullable = true)\n",
      " |-- higher_ed: string (nullable = true)\n",
      " |-- internet_access: string (nullable = true)\n",
      " |-- romantic_relationship: string (nullable = true)\n",
      " |-- family_relationship: integer (nullable = true)\n",
      " |-- free_time: integer (nullable = true)\n",
      " |-- social: integer (nullable = true)\n",
      " |-- weekday_alcohol: integer (nullable = true)\n",
      " |-- weekend_alcohol: integer (nullable = true)\n",
      " |-- health: integer (nullable = true)\n",
      " |-- absences: integer (nullable = true)\n",
      " |-- grade_1: integer (nullable = true)\n",
      " |-- grade_2: integer (nullable = true)\n",
      " |-- final_grade: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 192.168.1.24:39111 in memory (size: 9.1 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị thông tin dữ liệu\n",
    "print('Math')\n",
    "print(f\"Số dòng: {df_math.count()}\")\n",
    "print(f\"Số cột: {len(df_math.columns)}\")\n",
    "\n",
    "display(df_math)\n",
    "df_math.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068bcc3",
   "metadata": {},
   "source": [
    "## Merging the two csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996de4c2",
   "metadata": {},
   "source": [
    "Đánh dấu môn học trước khi gộp 2 dataset lại \n",
    "+ portuguese: 0\n",
    "+ math: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00e1f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Thêm cột portuguese cho math với giá trị 0\n",
    "df_portuguese = df_portuguese.withColumn(\"subject_code\", lit(0))\n",
    "\n",
    "# Thêm cột subject_code cho math với giá trị 1\n",
    "df_math = df_math.withColumn(\"subject_code\", lit(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c3bb3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "#Kiểm tra trước khi kết hợp\n",
    "# Lấy danh sách các cột\n",
    "cols_math = df_math.columns\n",
    "cols_portuguese = df_portuguese.columns\n",
    "\n",
    "# Kiểm tra xem các cột có giống nhau không\n",
    "if cols_math != cols_portuguese:\n",
    "    print(\"ERROR: Hai DataFrame không có cùng cấu trúc!\")\n",
    "else: \n",
    "    # Kết hợp hai DataFrame\n",
    "    df = df_math.unionByName(df_portuguese)  \n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78813cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một cột mới với giá trị ID tăng dần bắt đầu từ 1\n",
    "df = df.withColumn(\"id_temp\", monotonically_increasing_id())\n",
    "\n",
    "# Sử dụng Window function để sắp xếp lại các ID từ 1 đến N\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.orderBy(\"id_temp\")\n",
    "df = df.withColumn(\"id\", row_number().over(windowSpec))\n",
    "\n",
    "# Loại bỏ cột student_id và id_temp, giữ lại cột id với giá trị từ 1 đến N\n",
    "df = df.drop(\"student_id\", \"id_temp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f62a0",
   "metadata": {},
   "source": [
    "Nhận thấy tên cột không ký tự đặc biệt, cũng không có khoảng cách.\n",
    "Kiểu dữ liệu cũng phù hợp. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07052273",
   "metadata": {},
   "source": [
    "### Thống kê thống kê các thuộc tính định lượng như: count, max, min, mean, stddev, tứ phân vị..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6278f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 192.168.1.24:39111 in memory (size: 12.4 KiB, free: 434.3 MiB)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 47.67421 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.059266 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.302442 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 54 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 10 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 12 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 73.7 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 192.168.1.24:39111 (size: 23.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 10) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 11) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 12.0 (TID 10)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 10)\n",
      "[Executor task launch worker for task 1.0 in stage 12.0 (TID 11)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 11)\n",
      "[Executor task launch worker for task 0.0 in stage 12.0 (TID 10)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 12.0 (TID 11)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 12.0 (TID 10)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 50.547922 ms\n",
      "[Executor task launch worker for task 0.0 in stage 12.0 (TID 10)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.833089 ms\n",
      "[Executor task launch worker for task 1.0 in stage 12.0 (TID 11)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 54.993077 ms\n",
      "[Executor task launch worker for task 0.0 in stage 12.0 (TID 10)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.574968 ms\n",
      "[Executor task launch worker for task 0.0 in stage 12.0 (TID 10)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 10). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 10) in 228 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 12.0 (TID 11)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 11). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 11) in 251 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 12 (showString at NativeMethodAccessorImpl.java:0) finished in 0.283 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 192.168.1.24:39111 in memory (size: 23.4 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 93.403643 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.642793 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.606553 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 13)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 14 (MapPartitionsRDD[62] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 382.6 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 97.3 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 192.168.1.24:39111 (size: 97.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[62] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 12) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 12)\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (177.6 KiB) non-empty blocks including 2 (177.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.120417 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.962725 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.94666 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.232667 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.266503 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.098213 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.585811 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.587807 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.510154 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 53.610676 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 403.608964 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 47.71882 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 246.525145 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 100.692842 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 136.2984 ms\n",
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.730471 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+------------------+------------+--------------------+---------------+-------------------+-------------------+----------+----------+--------------------+--------+-------------+------------+-------------------+--------------+--------------+------------------+----------+--------------+---------+---------------+---------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+-------------------+-----------------+\n",
      "|summary|school| sex|               age|address_type|         family_size|  parent_status|   mother_education|   father_education|mother_job|father_job|school_choice_reason|guardian|  travel_time|  study_time|     class_failures|school_support|family_support|extra_paid_classes|activities|nursery_school|higher_ed|internet_access|romantic_relationship|family_relationship|         free_time|            social|   weekday_alcohol|   weekend_alcohol|            health|         absences|          grade_1|           grade_2|       final_grade|       subject_code|               id|\n",
      "+-------+------+----+------------------+------------+--------------------+---------------+-------------------+-------------------+----------+----------+--------------------+--------+-------------+------------+-------------------+--------------+--------------+------------------+----------+--------------+---------+---------------+---------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+-------------------+-----------------+\n",
      "|  count|  1044|1044|              1044|        1044|                1044|           1044|               1044|               1044|      1044|      1044|                1044|    1044|         1044|        1044|               1044|          1044|          1044|              1044|      1044|          1044|     1044|           1044|                 1044|               1044|              1044|              1044|              1044|              1044|              1044|             1044|             1044|              1044|              1044|               1044|             1044|\n",
      "|   mean|  NULL|NULL|16.726053639846743|        NULL|                NULL|           NULL|               NULL|               NULL|      NULL|      NULL|                NULL|    NULL|         NULL|        NULL|0.26436781609195403|          NULL|          NULL|              NULL|      NULL|          NULL|     NULL|           NULL|                 NULL|  3.935823754789272|3.2011494252873565|3.1561302681992336|1.4942528735632183|2.2844827586206895| 3.543103448275862|4.434865900383142|11.21360153256705|11.246168582375478|11.341954022988507| 0.3783524904214559|            522.5|\n",
      "| stddev|  NULL|NULL|1.2399746931649538|        NULL|                NULL|           NULL|               NULL|               NULL|      NULL|      NULL|                NULL|    NULL|         NULL|        NULL| 0.6561417798335698|          NULL|          NULL|              NULL|      NULL|          NULL|     NULL|           NULL|                 NULL| 0.9334007716663543|1.0315068335871396|1.1525746602053926|0.9117142815596276|1.2851048062618582|1.4247034122490199|6.210016564763475|2.983393934015448|  3.28507106798263| 3.864795804383607|0.48520860315415704|301.5211435372319|\n",
      "|    min|    GP|   F|                15|       Rural|      Greater than 3|          Apart|   5th to 9th grade|   5th to 9th grade|   at_home|   at_home|              course|  father|15 to 30 min.|2 to 5 hours|                  0|            no|            no|                no|        no|            no|       no|             no|                   no|                  1|                 1|                 1|                 1|                 1|                 1|                0|                0|                 0|                 0|                  0|                1|\n",
      "|    25%|  NULL|NULL|                16|        NULL|                NULL|           NULL|               NULL|               NULL|      NULL|      NULL|                NULL|    NULL|         NULL|        NULL|                  0|          NULL|          NULL|              NULL|      NULL|          NULL|     NULL|           NULL|                 NULL|                  4|                 3|                 2|                 1|                 1|                 3|                0|                9|                 9|                10|                  0|              261|\n",
      "|    50%|  NULL|NULL|                17|        NULL|                NULL|           NULL|               NULL|               NULL|      NULL|      NULL|                NULL|    NULL|         NULL|        NULL|                  0|          NULL|          NULL|              NULL|      NULL|          NULL|     NULL|           NULL|                 NULL|                  4|                 3|                 3|                 1|                 2|                 4|                2|               11|                11|                11|                  0|              522|\n",
      "|    75%|  NULL|NULL|                18|        NULL|                NULL|           NULL|               NULL|               NULL|      NULL|      NULL|                NULL|    NULL|         NULL|        NULL|                  0|          NULL|          NULL|              NULL|      NULL|          NULL|     NULL|           NULL|                 NULL|                  5|                 4|                 4|                 2|                 3|                 5|                6|               13|                13|                14|                  1|              783|\n",
      "|    max|    MS|   M|                22|       Urban|Less than or equa...|Living together|secondary education|secondary education|   teacher|   teacher|          reputation|   other|      >1 hour|   >10 hours|                  3|           yes|           yes|               yes|       yes|           yes|      yes|            yes|                  yes|                  5|                 5|                 5|                 5|                 5|                 5|               75|               19|                19|                20|                  1|             1044|\n",
      "+-------+------+----+------------------+------------+--------------------+---------------+-------------------+-------------------+----------+----------+--------------------+--------+-------------+------------+-------------------+--------------+--------------+------------------+----------+--------------+---------+---------------+---------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+-------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 14.0 (TID 12)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 12). 7527 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 12) in 2528 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 14 (showString at NativeMethodAccessorImpl.java:0) finished in 2.824 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 14: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 11 finished: showString at NativeMethodAccessorImpl.java:0, took 2.835331 s\n",
      "\r",
      "                                                                                \r",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.335714 ms\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7cf95",
   "metadata": {},
   "source": [
    "### Thống kê số dòng null trong tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce57a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 71 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 12 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 15 (MapPartitionsRDD[71] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 39.3 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 192.168.1.24:39111 (size: 16.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[71] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 13) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 14) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 15.0 (TID 14)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 14)\n",
      "[Executor task launch worker for task 0.0 in stage 15.0 (TID 13)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 13)\n",
      "[Executor task launch worker for task 1.0 in stage 15.0 (TID 14)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 15.0 (TID 13)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 15.0 (TID 14)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 14). 2354 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 14) in 24 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 15.0 (TID 13)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 13). 2354 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 13) in 29 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.037 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 13 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[74] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 12.5 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[74] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 15) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 17.0 (TID 15)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 15)\n",
      "[Executor task launch worker for task 0.0 in stage 17.0 (TID 15)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 17.0 (TID 15)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 17.0 (TID 15)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 15). 3995 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 15) in 19 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.025 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 17: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 13 finished: count at NativeMethodAccessorImpl.java:0, took 0.035353 s\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 192.168.1.24:39111 in memory (size: 16.3 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.022718 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 83 (toPandas at /tmp/ipykernel_14509/2545133016.py:10) as input to shuffle 4\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 14 (toPandas at /tmp/ipykernel_14509/2545133016.py:10) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 18 (toPandas at /tmp/ipykernel_14509/2545133016.py:10)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 18 (MapPartitionsRDD[83] at toPandas at /tmp/ipykernel_14509/2545133016.py:10), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 88.4 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 28.9 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 192.168.1.24:39111 (size: 28.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[83] at toPandas at /tmp/ipykernel_14509/2545133016.py:10) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 16) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 18.0 (TID 17) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 18.0 (TID 16)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 16)\n",
      "[Executor task launch worker for task 1.0 in stage 18.0 (TID 17)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 18.0 (TID 17)\n",
      "[Executor task launch worker for task 1.0 in stage 18.0 (TID 17)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 18.0 (TID 16)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 18.0 (TID 17)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.552944 ms\n",
      "[Executor task launch worker for task 1.0 in stage 18.0 (TID 17)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.781298 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.497446 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 92 (toPandas at /tmp/ipykernel_14509/2545133016.py:10) as input to shuffle 5\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 15 (toPandas at /tmp/ipykernel_14509/2545133016.py:10) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 19 (toPandas at /tmp/ipykernel_14509/2545133016.py:10)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 19 (MapPartitionsRDD[92] at toPandas at /tmp/ipykernel_14509/2545133016.py:10), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 98.1 KiB, free 433.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 31.5 KiB, free 433.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 192.168.1.24:39111 (size: 31.5 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 18.0 (TID 17)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 18.0 (TID 17). 2354 bytes result sent to driver\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[92] at toPandas at /tmp/ipykernel_14509/2545133016.py:10) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 2 tasks resource profile 0\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 18.0 (TID 17) in 103 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 18.0 (TID 16)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 16). 2354 bytes result sent to driver\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 18) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 19) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 16) in 119 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "[Executor task launch worker for task 1.0 in stage 19.0 (TID 19)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 19)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 18 (toPandas at /tmp/ipykernel_14509/2545133016.py:10) finished in 0.125 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 19)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Executor task launch worker for task 1.0 in stage 19.0 (TID 19)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 19.0 (TID 18)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 18)\n",
      "[Executor task launch worker for task 0.0 in stage 19.0 (TID 18)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 1.0 in stage 19.0 (TID 19)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 40.610801 ms\n",
      "[Executor task launch worker for task 0.0 in stage 19.0 (TID 18)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 18). 2354 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 18) in 57 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 19.0 (TID 19)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 19). 2354 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 19) in 64 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 19 (toPandas at /tmp/ipykernel_14509/2545133016.py:10) finished in 0.086 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.73577 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.064448 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: toPandas at /tmp/ipykernel_14509/2545133016.py:10\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 16 (toPandas at /tmp/ipykernel_14509/2545133016.py:10) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 (toPandas at /tmp/ipykernel_14509/2545133016.py:10)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 20, ShuffleMapStage 21)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[98] at toPandas at /tmp/ipykernel_14509/2545133016.py:10), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 74.6 KiB, free 433.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 21.2 KiB, free 433.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 192.168.1.24:39111 (size: 21.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 22 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[98] at toPandas at /tmp/ipykernel_14509/2545133016.py:10) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 20) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7724 bytes) \n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 21) (192.168.1.24, executor driver, partition 1, NODE_LOCAL, 7724 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 22.0 (TID 20)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 20)\n",
      "[Executor task launch worker for task 0.0 in stage 22.0 (TID 20)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 22.0 (TID 20)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 1.0 in stage 22.0 (TID 21)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 21)\n",
      "[Executor task launch worker for task 1.0 in stage 22.0 (TID 21)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 1.0 in stage 22.0 (TID 21)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 22.0 (TID 20)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.413503 ms\n",
      "[Executor task launch worker for task 0.0 in stage 22.0 (TID 20)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 20). 5334 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 20) in 38 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 22.0 (TID 21)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.623761 ms\n",
      "[Executor task launch worker for task 1.0 in stage 22.0 (TID 21)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 21). 5334 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 21) in 59 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 22 (toPandas at /tmp/ipykernel_14509/2545133016.py:10) finished in 0.070 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 22: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 16 finished: toPandas at /tmp/ipykernel_14509/2545133016.py:10, took 0.075384 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number missing</th>\n",
       "      <th>Percent %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_alcohol</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>higher_ed</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internet_access</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>romantic_relationship</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_relationship</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free_time</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekend_alcohol</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activities</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absences</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final_grade</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject_code</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nursery_school</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra_paid_classes</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_job</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address_type</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_size</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent_status</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_education</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father_education</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father_job</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_support</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school_choice_reason</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guardian</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel_time</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study_time</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_failures</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school_support</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Number missing  Percent %\n",
       "school                            0.0        0.0\n",
       "weekday_alcohol                   0.0        0.0\n",
       "higher_ed                         0.0        0.0\n",
       "internet_access                   0.0        0.0\n",
       "romantic_relationship             0.0        0.0\n",
       "family_relationship               0.0        0.0\n",
       "free_time                         0.0        0.0\n",
       "social                            0.0        0.0\n",
       "weekend_alcohol                   0.0        0.0\n",
       "activities                        0.0        0.0\n",
       "health                            0.0        0.0\n",
       "absences                          0.0        0.0\n",
       "grade_1                           0.0        0.0\n",
       "grade_2                           0.0        0.0\n",
       "final_grade                       0.0        0.0\n",
       "subject_code                      0.0        0.0\n",
       "nursery_school                    0.0        0.0\n",
       "extra_paid_classes                0.0        0.0\n",
       "sex                               0.0        0.0\n",
       "mother_job                        0.0        0.0\n",
       "age                               0.0        0.0\n",
       "address_type                      0.0        0.0\n",
       "family_size                       0.0        0.0\n",
       "parent_status                     0.0        0.0\n",
       "mother_education                  0.0        0.0\n",
       "father_education                  0.0        0.0\n",
       "father_job                        0.0        0.0\n",
       "family_support                    0.0        0.0\n",
       "school_choice_reason              0.0        0.0\n",
       "guardian                          0.0        0.0\n",
       "travel_time                       0.0        0.0\n",
       "study_time                        0.0        0.0\n",
       "class_failures                    0.0        0.0\n",
       "school_support                    0.0        0.0\n",
       "id                                0.0        0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Thống kê Lượng giá trị null cho từng cột\n",
    "null_counts = df.agg(*[count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "total_null_counts_percent = null_counts.agg(*[sum(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "# Phần trăm missing data\n",
    "total_rows = df.count()\n",
    "null_percentages = null_counts.select(*[(col(c) / total_rows).alias(c + '_percent') for c in null_counts.columns])\n",
    "\n",
    "# Hiển thị\n",
    "missing_data = null_counts.union(null_percentages).toPandas().transpose()\n",
    "missing_data.columns = [\"Number missing\",\"Percent %\"]\n",
    "missing_data = missing_data.sort_values(by=\"Percent %\", ascending=False)\n",
    "display(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418a6ac",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e135123",
   "metadata": {},
   "source": [
    "### Trước tiên cần loại bỏ các thuộc tính không cần thiết và ít ảnh hưởng đến mô hình dự đoán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a821f27",
   "metadata": {},
   "source": [
    "### Trong tập dữ liệu, nhóm sẽ ưu tiên giữ các thuộc tính liên quan đến việc học (nhóm 1) và về học sinh (nhóm 2), hơn là về gia đình học sinh (nhóm 3):\n",
    "#### Nhóm 1:\n",
    "+ class_failures, study_time, absences, grade_1, grade_2\n",
    "+ school_choice_reason, travel_time, school_support, family_support, extra_paid_classes, higher_ed, internet_access\n",
    "\n",
    "#### Nhóm 2:\n",
    "+ sex, age, address_type, activities, romantic_relationship, family_relationship, free_time, social, weekday_alcohol, weekend_alcohol, health, nursery_school\n",
    "\n",
    "#### Nhóm 3:\n",
    "+ family_size, parent_status, mother_education, father_education, mother_job, father_job, guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f9b40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bỏ đi tính cá nhân trong tập dữ liệu\n",
    "drop_cols_1 = ['student_id', 'school']\n",
    "\n",
    "# Bỏ các thuộc tính ít cần\n",
    "drop_cols_2 = ['family_size', 'parent_status', 'mother_job', 'father_job','nursery_school', 'travel_time']\n",
    "\n",
    "# Kết hợp cả hai danh sách\n",
    "drop_cols = drop_cols_1 + drop_cols_2\n",
    "df = df.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa04ac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- address_type: string (nullable = true)\n",
      " |-- mother_education: string (nullable = true)\n",
      " |-- father_education: string (nullable = true)\n",
      " |-- school_choice_reason: string (nullable = true)\n",
      " |-- guardian: string (nullable = true)\n",
      " |-- study_time: string (nullable = true)\n",
      " |-- class_failures: integer (nullable = true)\n",
      " |-- school_support: string (nullable = true)\n",
      " |-- family_support: string (nullable = true)\n",
      " |-- extra_paid_classes: string (nullable = true)\n",
      " |-- activities: string (nullable = true)\n",
      " |-- higher_ed: string (nullable = true)\n",
      " |-- internet_access: string (nullable = true)\n",
      " |-- romantic_relationship: string (nullable = true)\n",
      " |-- family_relationship: integer (nullable = true)\n",
      " |-- free_time: integer (nullable = true)\n",
      " |-- social: integer (nullable = true)\n",
      " |-- weekday_alcohol: integer (nullable = true)\n",
      " |-- weekend_alcohol: integer (nullable = true)\n",
      " |-- health: integer (nullable = true)\n",
      " |-- absences: integer (nullable = true)\n",
      " |-- grade_1: integer (nullable = true)\n",
      " |-- grade_2: integer (nullable = true)\n",
      " |-- final_grade: integer (nullable = true)\n",
      " |-- subject_code: integer (nullable = false)\n",
      " |-- id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7737dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 192.168.1.24:39111 in memory (size: 28.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on 192.168.1.24:39111 in memory (size: 21.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 107 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 17 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 23 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 23 (MapPartitionsRDD[107] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 192.168.1.24:39111 in memory (size: 31.5 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 39.3 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 192.168.1.24:39111 (size: 16.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[107] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 22) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 23) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 23.0 (TID 23)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 23)\n",
      "[Executor task launch worker for task 0.0 in stage 23.0 (TID 22)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 22)\n",
      "[Executor task launch worker for task 1.0 in stage 23.0 (TID 23)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 23.0 (TID 22)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 23.0 (TID 23)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 23). 2354 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 23) in 33 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 23.0 (TID 22)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 22). 2354 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 22) in 36 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 23 (count at NativeMethodAccessorImpl.java:0) finished in 0.079 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 18 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 12.5 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 24) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 25.0 (TID 24)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 24)\n",
      "[Executor task launch worker for task 0.0 in stage 25.0 (TID 24)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 25.0 (TID 24)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 25.0 (TID 24)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 24). 3995 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 24) in 23 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.032 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 25: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 18 finished: count at NativeMethodAccessorImpl.java:0, took 0.037418 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.537778 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số dòng: 1044\n",
      "Số cột: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.226579 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.383202 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.129316 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.759314 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 122 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 19 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 27 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 26)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 26)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 26 (MapPartitionsRDD[122] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 79.8 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 433.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 192.168.1.24:39111 (size: 28.3 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 25 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[122] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 26.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 26.0 (TID 25) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 26.0 (TID 26) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 26.0 (TID 26)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 26.0 (TID 26)\n",
      "[Executor task launch worker for task 0.0 in stage 26.0 (TID 25)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 26.0 (TID 25)\n",
      "[Executor task launch worker for task 1.0 in stage 26.0 (TID 26)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.939016 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+----------------+----------------+--------------------+--------+------------+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+---------------+------+--------+-------+-------+-----------+------------+---+\n",
      "|sex|age|address_type|mother_education|father_education|school_choice_reason|guardian|  study_time|class_failures|school_support|family_support|extra_paid_classes|activities|higher_ed|internet_access|romantic_relationship|family_relationship|free_time|social|weekday_alcohol|weekend_alcohol|health|absences|grade_1|grade_2|final_grade|subject_code| id|\n",
      "+---+---+------------+----------------+----------------+--------------------+--------+------------+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+---------------+------+--------+-------+-------+-----------+------------+---+\n",
      "|  F| 18|       Urban|higher education|higher education|              course|  mother|2 to 5 hours|             0|           yes|            no|                no|        no|      yes|             no|                   no|                  4|        3|     4|              1|              1|     3|       6|      5|      6|          6|           1|  1|\n",
      "+---+---+------------+----------------+----------------+--------------------+--------+------------+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+---------------+------+--------+-------+-------+-----------+------------+---+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- address_type: string (nullable = true)\n",
      " |-- mother_education: string (nullable = true)\n",
      " |-- father_education: string (nullable = true)\n",
      " |-- school_choice_reason: string (nullable = true)\n",
      " |-- guardian: string (nullable = true)\n",
      " |-- study_time: string (nullable = true)\n",
      " |-- class_failures: integer (nullable = true)\n",
      " |-- school_support: string (nullable = true)\n",
      " |-- family_support: string (nullable = true)\n",
      " |-- extra_paid_classes: string (nullable = true)\n",
      " |-- activities: string (nullable = true)\n",
      " |-- higher_ed: string (nullable = true)\n",
      " |-- internet_access: string (nullable = true)\n",
      " |-- romantic_relationship: string (nullable = true)\n",
      " |-- family_relationship: integer (nullable = true)\n",
      " |-- free_time: integer (nullable = true)\n",
      " |-- social: integer (nullable = true)\n",
      " |-- weekday_alcohol: integer (nullable = true)\n",
      " |-- weekend_alcohol: integer (nullable = true)\n",
      " |-- health: integer (nullable = true)\n",
      " |-- absences: integer (nullable = true)\n",
      " |-- grade_1: integer (nullable = true)\n",
      " |-- grade_2: integer (nullable = true)\n",
      " |-- final_grade: integer (nullable = true)\n",
      " |-- subject_code: integer (nullable = false)\n",
      " |-- id: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 26.0 (TID 25)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 26.0 (TID 26)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 26.0 (TID 26)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.561931 ms\n",
      "[Executor task launch worker for task 0.0 in stage 26.0 (TID 25)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.749792 ms\n",
      "[Executor task launch worker for task 1.0 in stage 26.0 (TID 26)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.619097 ms\n",
      "[Executor task launch worker for task 0.0 in stage 26.0 (TID 25)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.992276 ms\n",
      "[Executor task launch worker for task 0.0 in stage 26.0 (TID 25)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 26.0 (TID 25). 3654 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 26.0 (TID 25) in 190 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 26.0 (TID 26)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 26.0 (TID 26). 3654 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 26.0 (TID 26) in 196 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 26 (showString at NativeMethodAccessorImpl.java:0) finished in 0.213 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 27)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 27 (MapPartitionsRDD[127] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 67.4 KiB, free 433.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 433.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 192.168.1.24:39111 (size: 27.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 26 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[127] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 27) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 27.0 (TID 27)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 27)\n",
      "[Executor task launch worker for task 0.0 in stage 27.0 (TID 27)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (980.0 B) non-empty blocks including 2 (980.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 27.0 (TID 27)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 27.0 (TID 27)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.377938 ms\n",
      "[Executor task launch worker for task 0.0 in stage 27.0 (TID 27)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.70008 ms\n",
      "[Executor task launch worker for task 0.0 in stage 27.0 (TID 27)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 27). 4977 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 27) in 50 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 27 (showString at NativeMethodAccessorImpl.java:0) finished in 0.058 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 27: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 19 finished: showString at NativeMethodAccessorImpl.java:0, took 0.285721 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.334937 ms\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị DataFrame kết hợp\n",
    "print(f\"Số dòng: {df.count()}\")\n",
    "print(f\"Số cột: {len(df.columns)}\")\n",
    "\n",
    "df.show(1)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4c63233",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 192.168.1.24:39111 in memory (size: 16.3 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on 192.168.1.24:39111 in memory (size: 27.0 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_25_piece0 on 192.168.1.24:39111 in memory (size: 28.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 140.600645 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.957004 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.595173 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 138 (toPandas at /tmp/ipykernel_14509/2560516005.py:8) as input to shuffle 8\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 20 (toPandas at /tmp/ipykernel_14509/2560516005.py:8) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 28 (toPandas at /tmp/ipykernel_14509/2560516005.py:8)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 28 (MapPartitionsRDD[138] at toPandas at /tmp/ipykernel_14509/2560516005.py:8), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 169.0 KiB, free 433.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 42.6 KiB, free 433.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 192.168.1.24:39111 (size: 42.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[138] at toPandas at /tmp/ipykernel_14509/2560516005.py:8) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 28.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 28.0 (TID 28) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 28.0 (TID 29) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 28.0 (TID 29)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 28.0 (TID 29)\n",
      "[Executor task launch worker for task 0.0 in stage 28.0 (TID 28)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 28.0 (TID 28)\n",
      "[Executor task launch worker for task 0.0 in stage 28.0 (TID 28)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 28.0 (TID 29)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 28.0 (TID 28)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.462689 ms\n",
      "[Executor task launch worker for task 1.0 in stage 28.0 (TID 29)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.177395 ms\n",
      "[Executor task launch worker for task 0.0 in stage 28.0 (TID 28)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.907623 ms\n",
      "[Executor task launch worker for task 1.0 in stage 28.0 (TID 29)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 126.047408 ms\n",
      "[Executor task launch worker for task 1.0 in stage 28.0 (TID 29)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.122623 ms\n",
      "[Executor task launch worker for task 1.0 in stage 28.0 (TID 29)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.14949 ms\n",
      "[Executor task launch worker for task 1.0 in stage 28.0 (TID 29)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 28.0 (TID 29). 3085 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 28.0 (TID 29) in 356 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 28.0 (TID 28)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 28.0 (TID 28). 3085 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 28.0 (TID 28) in 405 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 28 (toPandas at /tmp/ipykernel_14509/2560516005.py:8) finished in 0.427 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_27_piece0 on 192.168.1.24:39111 in memory (size: 42.6 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 55.84636 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 141 (toPandas at /tmp/ipykernel_14509/2560516005.py:8) as input to shuffle 9\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 21 (toPandas at /tmp/ipykernel_14509/2560516005.py:8) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 30 (toPandas at /tmp/ipykernel_14509/2560516005.py:8)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 29)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 30 (MapPartitionsRDD[141] at toPandas at /tmp/ipykernel_14509/2560516005.py:8), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 161.6 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 50.8 KiB, free 433.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 192.168.1.24:39111 (size: 50.8 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[141] at toPandas at /tmp/ipykernel_14509/2560516005.py:8) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 30.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 30.0 (TID 30) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 30.0 (TID 30)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 30.0 (TID 30)\n",
      "[Executor task launch worker for task 0.0 in stage 30.0 (TID 30)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (13.3 KiB) non-empty blocks including 2 (13.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 30.0 (TID 30)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "[Executor task launch worker for task 0.0 in stage 30.0 (TID 30)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 80.453381 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 30.0 (TID 30)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 30.0 (TID 30). 5958 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 30.0 (TID 30) in 122 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 30 (toPandas at /tmp/ipykernel_14509/2560516005.py:8) finished in 0.133 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.242755 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: toPandas at /tmp/ipykernel_14509/2560516005.py:8\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 22 (toPandas at /tmp/ipykernel_14509/2560516005.py:8) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 33 (toPandas at /tmp/ipykernel_14509/2560516005.py:8)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 32)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 33 (MapPartitionsRDD[144] at toPandas at /tmp/ipykernel_14509/2560516005.py:8), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 26.1 KiB, free 433.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 433.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 192.168.1.24:39111 (size: 8.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[144] at toPandas at /tmp/ipykernel_14509/2560516005.py:8) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 33.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 33.0 (TID 31) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 33.0 (TID 31)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 33.0 (TID 31)\n",
      "[Executor task launch worker for task 0.0 in stage 33.0 (TID 31)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (117.0 B) non-empty blocks including 1 (117.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 33.0 (TID 31)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 33.0 (TID 31)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.803768 ms\n",
      "[Executor task launch worker for task 0.0 in stage 33.0 (TID 31)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 33.0 (TID 31). 4041 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 33.0 (TID 31) in 17 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 33 (toPandas at /tmp/ipykernel_14509/2560516005.py:8) finished in 0.023 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 33: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 22 finished: toPandas at /tmp/ipykernel_14509/2560516005.py:8, took 0.032423 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Loại bỏ cột student_id, grade_1, grade_2, final_grade không cần xem số giá trị duy nhất\n",
    "df_distinct = df.drop(\"id\", \"grade_1\", \"grade_2\", \"final_grade\")\n",
    "\n",
    "# Tính số giá trị duy nhất của từng cột hiện tại\n",
    "distinct_counts = df_distinct.agg(*(countDistinct(col(c)).alias(c) for c in df_distinct.columns))\n",
    "\n",
    "# Chuyển kết quả sang Pandas DataFrame (dữ liệu sau khi count nhỏ)\n",
    "distinct_counts_pd = distinct_counts.toPandas().T.reset_index()\n",
    "distinct_counts_pd.columns = ['column', 'distinct_count']\n",
    "\n",
    "# Sắp xếp kết quả từ ít giá trị nhất đến nhiều giá trị nhất\n",
    "sorted_distinct_counts_pd = distinct_counts_pd.sort_values(by='distinct_count')\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(12, 10))\n",
    "bars = plt.barh(sorted_distinct_counts_pd['column'], sorted_distinct_counts_pd['distinct_count'])\n",
    "plt.xlabel('Distinct Value Count')\n",
    "plt.title('Number of Distinct Values in Each Column')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Thêm giá trị label cho mỗi bar\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{int(bar.get_width())}', va='center', ha='left')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610a5f0",
   "metadata": {},
   "source": [
    "## Xử lý các thuộc tính non-numberic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05fdb3d",
   "metadata": {},
   "source": [
    "### Sau khi loại bỏ ta sẽ tiến hành xử lý các thuộc tính non-numberic còn lại"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4190a5",
   "metadata": {},
   "source": [
    "### 1. Dạng thuộc tính non-binary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed45a14",
   "metadata": {},
   "source": [
    "#### 1.1 Cột mother_education, và father_education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "292006bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----------------------------+\n",
      "|mother_education             |father_education             |\n",
      "+-----------------------------+-----------------------------+\n",
      "|higher education             |higher education             |\n",
      "|primary education (4th grade)|primary education (4th grade)|\n",
      "|primary education (4th grade)|primary education (4th grade)|\n",
      "|higher education             |5th to 9th grade             |\n",
      "|secondary education          |secondary education          |\n",
      "|higher education             |secondary education          |\n",
      "|5th to 9th grade             |5th to 9th grade             |\n",
      "|higher education             |higher education             |\n",
      "|secondary education          |5th to 9th grade             |\n",
      "|secondary education          |higher education             |\n",
      "|higher education             |higher education             |\n",
      "|5th to 9th grade             |primary education (4th grade)|\n",
      "|higher education             |higher education             |\n",
      "|higher education             |secondary education          |\n",
      "|5th to 9th grade             |5th to 9th grade             |\n",
      "|higher education             |higher education             |\n",
      "|higher education             |higher education             |\n",
      "|secondary education          |secondary education          |\n",
      "|secondary education          |5th to 9th grade             |\n",
      "|higher education             |secondary education          |\n",
      "+-----------------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.062306 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.605445 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 23 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 34 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 34 (MapPartitionsRDD[154] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 39.8 KiB, free 433.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 192.168.1.24:39111 (size: 16.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[154] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 34.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 34.0 (TID 32) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 34.0 (TID 32)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 34.0 (TID 32)\n",
      "[Executor task launch worker for task 0.0 in stage 34.0 (TID 32)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 34.0 (TID 32)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.640371 ms\n",
      "[Executor task launch worker for task 0.0 in stage 34.0 (TID 32)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.452029 ms\n",
      "[Executor task launch worker for task 0.0 in stage 34.0 (TID 32)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 34.0 (TID 32)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 34.0 (TID 32)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 34.0 (TID 32). 2285 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 34.0 (TID 32) in 23 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 34 (showString at NativeMethodAccessorImpl.java:0) finished in 0.030 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 34: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 23 finished: showString at NativeMethodAccessorImpl.java:0, took 0.040153 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.758547 ms\n"
     ]
    }
   ],
   "source": [
    "df.select(\"mother_education\", \"father_education\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3fa8297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.638898 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 163 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 10\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 24 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 35 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 35 (MapPartitionsRDD[163] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 49.8 KiB, free 433.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 31 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[163] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 35.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 35.0 (TID 33) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 35.0 (TID 34) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 35.0 (TID 33)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 35.0 (TID 33)\n",
      "[Executor task launch worker for task 0.0 in stage 35.0 (TID 33)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 35.0 (TID 34)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 35.0 (TID 34)\n",
      "[Executor task launch worker for task 1.0 in stage 35.0 (TID 34)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_28_piece0 on 192.168.1.24:39111 in memory (size: 50.8 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 35.0 (TID 33)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.240808 ms\n",
      "[Executor task launch worker for task 0.0 in stage 35.0 (TID 33)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.208353 ms\n",
      "[Executor task launch worker for task 0.0 in stage 35.0 (TID 33)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.990944 ms\n",
      "[Executor task launch worker for task 0.0 in stage 35.0 (TID 33)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 35.0 (TID 33). 2917 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 35.0 (TID 33) in 60 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_30_piece0 on 192.168.1.24:39111 in memory (size: 16.6 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 35.0 (TID 34)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 35.0 (TID 34). 2917 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 35.0 (TID 34) in 167 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 35 (showString at NativeMethodAccessorImpl.java:0) finished in 0.258 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 192.168.1.24:39111 in memory (size: 97.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_29_piece0 on 192.168.1.24:39111 in memory (size: 8.9 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 85.791596 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 25 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 37 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 36)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 37 (MapPartitionsRDD[166] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 63.3 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 26.4 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 192.168.1.24:39111 (size: 26.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[166] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 37.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 37.0 (TID 35) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 37.0 (TID 35)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 37.0 (TID 35)\n",
      "[Executor task launch worker for task 0.0 in stage 37.0 (TID 35)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (802.0 B) non-empty blocks including 2 (802.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 37.0 (TID 35)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 37.0 (TID 35)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 82.180974 ms\n",
      "[Executor task launch worker for task 0.0 in stage 37.0 (TID 35)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 37.0 (TID 35). 5423 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 37.0 (TID 35) in 109 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 37 (showString at NativeMethodAccessorImpl.java:0) finished in 0.121 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 37: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 25 finished: showString at NativeMethodAccessorImpl.java:0, took 0.127922 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.626744 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 175 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 11\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 26 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 38 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 38 (MapPartitionsRDD[175] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_33 stored as values in memory (estimated size 49.8 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_33_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_33_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 33 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[175] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 38.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 38.0 (TID 36) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 38.0 (TID 37) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 38.0 (TID 36)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 38.0 (TID 36)\n",
      "[Executor task launch worker for task 1.0 in stage 38.0 (TID 37)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 38.0 (TID 37)\n",
      "[Executor task launch worker for task 1.0 in stage 38.0 (TID 37)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 38.0 (TID 36)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 38.0 (TID 36)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 38.0 (TID 36). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 38.0 (TID 37)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 38.0 (TID 37). 2917 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 38.0 (TID 36) in 28 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 38.0 (TID 37) in 31 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 38 (showString at NativeMethodAccessorImpl.java:0) finished in 0.042 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(11), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|mother_education             |\n",
      "+-----------------------------+\n",
      "|5th to 9th grade             |\n",
      "|none                         |\n",
      "|secondary education          |\n",
      "|primary education (4th grade)|\n",
      "|higher education             |\n",
      "+-----------------------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|father_education             |\n",
      "+-----------------------------+\n",
      "|5th to 9th grade             |\n",
      "|none                         |\n",
      "|secondary education          |\n",
      "|primary education (4th grade)|\n",
      "|higher education             |\n",
      "+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 27 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 40 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 39)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 40 (MapPartitionsRDD[178] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_34 stored as values in memory (estimated size 63.3 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_34_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_34_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 34 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[178] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 40.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 40.0 (TID 38) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 40.0 (TID 38)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 40.0 (TID 38)\n",
      "[Executor task launch worker for task 0.0 in stage 40.0 (TID 38)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (802.0 B) non-empty blocks including 2 (802.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 40.0 (TID 38)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 40.0 (TID 38)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 40.0 (TID 38). 5423 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 40.0 (TID 38) in 22 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 40 (showString at NativeMethodAccessorImpl.java:0) finished in 0.031 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 40: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 27 finished: showString at NativeMethodAccessorImpl.java:0, took 0.044643 s\n"
     ]
    }
   ],
   "source": [
    "# Lấy tất cả các thể loại duy nhất\n",
    "unique_mother_edu_df = df.select(\"mother_education\").distinct()\n",
    "unique_father_edu_df = df.select(\"father_education\").distinct()\n",
    "\n",
    "# Loại bỏ các dòng có giá trị rỗng trong cột \"mother_education\"\n",
    "unique_mother_edu_df.show(truncate=False)\n",
    "unique_father_edu_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aecc396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 187 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2) as input to shuffle 12\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 28 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 41 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 41 (MapPartitionsRDD[187] at toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_35 stored as values in memory (estimated size 49.8 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_35_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_35_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 35 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[187] at toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 41.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 41.0 (TID 39) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 41.0 (TID 40) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 41.0 (TID 40)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 41.0 (TID 40)\n",
      "[Executor task launch worker for task 0.0 in stage 41.0 (TID 39)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 41.0 (TID 39)\n",
      "[Executor task launch worker for task 0.0 in stage 41.0 (TID 39)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 41.0 (TID 39)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 41.0 (TID 39). 2917 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 41.0 (TID 39) in 34 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 41.0 (TID 40)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 41.0 (TID 40)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 41.0 (TID 40). 2917 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 41.0 (TID 40) in 60 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 41 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2) finished in 0.066 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(12), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.748001 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 29 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 43 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 42)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 43 (MapPartitionsRDD[192] at toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_36 stored as values in memory (estimated size 66.2 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_36_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_36_piece0 in memory on 192.168.1.24:39111 (size: 27.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 36 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[192] at toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 43.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 43.0 (TID 41) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 43.0 (TID 41)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 43.0 (TID 41)\n",
      "[Executor task launch worker for task 0.0 in stage 43.0 (TID 41)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (802.0 B) non-empty blocks including 2 (802.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 43.0 (TID 41)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 43.0 (TID 41)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.681567 ms\n",
      "[Executor task launch worker for task 0.0 in stage 43.0 (TID 41)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 43.0 (TID 41). 5894 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 43.0 (TID 41) in 59 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 43 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:2) finished in 0.077 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 43: Stage finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 201 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3) as input to shuffle 13\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 30 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 44 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 44 (MapPartitionsRDD[201] at toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_37 stored as values in memory (estimated size 49.8 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_37_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_37_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 37 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[201] at toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 44.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 44.0 (TID 42) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 44.0 (TID 43) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 44.0 (TID 43)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 44.0 (TID 43)\n",
      "[Executor task launch worker for task 1.0 in stage 44.0 (TID 43)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 44.0 (TID 42)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 44.0 (TID 42)\n",
      "[Executor task launch worker for task 1.0 in stage 44.0 (TID 43)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 44.0 (TID 43). 2917 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 44.0 (TID 43) in 32 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 44.0 (TID 42)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 44.0 (TID 42)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 44.0 (TID 42). 2917 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 44.0 (TID 42) in 65 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 44 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3) finished in 0.090 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(13), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 31 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 46 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 45)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 46 (MapPartitionsRDD[206] at toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_38 stored as values in memory (estimated size 66.2 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_38_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 433.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_38_piece0 in memory on 192.168.1.24:39111 (size: 27.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 38 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[206] at toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 46.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 46.0 (TID 44) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 46.0 (TID 44)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 46.0 (TID 44)\n",
      "[Executor task launch worker for task 0.0 in stage 46.0 (TID 44)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (802.0 B) non-empty blocks including 2 (802.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 46.0 (TID 44)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 46.0 (TID 44)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 46.0 (TID 44). 5894 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 46.0 (TID 44) in 20 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 46.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 46 (toLocalIterator at /tmp/ipykernel_14509/3529010242.py:3) finished in 0.033 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 46: Stage finished\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_34_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_35_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_38_piece0 on 192.168.1.24:39111 in memory (size: 27.7 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_31_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_36_piece0 on 192.168.1.24:39111 in memory (size: 27.7 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_33_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_37_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_32_piece0 on 192.168.1.24:39111 in memory (size: 26.4 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Chuyển danh sách học vấn thành một danh sách Python\n",
    "list_mother_edu = [row[\"mother_education\"] for row in unique_mother_edu_df.toLocalIterator()]\n",
    "list_father_edu = [row[\"father_education\"] for row in unique_father_edu_df.toLocalIterator()]\n",
    "\n",
    "# Thêm cột cho mỗi thể loại và gán giá trị 1 nếu thể loại đó xuất hiện, ngược lại gán giá trị 0\n",
    "for mother_edu in list_mother_edu:\n",
    "    mother_edu_column_name = f\"mother_education_is_{mother_edu.replace(' ', '_')}\"\n",
    "    df = df.withColumn(mother_edu_column_name, col(\"mother_education\").contains(mother_edu).cast(\"int\"))\n",
    "\n",
    "for father_edu in list_father_edu:\n",
    "    father_edu_column_name = f\"father_education_is_{father_edu.replace(' ', '_')}\"\n",
    "    df = df.withColumn(father_edu_column_name, col(\"father_education\").contains(father_edu).cast(\"int\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bc794c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+\n",
      "|mother_education             |father_education             |mother_education_is_5th_to_9th_grade|mother_education_is_none|mother_education_is_secondary_education|mother_education_is_primary_education_(4th_grade)|mother_education_is_higher_education|father_education_is_5th_to_9th_grade|father_education_is_none|father_education_is_secondary_education|father_education_is_primary_education_(4th_grade)|father_education_is_higher_education|\n",
      "+-----------------------------+-----------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+\n",
      "|higher education             |higher education             |0                                   |0                       |0                                      |0                                                |1                                   |0                                   |0                       |0                                      |0                                                |1                                   |\n",
      "|primary education (4th grade)|primary education (4th grade)|0                                   |0                       |0                                      |1                                                |0                                   |0                                   |0                       |0                                      |1                                                |0                                   |\n",
      "|primary education (4th grade)|primary education (4th grade)|0                                   |0                       |0                                      |1                                                |0                                   |0                                   |0                       |0                                      |1                                                |0                                   |\n",
      "|higher education             |5th to 9th grade             |0                                   |0                       |0                                      |0                                                |1                                   |1                                   |0                       |0                                      |0                                                |0                                   |\n",
      "|secondary education          |secondary education          |0                                   |0                       |1                                      |0                                                |0                                   |0                                   |0                       |1                                      |0                                                |0                                   |\n",
      "|higher education             |secondary education          |0                                   |0                       |0                                      |0                                                |1                                   |0                                   |0                       |1                                      |0                                                |0                                   |\n",
      "|5th to 9th grade             |5th to 9th grade             |1                                   |0                       |0                                      |0                                                |0                                   |1                                   |0                       |0                                      |0                                                |0                                   |\n",
      "|higher education             |higher education             |0                                   |0                       |0                                      |0                                                |1                                   |0                                   |0                       |0                                      |0                                                |1                                   |\n",
      "|secondary education          |5th to 9th grade             |0                                   |0                       |1                                      |0                                                |0                                   |1                                   |0                       |0                                      |0                                                |0                                   |\n",
      "|secondary education          |higher education             |0                                   |0                       |1                                      |0                                                |0                                   |0                                   |0                       |0                                      |0                                                |1                                   |\n",
      "|higher education             |higher education             |0                                   |0                       |0                                      |0                                                |1                                   |0                                   |0                       |0                                      |0                                                |1                                   |\n",
      "|5th to 9th grade             |primary education (4th grade)|1                                   |0                       |0                                      |0                                                |0                                   |0                                   |0                       |0                                      |1                                                |0                                   |\n",
      "|higher education             |higher education             |0                                   |0                       |0                                      |0                                                |1                                   |0                                   |0                       |0                                      |0                                                |1                                   |\n",
      "|higher education             |secondary education          |0                                   |0                       |0                                      |0                                                |1                                   |0                                   |0                       |1                                      |0                                                |0                                   |\n",
      "|5th to 9th grade             |5th to 9th grade             |1                                   |0                       |0                                      |0                                                |0                                   |1                                   |0                       |0                                      |0                                                |0                                   |\n",
      "|higher education             |higher education             |0                                   |0                       |0                                      |0                                                |1                                   |0                                   |0                       |0                                      |0                                                |1                                   |\n",
      "|higher education             |higher education             |0                                   |0                       |0                                      |0                                                |1                                   |0                                   |0                       |0                                      |0                                                |1                                   |\n",
      "|secondary education          |secondary education          |0                                   |0                       |1                                      |0                                                |0                                   |0                                   |0                       |1                                      |0                                                |0                                   |\n",
      "|secondary education          |5th to 9th grade             |0                                   |0                       |1                                      |0                                                |0                                   |1                                   |0                       |0                                      |0                                                |0                                   |\n",
      "|higher education             |secondary education          |0                                   |0                       |0                                      |0                                                |1                                   |0                                   |0                       |1                                      |0                                                |0                                   |\n",
      "+-----------------------------+-----------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.373183 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.666768 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 32 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 47 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 47 (MapPartitionsRDD[216] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_39 stored as values in memory (estimated size 53.7 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_39_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_39_piece0 in memory on 192.168.1.24:39111 (size: 19.8 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 39 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[216] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 47.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 47.0 (TID 45) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 47.0 (TID 45)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 47.0 (TID 45)\n",
      "[Executor task launch worker for task 0.0 in stage 47.0 (TID 45)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 47.0 (TID 45)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.33965 ms\n",
      "[Executor task launch worker for task 0.0 in stage 47.0 (TID 45)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 47.0 (TID 45)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 47.0 (TID 45)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 47.0 (TID 45). 2634 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 47.0 (TID 45) in 21 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 47 (showString at NativeMethodAccessorImpl.java:0) finished in 0.027 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 47: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 32 finished: showString at NativeMethodAccessorImpl.java:0, took 0.029410 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.401709 ms\n"
     ]
    }
   ],
   "source": [
    "# Chọn cột \"mother_education\" và các cột bắt đầu từ \"5th to 9th grade\" trở đi và hiển thị\n",
    "df.select([\"mother_education\", \"father_education\"] + [col(column) for column in df.columns[df.columns.index(\"mother_education_is_5th_to_9th_grade\"):]]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2da8c6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tiến hành xóa sau khi xử lý xong\n",
    "df = df.drop(\"mother_education\", \"father_education\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1911b",
   "metadata": {},
   "source": [
    "#### 1.2. school_choice_reason, guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13c3c44b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.829756 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.371597 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 33 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 48 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 48 (MapPartitionsRDD[226] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_40 stored as values in memory (estimated size 39.8 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_40_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_40_piece0 in memory on 192.168.1.24:39111 (size: 16.6 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 40 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[226] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 48.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 48.0 (TID 46) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 48.0 (TID 46)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 48.0 (TID 46)\n",
      "[Executor task launch worker for task 0.0 in stage 48.0 (TID 46)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 48.0 (TID 46)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.218295 ms\n",
      "[Executor task launch worker for task 0.0 in stage 48.0 (TID 46)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 48.0 (TID 46)\n",
      "[rdd_35_0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|school_choice_reason|guardian|\n",
      "+--------------------+--------+\n",
      "|course              |mother  |\n",
      "|course              |father  |\n",
      "|other               |mother  |\n",
      "|home                |mother  |\n",
      "|home                |father  |\n",
      "|reputation          |mother  |\n",
      "|home                |mother  |\n",
      "|home                |mother  |\n",
      "|home                |mother  |\n",
      "|home                |mother  |\n",
      "|reputation          |mother  |\n",
      "|reputation          |father  |\n",
      "|course              |father  |\n",
      "|course              |mother  |\n",
      "|home                |other   |\n",
      "|home                |mother  |\n",
      "|reputation          |mother  |\n",
      "|reputation          |mother  |\n",
      "|course              |mother  |\n",
      "|home                |father  |\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 48.0 (TID 46)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 48.0 (TID 46). 2174 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 48.0 (TID 46) in 10 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 48.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 48 (showString at NativeMethodAccessorImpl.java:0) finished in 0.014 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 48: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 33 finished: showString at NativeMethodAccessorImpl.java:0, took 0.016299 s\n"
     ]
    }
   ],
   "source": [
    "df.select(\"school_choice_reason\", \"guardian\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6732e1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 235 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 14\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 34 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 49 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 49 (MapPartitionsRDD[235] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_41 stored as values in memory (estimated size 49.8 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_41_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_41_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 41 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[235] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 49.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 49.0 (TID 47) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 49.0 (TID 48) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 49.0 (TID 47)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 49.0 (TID 47)\n",
      "[Executor task launch worker for task 1.0 in stage 49.0 (TID 48)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 49.0 (TID 48)\n",
      "[Executor task launch worker for task 0.0 in stage 49.0 (TID 47)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 49.0 (TID 48)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 49.0 (TID 47)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 49.0 (TID 47). 2917 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 49.0 (TID 47) in 13 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 49.0 (TID 48)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 49.0 (TID 48). 2917 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 49.0 (TID 48) in 16 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 49 (showString at NativeMethodAccessorImpl.java:0) finished in 0.020 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(14), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 35 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 51 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 50)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 51 (MapPartitionsRDD[238] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_42 stored as values in memory (estimated size 63.3 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_42_piece0 stored as bytes in memory (estimated size 26.4 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_42_piece0 in memory on 192.168.1.24:39111 (size: 26.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 42 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[238] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 51.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 51.0 (TID 49) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 51.0 (TID 49)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 51.0 (TID 49)\n",
      "[Executor task launch worker for task 0.0 in stage 51.0 (TID 49)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (592.0 B) non-empty blocks including 2 (592.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 51.0 (TID 49)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 51.0 (TID 49)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 51.0 (TID 49). 5376 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 51.0 (TID 49) in 12 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 51.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 51 (showString at NativeMethodAccessorImpl.java:0) finished in 0.017 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 51: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 35 finished: showString at NativeMethodAccessorImpl.java:0, took 0.019922 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|school_choice_reason|\n",
      "+--------------------+\n",
      "|reputation          |\n",
      "|course              |\n",
      "|other               |\n",
      "|home                |\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 247 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 15\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 36 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 52 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 52 (MapPartitionsRDD[247] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_43 stored as values in memory (estimated size 49.8 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_43_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_43_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 43 from broadcast at DAGScheduler.scala:1585\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_41_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 52 (MapPartitionsRDD[247] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 52.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 52.0 (TID 50) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 52.0 (TID 51) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 52.0 (TID 50)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 52.0 (TID 50)\n",
      "[Executor task launch worker for task 1.0 in stage 52.0 (TID 51)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 52.0 (TID 51)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_39_piece0 on 192.168.1.24:39111 in memory (size: 19.8 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_40_piece0 on 192.168.1.24:39111 in memory (size: 16.6 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_42_piece0 on 192.168.1.24:39111 in memory (size: 26.4 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 52.0 (TID 51)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 52.0 (TID 50)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 52.0 (TID 51)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 52.0 (TID 51). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 52.0 (TID 50)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 52.0 (TID 50). 2917 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 52.0 (TID 50) in 26 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 52.0 (TID 51) in 25 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 52 (showString at NativeMethodAccessorImpl.java:0) finished in 0.048 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(15), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|guardian|\n",
      "+--------+\n",
      "|father  |\n",
      "|mother  |\n",
      "|other   |\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 37 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 54 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 53)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 54 (MapPartitionsRDD[250] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_44 stored as values in memory (estimated size 63.3 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_44_piece0 stored as bytes in memory (estimated size 26.4 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_44_piece0 in memory on 192.168.1.24:39111 (size: 26.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 44 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[250] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 54.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 54.0 (TID 52) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 54.0 (TID 52)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 54.0 (TID 52)\n",
      "[Executor task launch worker for task 0.0 in stage 54.0 (TID 52)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (320.0 B) non-empty blocks including 2 (320.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 54.0 (TID 52)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "[Executor task launch worker for task 0.0 in stage 54.0 (TID 52)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 54.0 (TID 52). 5335 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 54.0 (TID 52) in 14 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 54 (showString at NativeMethodAccessorImpl.java:0) finished in 0.021 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 54: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 37 finished: showString at NativeMethodAccessorImpl.java:0, took 0.022987 s\n"
     ]
    }
   ],
   "source": [
    "# Lấy tất cả các giá trị duy nhất\n",
    "unique_school_choice_reason_df = df.select(\"school_choice_reason\").distinct()\n",
    "unique_guardian_df = df.select(\"guardian\").distinct()\n",
    "\n",
    "unique_school_choice_reason_df.show(truncate=False)\n",
    "unique_guardian_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "676438e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 259 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2) as input to shuffle 16\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 38 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 55 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 55 (MapPartitionsRDD[259] at toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_45 stored as values in memory (estimated size 49.8 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_45_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_45_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 45 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[259] at toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 55.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 55.0 (TID 53) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 55.0 (TID 54) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 55.0 (TID 54)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 55.0 (TID 54)\n",
      "[Executor task launch worker for task 0.0 in stage 55.0 (TID 53)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 55.0 (TID 53)\n",
      "[Executor task launch worker for task 1.0 in stage 55.0 (TID 54)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 55.0 (TID 53)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 55.0 (TID 54)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 55.0 (TID 54). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 55.0 (TID 53)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 55.0 (TID 53). 2917 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 55.0 (TID 54) in 14 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 55.0 (TID 53) in 14 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 55 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2) finished in 0.019 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 39 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 57 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 56)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 57 (MapPartitionsRDD[264] at toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_46 stored as values in memory (estimated size 66.2 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_46_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_46_piece0 in memory on 192.168.1.24:39111 (size: 27.7 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 46 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[264] at toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 57.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 57.0 (TID 55) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 57.0 (TID 55)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 57.0 (TID 55)\n",
      "[Executor task launch worker for task 0.0 in stage 57.0 (TID 55)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (592.0 B) non-empty blocks including 2 (592.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 57.0 (TID 55)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 57.0 (TID 55)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 57.0 (TID 55). 5829 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 57.0 (TID 55) in 13 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 57.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 57 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:2) finished in 0.018 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 57: Stage finished\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 273 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3) as input to shuffle 17\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 40 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 58 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 58 (MapPartitionsRDD[273] at toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_47 stored as values in memory (estimated size 49.8 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_47_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_47_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 47 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[273] at toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 58.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 58.0 (TID 56) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 58.0 (TID 57) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 58.0 (TID 56)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 58.0 (TID 56)\n",
      "[Executor task launch worker for task 1.0 in stage 58.0 (TID 57)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 58.0 (TID 57)\n",
      "[Executor task launch worker for task 1.0 in stage 58.0 (TID 57)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 58.0 (TID 56)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 58.0 (TID 56)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 58.0 (TID 56). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 58.0 (TID 57)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 58.0 (TID 57). 2917 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 58.0 (TID 56) in 18 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 58.0 (TID 57) in 18 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 58.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 58 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3) finished in 0.026 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(17), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 41 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 60 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 59)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 60 (MapPartitionsRDD[278] at toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_48 stored as values in memory (estimated size 66.2 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_48_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_48_piece0 in memory on 192.168.1.24:39111 (size: 27.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[278] at toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 60.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 60.0 (TID 58) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 60.0 (TID 58)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 60.0 (TID 58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 60.0 (TID 58)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (320.0 B) non-empty blocks including 2 (320.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 60.0 (TID 58)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 60.0 (TID 58)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 60.0 (TID 58). 5552 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 60.0 (TID 58) in 16 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 60.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 60 (toLocalIterator at /tmp/ipykernel_14509/3840655776.py:3) finished in 0.023 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 41 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 60: Stage finished\n"
     ]
    }
   ],
   "source": [
    "# Chuyển danh sách học vấn thành một danh sách Python\n",
    "list_school_choice = [row[\"school_choice_reason\"] for row in unique_school_choice_reason_df.toLocalIterator()]\n",
    "list_guardian = [row[\"guardian\"] for row in unique_guardian_df.toLocalIterator()]\n",
    "\n",
    "# Thêm cột cho mỗi thể loại và gán giá trị 1 nếu thể loại đó xuất hiện, ngược lại gán giá trị 0\n",
    "for school_choice in list_school_choice:\n",
    "    school_choice_column_name = f\"{school_choice.replace(' ', '_')}_school_choice\"\n",
    "    df = df.withColumn(school_choice_column_name, col(\"school_choice_reason\").contains(school_choice).cast(\"int\"))\n",
    "\n",
    "for guardian in list_guardian:\n",
    "    guardian_column_name = f\"{guardian.replace(' ', '_')}_guardian\"\n",
    "    df = df.withColumn(guardian_column_name, col(\"guardian\").contains(guardian).cast(\"int\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa7de1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+\n",
      "|school_choice_reason|guardian|reputation_school_choice|course_school_choice|other_school_choice|home_school_choice|father_guardian|mother_guardian|other_guardian|\n",
      "+--------------------+--------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+\n",
      "|course              |mother  |0                       |1                   |0                  |0                 |0              |1              |1             |\n",
      "|course              |father  |0                       |1                   |0                  |0                 |1              |0              |0             |\n",
      "|other               |mother  |0                       |0                   |1                  |0                 |0              |1              |1             |\n",
      "|home                |mother  |0                       |0                   |0                  |1                 |0              |1              |1             |\n",
      "|home                |father  |0                       |0                   |0                  |1                 |1              |0              |0             |\n",
      "|reputation          |mother  |1                       |0                   |0                  |0                 |0              |1              |1             |\n",
      "|home                |mother  |0                       |0                   |0                  |1                 |0              |1              |1             |\n",
      "|home                |mother  |0                       |0                   |0                  |1                 |0              |1              |1             |\n",
      "|home                |mother  |0                       |0                   |0                  |1                 |0              |1              |1             |\n",
      "|home                |mother  |0                       |0                   |0                  |1                 |0              |1              |1             |\n",
      "|reputation          |mother  |1                       |0                   |0                  |0                 |0              |1              |1             |\n",
      "|reputation          |father  |1                       |0                   |0                  |0                 |1              |0              |0             |\n",
      "|course              |father  |0                       |1                   |0                  |0                 |1              |0              |0             |\n",
      "|course              |mother  |0                       |1                   |0                  |0                 |0              |1              |1             |\n",
      "|home                |other   |0                       |0                   |0                  |1                 |0              |0              |1             |\n",
      "|home                |mother  |0                       |0                   |0                  |1                 |0              |1              |1             |\n",
      "|reputation          |mother  |1                       |0                   |0                  |0                 |0              |1              |1             |\n",
      "|reputation          |mother  |1                       |0                   |0                  |0                 |0              |1              |1             |\n",
      "|course              |mother  |0                       |1                   |0                  |0                 |0              |1              |1             |\n",
      "|home                |father  |0                       |0                   |0                  |1                 |1              |0              |0             |\n",
      "+--------------------+--------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.873383 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.213719 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 42 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 61 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 61 (MapPartitionsRDD[288] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_49 stored as values in memory (estimated size 49.7 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_49_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_49_piece0 in memory on 192.168.1.24:39111 (size: 19.1 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 49 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[288] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 61.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 61.0 (TID 59) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 61.0 (TID 59)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 61.0 (TID 59)\n",
      "[Executor task launch worker for task 0.0 in stage 61.0 (TID 59)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 61.0 (TID 59)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.271499 ms\n",
      "[Executor task launch worker for task 0.0 in stage 61.0 (TID 59)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 61.0 (TID 59)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 61.0 (TID 59)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 61.0 (TID 59). 2428 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 61.0 (TID 59) in 12 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 61.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 61 (showString at NativeMethodAccessorImpl.java:0) finished in 0.016 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 61: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 42 finished: showString at NativeMethodAccessorImpl.java:0, took 0.018061 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.581785 ms\n"
     ]
    }
   ],
   "source": [
    "df.select([\"school_choice_reason\", \"guardian\"] + [col(column) for column in df.columns[df.columns.index(\"reputation_school_choice\"):]]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bfe291d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tiến hành xóa sau khi xử lý xong\n",
    "df = df.drop(\"school_choice_reason\", \"guardian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fffac6",
   "metadata": {},
   "source": [
    "#### 1.4. study_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d63276df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|study_time   |\n",
      "+-------------+\n",
      "|2 to 5 hours |\n",
      "|2 to 5 hours |\n",
      "|2 to 5 hours |\n",
      "|5 to 10 hours|\n",
      "|2 to 5 hours |\n",
      "|2 to 5 hours |\n",
      "|2 to 5 hours |\n",
      "|2 to 5 hours |\n",
      "|2 to 5 hours |\n",
      "|2 to 5 hours |\n",
      "|2 to 5 hours |\n",
      "|5 to 10 hours|\n",
      "|<2 hours     |\n",
      "|2 to 5 hours |\n",
      "|5 to 10 hours|\n",
      "|<2 hours     |\n",
      "|5 to 10 hours|\n",
      "|2 to 5 hours |\n",
      "|<2 hours     |\n",
      "|<2 hours     |\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.693415 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.962793 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 43 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 62 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 62 (MapPartitionsRDD[298] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_50 stored as values in memory (estimated size 39.0 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_50_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_50_piece0 in memory on 192.168.1.24:39111 (size: 16.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 50 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[298] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 62.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 62.0 (TID 60) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 62.0 (TID 60)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 62.0 (TID 60)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_48_piece0 on 192.168.1.24:39111 in memory (size: 27.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_44_piece0 on 192.168.1.24:39111 in memory (size: 26.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_49_piece0 on 192.168.1.24:39111 in memory (size: 19.1 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_47_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 62.0 (TID 60)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_43_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_46_piece0 on 192.168.1.24:39111 in memory (size: 27.7 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_45_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.3 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 62.0 (TID 60)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.658514 ms\n",
      "[Executor task launch worker for task 0.0 in stage 62.0 (TID 60)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 62.0 (TID 60)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 62.0 (TID 60)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 62.0 (TID 60). 2086 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 62.0 (TID 60) in 12 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 62.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 62 (showString at NativeMethodAccessorImpl.java:0) finished in 0.024 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 62: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 43 finished: showString at NativeMethodAccessorImpl.java:0, took 0.026106 s\n"
     ]
    }
   ],
   "source": [
    "df.select(\"study_time\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13e130ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|study_time   |\n",
      "+-------------+\n",
      "|5 to 10 hours|\n",
      "|>10 hours    |\n",
      "|<2 hours     |\n",
      "|2 to 5 hours |\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 307 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 18\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 44 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 63 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 63 (MapPartitionsRDD[307] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_51 stored as values in memory (estimated size 49.8 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_51_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_51_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 51 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 63 (MapPartitionsRDD[307] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 63.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 63.0 (TID 61) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 63.0 (TID 62) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 63.0 (TID 61)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 63.0 (TID 61)\n",
      "[Executor task launch worker for task 1.0 in stage 63.0 (TID 62)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 63.0 (TID 62)\n",
      "[Executor task launch worker for task 1.0 in stage 63.0 (TID 62)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 63.0 (TID 61)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 63.0 (TID 62)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 63.0 (TID 62). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 63.0 (TID 61)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 63.0 (TID 61). 2917 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 63.0 (TID 62) in 13 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 63.0 (TID 61) in 14 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 63.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 63 (showString at NativeMethodAccessorImpl.java:0) finished in 0.019 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(18), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 45 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 65 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 64)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 65 (MapPartitionsRDD[310] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_52 stored as values in memory (estimated size 63.3 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_52_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_52_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 52 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[310] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 65.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 65.0 (TID 63) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 65.0 (TID 63)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 65.0 (TID 63)\n",
      "[Executor task launch worker for task 0.0 in stage 65.0 (TID 63)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (624.0 B) non-empty blocks including 2 (624.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 65.0 (TID 63)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 65.0 (TID 63)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 65.0 (TID 63). 5373 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 65.0 (TID 63) in 8 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 65.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 65 (showString at NativeMethodAccessorImpl.java:0) finished in 0.013 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 65: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 45 finished: showString at NativeMethodAccessorImpl.java:0, took 0.015529 s\n"
     ]
    }
   ],
   "source": [
    "# Lấy tất cả các giá trị duy nhất\n",
    "unique_study_time_df = df.select(\"study_time\").distinct()\n",
    "\n",
    "unique_study_time_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "315454bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|study_time   |study_time_encoded|\n",
      "+-------------+------------------+\n",
      "|2 to 5 hours |1                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|5 to 10 hours|2                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|5 to 10 hours|2                 |\n",
      "|<2 hours     |0                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|5 to 10 hours|2                 |\n",
      "|<2 hours     |0                 |\n",
      "|5 to 10 hours|2                 |\n",
      "|2 to 5 hours |1                 |\n",
      "|<2 hours     |0                 |\n",
      "|<2 hours     |0                 |\n",
      "+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.449475 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.304228 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 46 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 66 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 66 (MapPartitionsRDD[320] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_53 stored as values in memory (estimated size 43.1 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_53_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_53_piece0 in memory on 192.168.1.24:39111 (size: 17.7 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 53 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[320] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 66.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 66.0 (TID 64) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 66.0 (TID 64)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 66.0 (TID 64)\n",
      "[Executor task launch worker for task 0.0 in stage 66.0 (TID 64)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 66.0 (TID 64)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.148402 ms\n",
      "[Executor task launch worker for task 0.0 in stage 66.0 (TID 64)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 66.0 (TID 64)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 66.0 (TID 64)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 66.0 (TID 64). 2119 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 66.0 (TID 64) in 13 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 66.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 66 (showString at NativeMethodAccessorImpl.java:0) finished in 0.016 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 66: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 46 finished: showString at NativeMethodAccessorImpl.java:0, took 0.018274 s\n"
     ]
    }
   ],
   "source": [
    "# Chuyển đổi cột travel_time thành giá trị số\n",
    "\n",
    "df = df.withColumn(\"study_time_encoded\", \n",
    "                   when(col(\"study_time\") == \"<2 hours\", 0)\n",
    "                   .when(col(\"study_time\") == \"2 to 5 hours\", 1)\n",
    "                   .when(col(\"study_time\") == \"5 to 10 hours\", 2)\n",
    "                   .otherwise(3))\n",
    "\n",
    "\n",
    "df.select(\"study_time\",\"study_time_encoded\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b0d81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"study_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e1b0c9",
   "metadata": {},
   "source": [
    "### 2. Dạng thuộc tính binary (có 2 giá trị)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a70968",
   "metadata": {},
   "source": [
    "sex, address_type, family_size, parent_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f71cafca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|sex|address_type|\n",
      "+---+------------+\n",
      "|F  |Urban       |\n",
      "|F  |Urban       |\n",
      "|F  |Urban       |\n",
      "|F  |Urban       |\n",
      "|F  |Urban       |\n",
      "|M  |Urban       |\n",
      "|M  |Urban       |\n",
      "|F  |Urban       |\n",
      "|M  |Urban       |\n",
      "|M  |Urban       |\n",
      "|F  |Urban       |\n",
      "|F  |Urban       |\n",
      "|M  |Urban       |\n",
      "|M  |Urban       |\n",
      "|M  |Urban       |\n",
      "|F  |Urban       |\n",
      "|F  |Urban       |\n",
      "|F  |Urban       |\n",
      "|M  |Urban       |\n",
      "|M  |Urban       |\n",
      "+---+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.27332 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.97438 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 47 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 67 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 67 (MapPartitionsRDD[330] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_54 stored as values in memory (estimated size 39.8 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_54_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_54_piece0 in memory on 192.168.1.24:39111 (size: 16.6 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 54 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[330] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 67.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 67.0 (TID 65) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 67.0 (TID 65)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 67.0 (TID 65)\n",
      "[Executor task launch worker for task 0.0 in stage 67.0 (TID 65)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 67.0 (TID 65)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.298545 ms\n",
      "[Executor task launch worker for task 0.0 in stage 67.0 (TID 65)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 67.0 (TID 65)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 67.0 (TID 65)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 67.0 (TID 65). 2075 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 67.0 (TID 65) in 9 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 67.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 67 (showString at NativeMethodAccessorImpl.java:0) finished in 0.013 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 47 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 67: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 47 finished: showString at NativeMethodAccessorImpl.java:0, took 0.014819 s\n"
     ]
    }
   ],
   "source": [
    "columns = [\"sex\", \"address_type\"] \n",
    "\n",
    "df.select(columns).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "478d3812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 339 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 19\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 48 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 68 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 68 (MapPartitionsRDD[339] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_55 stored as values in memory (estimated size 49.8 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_55_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_55_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_53_piece0 on 192.168.1.24:39111 in memory (size: 17.7 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 55 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 68 (MapPartitionsRDD[339] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 68.0 with 2 tasks resource profile 0\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_52_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 68.0 (TID 66) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 68.0 (TID 67) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 68.0 (TID 66)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 68.0 (TID 66)\n",
      "[Executor task launch worker for task 1.0 in stage 68.0 (TID 67)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 68.0 (TID 67)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_51_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_54_piece0 on 192.168.1.24:39111 in memory (size: 16.6 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 68.0 (TID 67)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 68.0 (TID 66)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 68.0 (TID 67)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 68.0 (TID 67). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 68.0 (TID 66)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 68.0 (TID 66). 2917 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 68.0 (TID 66) in 17 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 68.0 (TID 67) in 17 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 68.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 68 (showString at NativeMethodAccessorImpl.java:0) finished in 0.027 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(19), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_50_piece0 on 192.168.1.24:39111 in memory (size: 16.4 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 49 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 70 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 69)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 70 (MapPartitionsRDD[342] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_56 stored as values in memory (estimated size 63.3 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_56_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_56_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 56 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[342] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 70.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 70.0 (TID 68) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 70.0 (TID 68)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 70.0 (TID 68)\n",
      "[Executor task launch worker for task 0.0 in stage 70.0 (TID 68)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 70.0 (TID 68)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 70.0 (TID 68)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 70.0 (TID 68). 5326 bytes result sent to driver\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|sex|\n",
      "+---+\n",
      "|F  |\n",
      "|M  |\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 70.0 (TID 68) in 10 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 70 (showString at NativeMethodAccessorImpl.java:0) finished in 0.014 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 70: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 49 finished: showString at NativeMethodAccessorImpl.java:0, took 0.016852 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 351 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 20\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 50 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 71 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 71 (MapPartitionsRDD[351] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_57 stored as values in memory (estimated size 49.8 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_57_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_57_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 57 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 71 (MapPartitionsRDD[351] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 71.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 71.0 (TID 69) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 71.0 (TID 70) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 71.0 (TID 69)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 71.0 (TID 69)\n",
      "[Executor task launch worker for task 1.0 in stage 71.0 (TID 70)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 71.0 (TID 70)\n",
      "[Executor task launch worker for task 0.0 in stage 71.0 (TID 69)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 71.0 (TID 70)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 71.0 (TID 69)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 71.0 (TID 69). 2917 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 71.0 (TID 69) in 11 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 71.0 (TID 70)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 71.0 (TID 70). 2917 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 71.0 (TID 70) in 14 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 71.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 71 (showString at NativeMethodAccessorImpl.java:0) finished in 0.019 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(20), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|address_type|\n",
      "+------------+\n",
      "|Urban       |\n",
      "|Rural       |\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 51 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 73 (showString at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 72)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 73 (MapPartitionsRDD[354] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_58 stored as values in memory (estimated size 63.3 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_58_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_58_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 58 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[354] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 73.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 73.0 (TID 71) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 73.0 (TID 71)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 73.0 (TID 71)\n",
      "[Executor task launch worker for task 0.0 in stage 73.0 (TID 71)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 73.0 (TID 71)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 73.0 (TID 71)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 73.0 (TID 71). 5329 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 73.0 (TID 71) in 13 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 73 (showString at NativeMethodAccessorImpl.java:0) finished in 0.019 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 73: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 51 finished: showString at NativeMethodAccessorImpl.java:0, took 0.024065 s\n"
     ]
    }
   ],
   "source": [
    "for col in columns:\n",
    "    df.select(col).distinct().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3de501c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 363 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 21\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 52 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 74 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 74 (MapPartitionsRDD[363] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_59 stored as values in memory (estimated size 49.8 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_59_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_59_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 59 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 74 (MapPartitionsRDD[363] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 74.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 74.0 (TID 72) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 74.0 (TID 73) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 74.0 (TID 73)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 74.0 (TID 73)\n",
      "[Executor task launch worker for task 0.0 in stage 74.0 (TID 72)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 74.0 (TID 72)\n",
      "[Executor task launch worker for task 1.0 in stage 74.0 (TID 73)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 74.0 (TID 72)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 74.0 (TID 73)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 74.0 (TID 73). 2917 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 74.0 (TID 73) in 12 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 74.0 (TID 72)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 74.0 (TID 72). 2917 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 74.0 (TID 72) in 15 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 74.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 74 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.020 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(21), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 53 (toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 76 (toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 75)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 76 (PythonRDD[369] at toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_60 stored as values in memory (estimated size 68.7 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_60_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_60_piece0 in memory on 192.168.1.24:39111 (size: 29.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 60 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 76 (PythonRDD[369] at toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 76.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 76.0 (TID 74) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 76.0 (TID 74)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 76.0 (TID 74)\n",
      "[Executor task launch worker for task 0.0 in stage 76.0 (TID 74)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 76.0 (TID 74)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 76.0 (TID 74)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 677, boot = 581, init = 95, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 76.0 (TID 74)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 76.0 (TID 74). 5540 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 76.0 (TID 74) in 694 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 76.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.api.python.PythonAccumulatorV2 - Connected to AccumulatorServer at host: 127.0.0.1 port: 42387\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 76 (toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5) finished in 0.703 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 53 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 76: Stage finished\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 378 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 22\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 54 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 77 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 77 (MapPartitionsRDD[378] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_61 stored as values in memory (estimated size 49.8 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_61_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_61_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 61 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 77 (MapPartitionsRDD[378] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 77.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 77.0 (TID 75) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 77.0 (TID 76) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 77.0 (TID 76)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 77.0 (TID 76)\n",
      "[Executor task launch worker for task 0.0 in stage 77.0 (TID 75)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 77.0 (TID 75)\n",
      "[Executor task launch worker for task 1.0 in stage 77.0 (TID 76)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 77.0 (TID 75)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 77.0 (TID 76)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 77.0 (TID 76). 2917 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 77.0 (TID 76) in 11 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 77.0 (TID 75)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 77.0 (TID 75). 2917 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 77.0 (TID 75) in 14 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 77 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.019 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(22), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column 'sex': ['M', 'F']\n",
      "Distinct values for column 'address_type': ['Urban', 'Rural']\n",
      "2D Array of distinct values:\n",
      "['M', 'F']\n",
      "['Urban', 'Rural']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 55 (toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 79 (toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 78)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 79 (PythonRDD[384] at toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_62 stored as values in memory (estimated size 68.7 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_62_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_62_piece0 in memory on 192.168.1.24:39111 (size: 29.5 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 62 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 79 (PythonRDD[384] at toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 79.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 79.0 (TID 77) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 79.0 (TID 77)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 79.0 (TID 77)\n",
      "[Executor task launch worker for task 0.0 in stage 79.0 (TID 77)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 79.0 (TID 77)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 79.0 (TID 77)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 99, boot = -126, init = 225, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 79.0 (TID 77)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 79.0 (TID 77). 5548 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 79.0 (TID 77) in 111 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 79.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 79 (toLocalIterator at /tmp/ipykernel_14509/4095349155.py:5) finished in 0.117 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 79: Stage finished\n"
     ]
    }
   ],
   "source": [
    "distinct_values = []\n",
    "\n",
    "# Lặp qua các cột và lưu các giá trị distinct vào mảng 2 chiều\n",
    "for column_name in columns:\n",
    "    distinct_set = set(df.select(column_name).distinct().rdd.flatMap(lambda x: x).toLocalIterator())\n",
    "    distinct_set.discard(None)\n",
    "    distinct_values.append(list(distinct_set))\n",
    "\n",
    "# Hiển thị kết quả\n",
    "for i, col in enumerate(columns):\n",
    "    print(f\"Distinct values for column '{col}': {distinct_values[i]}\")\n",
    "\n",
    "# In kết quả mảng 2 chiều\n",
    "print(\"2D Array of distinct values:\")\n",
    "for row in distinct_values:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9316b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_60_piece0 on 192.168.1.24:39111 in memory (size: 29.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_57_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_58_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_56_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_55_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_59_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_62_piece0 on 192.168.1.24:39111 in memory (size: 29.5 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_61_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.695866 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 396 (showString at <unknown>:0) as input to shuffle 23\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 56 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 81 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 80)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 80)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 80 (MapPartitionsRDD[396] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_63 stored as values in memory (estimated size 79.8 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_63_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_63_piece0 in memory on 192.168.1.24:39111 (size: 28.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 63 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 80 (MapPartitionsRDD[396] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 80.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 80.0 (TID 78) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 80.0 (TID 79) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 80.0 (TID 78)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 80.0 (TID 78)\n",
      "[Executor task launch worker for task 1.0 in stage 80.0 (TID 79)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 80.0 (TID 79)\n",
      "[Executor task launch worker for task 0.0 in stage 80.0 (TID 78)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 80.0 (TID 79)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 80.0 (TID 78)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 80.0 (TID 78). 3654 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 80.0 (TID 78) in 11 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 80.0 (TID 79)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 80.0 (TID 79). 3654 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 80.0 (TID 79) in 15 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 80.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 80 (showString at <unknown>:0) finished in 0.020 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 81)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 81 (MapPartitionsRDD[401] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_64 stored as values in memory (estimated size 81.3 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_64_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_64_piece0 in memory on 192.168.1.24:39111 (size: 30.0 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 64 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[401] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 81.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 81.0 (TID 80) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 81.0 (TID 80)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 81.0 (TID 80)\n",
      "[Executor task launch worker for task 0.0 in stage 81.0 (TID 80)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (5.9 KiB) non-empty blocks including 2 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 81.0 (TID 80)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 81.0 (TID 80)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.434116 ms\n",
      "[Executor task launch worker for task 0.0 in stage 81.0 (TID 80)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 81.0 (TID 80). 7125 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 81.0 (TID 80) in 40 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 81.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 81 (showString at <unknown>:0) finished in 0.046 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 56 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 81: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 56 finished: showString at <unknown>:0, took 0.068820 s\n",
      "\r",
      "                                                                                \r",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.257993 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "|sex|age|address_type|class_failures|school_support|family_support|extra_paid_classes|activities|higher_ed|internet_access|romantic_relationship|family_relationship|free_time|social|weekday_alcohol|weekend_alcohol|health|absences|grade_1|grade_2|final_grade|subject_code| id|mother_education_is_5th_to_9th_grade|mother_education_is_none|mother_education_is_secondary_education|mother_education_is_primary_education_(4th_grade)|mother_education_is_higher_education|father_education_is_5th_to_9th_grade|father_education_is_none|father_education_is_secondary_education|father_education_is_primary_education_(4th_grade)|father_education_is_higher_education|reputation_school_choice|course_school_choice|other_school_choice|home_school_choice|father_guardian|mother_guardian|other_guardian|study_time_encoded|sex_is_M|address_type_is_Urban|\n",
      "+---+---+------------+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "|  F| 18|       Urban|             0|           yes|            no|                no|        no|      yes|             no|                   no|                  4|        3|     4|              1|              1|     3|       6|      5|      6|          6|           1|  1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   1|                  0|                 0|              0|              1|             1|                 1|       0|                    1|\n",
      "|  F| 17|       Urban|             0|            no|           yes|                no|        no|      yes|            yes|                   no|                  5|        3|     3|              1|              1|     3|       4|      5|      5|          6|           1|  2|                                   0|                       0|                                      0|                                                1|                                   0|                                   0|                       0|                                      0|                                                1|                                   0|                       0|                   1|                  0|                 0|              1|              0|             0|                 1|       0|                    1|\n",
      "|  F| 15|       Urban|             3|           yes|            no|               yes|        no|      yes|            yes|                   no|                  4|        3|     2|              2|              3|     3|      10|      7|      8|         10|           1|  3|                                   0|                       0|                                      0|                                                1|                                   0|                                   0|                       0|                                      0|                                                1|                                   0|                       0|                   0|                  1|                 0|              0|              1|             1|                 1|       0|                    1|\n",
      "|  F| 15|       Urban|             0|            no|           yes|               yes|       yes|      yes|            yes|                  yes|                  3|        2|     2|              1|              1|     5|       2|     15|     14|         15|           1|  4|                                   0|                       0|                                      0|                                                0|                                   1|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   0|                  0|                 1|              0|              1|             1|                 2|       0|                    1|\n",
      "|  F| 16|       Urban|             0|            no|           yes|               yes|        no|      yes|             no|                   no|                  4|        3|     2|              1|              2|     5|       4|      6|     10|         10|           1|  5|                                   0|                       0|                                      1|                                                0|                                   0|                                   0|                       0|                                      1|                                                0|                                   0|                       0|                   0|                  0|                 1|              1|              0|             0|                 1|       0|                    1|\n",
      "|  M| 16|       Urban|             0|            no|           yes|               yes|       yes|      yes|            yes|                   no|                  5|        4|     2|              1|              2|     5|      10|     15|     15|         15|           1|  6|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      1|                                                0|                                   0|                       1|                   0|                  0|                 0|              0|              1|             1|                 1|       1|                    1|\n",
      "|  M| 16|       Urban|             0|            no|            no|                no|        no|      yes|            yes|                   no|                  4|        4|     4|              1|              1|     3|       0|     12|     12|         11|           1|  7|                                   1|                       0|                                      0|                                                0|                                   0|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   0|                  0|                 1|              0|              1|             1|                 1|       1|                    1|\n",
      "|  F| 17|       Urban|             0|           yes|           yes|                no|        no|      yes|             no|                   no|                  4|        1|     4|              1|              1|     1|       6|      6|      5|          6|           1|  8|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   0|                  0|                 1|              0|              1|             1|                 1|       0|                    1|\n",
      "|  M| 15|       Urban|             0|            no|           yes|               yes|        no|      yes|            yes|                   no|                  4|        2|     2|              1|              1|     1|       0|     16|     18|         19|           1|  9|                                   0|                       0|                                      1|                                                0|                                   0|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   0|                  0|                 1|              0|              1|             1|                 1|       1|                    1|\n",
      "|  M| 15|       Urban|             0|            no|           yes|               yes|       yes|      yes|            yes|                   no|                  5|        5|     1|              1|              1|     5|       0|     14|     15|         15|           1| 10|                                   0|                       0|                                      1|                                                0|                                   0|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   0|                  0|                 1|              0|              1|             1|                 1|       1|                    1|\n",
      "|  F| 15|       Urban|             0|            no|           yes|               yes|        no|      yes|            yes|                   no|                  3|        3|     3|              1|              2|     2|       0|     10|      8|          9|           1| 11|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       1|                   0|                  0|                 0|              0|              1|             1|                 1|       0|                    1|\n",
      "|  F| 15|       Urban|             0|            no|           yes|                no|       yes|      yes|            yes|                   no|                  5|        2|     2|              1|              1|     4|       4|     10|     12|         12|           1| 12|                                   1|                       0|                                      0|                                                0|                                   0|                                   0|                       0|                                      0|                                                1|                                   0|                       1|                   0|                  0|                 0|              1|              0|             0|                 2|       0|                    1|\n",
      "|  M| 15|       Urban|             0|            no|           yes|               yes|       yes|      yes|            yes|                   no|                  4|        3|     3|              1|              3|     5|       2|     14|     14|         14|           1| 13|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   1|                  0|                 0|              1|              0|             0|                 0|       1|                    1|\n",
      "|  M| 15|       Urban|             0|            no|           yes|               yes|        no|      yes|            yes|                   no|                  5|        4|     3|              1|              2|     3|       2|     10|     10|         11|           1| 14|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      1|                                                0|                                   0|                       0|                   1|                  0|                 0|              0|              1|             1|                 1|       1|                    1|\n",
      "|  M| 15|       Urban|             0|            no|           yes|                no|        no|      yes|            yes|                  yes|                  4|        5|     2|              1|              1|     3|       0|     14|     16|         16|           1| 15|                                   1|                       0|                                      0|                                                0|                                   0|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   0|                  0|                 1|              0|              0|             1|                 2|       1|                    1|\n",
      "|  F| 16|       Urban|             0|            no|           yes|                no|        no|      yes|            yes|                   no|                  4|        4|     4|              1|              2|     2|       4|     14|     14|         14|           1| 16|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   0|                  0|                 1|              0|              1|             1|                 0|       0|                    1|\n",
      "|  F| 16|       Urban|             0|            no|           yes|               yes|       yes|      yes|            yes|                   no|                  3|        2|     3|              1|              2|     2|       6|     13|     14|         14|           1| 17|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       1|                   0|                  0|                 0|              0|              1|             1|                 2|       0|                    1|\n",
      "|  F| 16|       Urban|             0|           yes|           yes|                no|       yes|      yes|             no|                   no|                  5|        3|     2|              1|              1|     4|       4|      8|     10|         10|           1| 18|                                   0|                       0|                                      1|                                                0|                                   0|                                   0|                       0|                                      1|                                                0|                                   0|                       1|                   0|                  0|                 0|              0|              1|             1|                 1|       0|                    1|\n",
      "|  M| 17|       Urban|             3|            no|           yes|                no|       yes|      yes|            yes|                   no|                  5|        5|     5|              2|              4|     5|      16|      6|      5|          5|           1| 19|                                   0|                       0|                                      1|                                                0|                                   0|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   1|                  0|                 0|              0|              1|             1|                 0|       1|                    1|\n",
      "|  M| 16|       Urban|             0|            no|            no|               yes|       yes|      yes|            yes|                   no|                  3|        1|     3|              1|              3|     5|       4|      8|     10|         10|           1| 20|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      1|                                                0|                                   0|                       0|                   0|                  0|                 1|              1|              0|             0|                 0|       1|                    1|\n",
      "+---+---+------------+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, col_name in enumerate(columns):\n",
    "    first_value = distinct_values[i][0]\n",
    "    first_value = first_value.replace(' ', '_')\n",
    "    new_col_name = col_name + \"_is_\" + first_value\n",
    "    df = df.withColumn(new_col_name, when(df[col_name] == first_value, 1).otherwise(0))\n",
    "\n",
    "# Hiển thị DataFrame kết quả\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0766139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+---------------------+\n",
      "|sex|address_type|sex_is_M|address_type_is_Urban|\n",
      "+---+------------+--------+---------------------+\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  M|       Urban|       1|                    1|\n",
      "|  M|       Urban|       1|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  M|       Urban|       1|                    1|\n",
      "|  M|       Urban|       1|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  M|       Urban|       1|                    1|\n",
      "|  M|       Urban|       1|                    1|\n",
      "|  M|       Urban|       1|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  F|       Urban|       0|                    1|\n",
      "|  M|       Urban|       1|                    1|\n",
      "|  M|       Urban|       1|                    1|\n",
      "+---+------------+--------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.803679 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.473462 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 57 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 82 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 82 (MapPartitionsRDD[411] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_65 stored as values in memory (estimated size 43.4 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_65_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_65_piece0 in memory on 192.168.1.24:39111 (size: 17.7 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 65 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[411] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 82.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 82.0 (TID 81) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 82.0 (TID 81)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 82.0 (TID 81)\n",
      "[Executor task launch worker for task 0.0 in stage 82.0 (TID 81)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 82.0 (TID 81)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.971716 ms\n",
      "[Executor task launch worker for task 0.0 in stage 82.0 (TID 81)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 82.0 (TID 81)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 82.0 (TID 81)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 82.0 (TID 81). 2103 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 82.0 (TID 81) in 16 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 82.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 82 (showString at <unknown>:0) finished in 0.022 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 82: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 57 finished: showString at <unknown>:0, took 0.023712 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.520423 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(\"sex\", \"address_type\", \"sex_is_M\", \"address_type_is_Urban\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab927f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xóa cột release_date\n",
    "df = df.drop(\"sex\", \"address_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce9a7e",
   "metadata": {},
   "source": [
    "### Yes No \n",
    "\"school_support\", \"family_support\", \"extra_paid_classes\", \"activities\", \"higher_ed\", \"internet_access\", \"romantic_relationship\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18daedda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+------------------+----------+---------+---------------+---------------------+\n",
      "|school_support|family_support|extra_paid_classes|activities|higher_ed|internet_access|romantic_relationship|\n",
      "+--------------+--------------+------------------+----------+---------+---------------+---------------------+\n",
      "|yes           |no            |no                |no        |yes      |no             |no                   |\n",
      "|no            |yes           |no                |no        |yes      |yes            |no                   |\n",
      "|yes           |no            |yes               |no        |yes      |yes            |no                   |\n",
      "|no            |yes           |yes               |yes       |yes      |yes            |yes                  |\n",
      "|no            |yes           |yes               |no        |yes      |no             |no                   |\n",
      "|no            |yes           |yes               |yes       |yes      |yes            |no                   |\n",
      "|no            |no            |no                |no        |yes      |yes            |no                   |\n",
      "|yes           |yes           |no                |no        |yes      |no             |no                   |\n",
      "|no            |yes           |yes               |no        |yes      |yes            |no                   |\n",
      "|no            |yes           |yes               |yes       |yes      |yes            |no                   |\n",
      "|no            |yes           |yes               |no        |yes      |yes            |no                   |\n",
      "|no            |yes           |no                |yes       |yes      |yes            |no                   |\n",
      "|no            |yes           |yes               |yes       |yes      |yes            |no                   |\n",
      "|no            |yes           |yes               |no        |yes      |yes            |no                   |\n",
      "|no            |yes           |no                |no        |yes      |yes            |yes                  |\n",
      "|no            |yes           |no                |no        |yes      |yes            |no                   |\n",
      "|no            |yes           |yes               |yes       |yes      |yes            |no                   |\n",
      "|yes           |yes           |no                |yes       |yes      |no             |no                   |\n",
      "|no            |yes           |no                |yes       |yes      |yes            |no                   |\n",
      "|no            |no            |yes               |yes       |yes      |yes            |no                   |\n",
      "+--------------+--------------+------------------+----------+---------+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.626759 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.833707 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 58 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 83 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 83 (MapPartitionsRDD[421] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_66 stored as values in memory (estimated size 43.7 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_66_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_66_piece0 in memory on 192.168.1.24:39111 (size: 17.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 66 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 83 (MapPartitionsRDD[421] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 83.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 83.0 (TID 82) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 83.0 (TID 82)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 83.0 (TID 82)\n",
      "[Executor task launch worker for task 0.0 in stage 83.0 (TID 82)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 83.0 (TID 82)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.300035 ms\n",
      "[Executor task launch worker for task 0.0 in stage 83.0 (TID 82)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.65715 ms\n",
      "[Executor task launch worker for task 0.0 in stage 83.0 (TID 82)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 83.0 (TID 82)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 83.0 (TID 82)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 83.0 (TID 82). 2573 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 83.0 (TID 82) in 17 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 83.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 83 (showString at <unknown>:0) finished in 0.021 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 83: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 58 finished: showString at <unknown>:0, took 0.023348 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.456103 ms\n"
     ]
    }
   ],
   "source": [
    "columns = [\"school_support\", \"family_support\",\n",
    "           \"extra_paid_classes\", \"activities\",\n",
    "           \"higher_ed\", \"internet_access\",\n",
    "           \"romantic_relationship\"]\n",
    "\n",
    "df.select(columns).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "405b8b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 430 (showString at <unknown>:0) as input to shuffle 24\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 59 (showString at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 84 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 84 (MapPartitionsRDD[430] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_67 stored as values in memory (estimated size 49.8 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_67_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_67_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 67 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 84 (MapPartitionsRDD[430] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 84.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 84.0 (TID 83) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 84.0 (TID 84) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 84.0 (TID 84)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 84.0 (TID 84)\n",
      "[Executor task launch worker for task 0.0 in stage 84.0 (TID 83)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 84.0 (TID 83)\n",
      "[Executor task launch worker for task 0.0 in stage 84.0 (TID 83)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 84.0 (TID 84)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 84.0 (TID 84)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 84.0 (TID 84). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 84.0 (TID 83)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 84.0 (TID 83). 2917 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 84.0 (TID 83) in 16 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 84.0 (TID 84) in 16 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 84.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 84 (showString at <unknown>:0) finished in 0.020 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(24), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 60 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 86 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 85)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 86 (MapPartitionsRDD[433] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_68 stored as values in memory (estimated size 63.3 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_68_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_68_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 68 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[433] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 86.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 86.0 (TID 85) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 86.0 (TID 85)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 86.0 (TID 85)\n",
      "[Executor task launch worker for task 0.0 in stage 86.0 (TID 85)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 86.0 (TID 85)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 86.0 (TID 85)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 86.0 (TID 85). 5330 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 86.0 (TID 85) in 8 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 86.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 86 (showString at <unknown>:0) finished in 0.012 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 86: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 60 finished: showString at <unknown>:0, took 0.013885 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|school_support|\n",
      "+--------------+\n",
      "|no            |\n",
      "|yes           |\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 442 (showString at <unknown>:0) as input to shuffle 25\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 61 (showString at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 87 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 87 (MapPartitionsRDD[442] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_69 stored as values in memory (estimated size 49.8 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_69_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_69_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 69 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 87 (MapPartitionsRDD[442] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 87.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 87.0 (TID 86) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 87.0 (TID 87) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 87.0 (TID 86)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 87.0 (TID 86)\n",
      "[Executor task launch worker for task 1.0 in stage 87.0 (TID 87)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 87.0 (TID 87)\n",
      "[Executor task launch worker for task 1.0 in stage 87.0 (TID 87)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 87.0 (TID 86)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_63_piece0 on 192.168.1.24:39111 in memory (size: 28.3 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_67_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_66_piece0 on 192.168.1.24:39111 in memory (size: 17.3 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 87.0 (TID 87)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 87.0 (TID 87). 2960 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 87.0 (TID 86)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 87.0 (TID 86). 2960 bytes result sent to driver\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_68_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 87.0 (TID 87) in 19 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 87.0 (TID 86) in 19 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 87.0, whose tasks have all completed, from pool \n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_65_piece0 on 192.168.1.24:39111 in memory (size: 17.7 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 87 (showString at <unknown>:0) finished in 0.029 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_64_piece0 on 192.168.1.24:39111 in memory (size: 30.0 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(25), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 62 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 89 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 88)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 89 (MapPartitionsRDD[445] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_70 stored as values in memory (estimated size 63.3 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_70_piece0 stored as bytes in memory (estimated size 26.4 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_70_piece0 in memory on 192.168.1.24:39111 (size: 26.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 70 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[445] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 89.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 89.0 (TID 88) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 89.0 (TID 88)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 89.0 (TID 88)\n",
      "[Executor task launch worker for task 0.0 in stage 89.0 (TID 88)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 89.0 (TID 88)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_69_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|family_support|\n",
      "+--------------+\n",
      "|no            |\n",
      "|yes           |\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 89.0 (TID 88)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 89.0 (TID 88). 5373 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 89.0 (TID 88) in 14 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 89.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 89 (showString at <unknown>:0) finished in 0.018 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 62 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 89: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 62 finished: showString at <unknown>:0, took 0.021340 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 454 (showString at <unknown>:0) as input to shuffle 26\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 63 (showString at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 90 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 90 (MapPartitionsRDD[454] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_71 stored as values in memory (estimated size 49.8 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_71_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_71_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 71 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 90 (MapPartitionsRDD[454] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 90.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 90.0 (TID 89) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 90.0 (TID 90) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 90.0 (TID 90)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 90.0 (TID 90)\n",
      "[Executor task launch worker for task 0.0 in stage 90.0 (TID 89)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 90.0 (TID 89)\n",
      "[Executor task launch worker for task 0.0 in stage 90.0 (TID 89)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 90.0 (TID 90)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 90.0 (TID 89)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 90.0 (TID 89). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 90.0 (TID 90)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 90.0 (TID 90). 2917 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 90.0 (TID 89) in 12 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 90.0 (TID 90) in 12 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 90.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 90 (showString at <unknown>:0) finished in 0.016 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(26), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 64 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 92 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 91)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 92 (MapPartitionsRDD[457] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_72 stored as values in memory (estimated size 63.3 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_72_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_72_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 72 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[457] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 92.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 92.0 (TID 91) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 92.0 (TID 91)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 92.0 (TID 91)\n",
      "[Executor task launch worker for task 0.0 in stage 92.0 (TID 91)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 92.0 (TID 91)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 92.0 (TID 91)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 92.0 (TID 91). 5330 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 92.0 (TID 91) in 8 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 92.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 92 (showString at <unknown>:0) finished in 0.014 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 64 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 92: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 64 finished: showString at <unknown>:0, took 0.017386 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|extra_paid_classes|\n",
      "+------------------+\n",
      "|no                |\n",
      "|yes               |\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 466 (showString at <unknown>:0) as input to shuffle 27\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 65 (showString at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 93 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 93 (MapPartitionsRDD[466] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_73 stored as values in memory (estimated size 49.8 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_73_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_73_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 73 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 93 (MapPartitionsRDD[466] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 93.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 93.0 (TID 92) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 93.0 (TID 93) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 93.0 (TID 92)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 93.0 (TID 92)\n",
      "[Executor task launch worker for task 1.0 in stage 93.0 (TID 93)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 93.0 (TID 93)\n",
      "[Executor task launch worker for task 0.0 in stage 93.0 (TID 92)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 93.0 (TID 93)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 93.0 (TID 92)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 93.0 (TID 92). 2917 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 93.0 (TID 92) in 10 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 93.0 (TID 93)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 93.0 (TID 93). 2917 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 93.0 (TID 93) in 13 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 93.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 93 (showString at <unknown>:0) finished in 0.017 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(27), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 66 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 95 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 94)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 95 (MapPartitionsRDD[469] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_74 stored as values in memory (estimated size 63.3 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_74_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_74_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 74 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[469] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 95.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 95.0 (TID 94) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 95.0 (TID 94)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 95.0 (TID 94)\n",
      "[Executor task launch worker for task 0.0 in stage 95.0 (TID 94)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 95.0 (TID 94)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 95.0 (TID 94)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 95.0 (TID 94). 5330 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 95.0 (TID 94) in 9 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 95.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 95 (showString at <unknown>:0) finished in 0.015 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 66 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 95: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 66 finished: showString at <unknown>:0, took 0.017470 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|activities|\n",
      "+----------+\n",
      "|no        |\n",
      "|yes       |\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 478 (showString at <unknown>:0) as input to shuffle 28\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 67 (showString at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 96 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 96 (MapPartitionsRDD[478] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_75 stored as values in memory (estimated size 49.8 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_75_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_75_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 75 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 96 (MapPartitionsRDD[478] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 96.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 96.0 (TID 95) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 96.0 (TID 96) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 96.0 (TID 96)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 96.0 (TID 96)\n",
      "[Executor task launch worker for task 0.0 in stage 96.0 (TID 95)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 96.0 (TID 95)\n",
      "[Executor task launch worker for task 0.0 in stage 96.0 (TID 95)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 96.0 (TID 96)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 96.0 (TID 95)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 96.0 (TID 95). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 96.0 (TID 96)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 96.0 (TID 96). 2917 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 96.0 (TID 96) in 15 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 96.0 (TID 95) in 15 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 96.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 96 (showString at <unknown>:0) finished in 0.021 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(28), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 68 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 98 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 97)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 98 (MapPartitionsRDD[481] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_76 stored as values in memory (estimated size 63.3 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_76_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_76_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 76 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[481] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 98.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 98.0 (TID 97) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 98.0 (TID 97)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 98.0 (TID 97)\n",
      "[Executor task launch worker for task 0.0 in stage 98.0 (TID 97)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 98.0 (TID 97)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 98.0 (TID 97)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 98.0 (TID 97). 5330 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 98.0 (TID 97) in 7 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 98.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 98 (showString at <unknown>:0) finished in 0.011 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 68 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 98: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 68 finished: showString at <unknown>:0, took 0.012974 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|higher_ed|\n",
      "+---------+\n",
      "|no       |\n",
      "|yes      |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 490 (showString at <unknown>:0) as input to shuffle 29\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 69 (showString at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 99 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 99 (MapPartitionsRDD[490] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_77 stored as values in memory (estimated size 49.8 KiB, free 433.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_77_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_77_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 77 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 99 (MapPartitionsRDD[490] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 99.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 99.0 (TID 98) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 99.0 (TID 99) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 99.0 (TID 98)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 99.0 (TID 98)\n",
      "[Executor task launch worker for task 1.0 in stage 99.0 (TID 99)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 99.0 (TID 99)\n",
      "[Executor task launch worker for task 0.0 in stage 99.0 (TID 98)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 99.0 (TID 99)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 99.0 (TID 98)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 99.0 (TID 98). 2917 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 99.0 (TID 98) in 10 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 99.0 (TID 99)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 99.0 (TID 99). 2917 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 99.0 (TID 99) in 12 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 99.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 99 (showString at <unknown>:0) finished in 0.017 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(29), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 70 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 101 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 100)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 101 (MapPartitionsRDD[493] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_78 stored as values in memory (estimated size 63.3 KiB, free 433.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_78_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_78_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 78 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 101 (MapPartitionsRDD[493] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 101.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 101.0 (TID 100) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 101.0 (TID 100)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 101.0 (TID 100)\n",
      "[Executor task launch worker for task 0.0 in stage 101.0 (TID 100)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 101.0 (TID 100)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 101.0 (TID 100)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 101.0 (TID 100). 5330 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 101.0 (TID 100) in 11 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 101.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 101 (showString at <unknown>:0) finished in 0.016 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 70 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 101: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 70 finished: showString at <unknown>:0, took 0.019090 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|internet_access|\n",
      "+---------------+\n",
      "|no             |\n",
      "|yes            |\n",
      "+---------------+\n",
      "\n",
      "+---------------------+\n",
      "|romantic_relationship|\n",
      "+---------------------+\n",
      "|no                   |\n",
      "|yes                  |\n",
      "+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_71_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_77_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_70_piece0 on 192.168.1.24:39111 in memory (size: 26.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_72_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_78_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_76_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_74_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_75_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_73_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 502 (showString at <unknown>:0) as input to shuffle 30\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 71 (showString at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 102 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 102 (MapPartitionsRDD[502] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_79 stored as values in memory (estimated size 49.8 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_79_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_79_piece0 in memory on 192.168.1.24:39111 (size: 20.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 79 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 102 (MapPartitionsRDD[502] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 102.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 102.0 (TID 101) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 102.0 (TID 102) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 102.0 (TID 102)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 102.0 (TID 102)\n",
      "[Executor task launch worker for task 0.0 in stage 102.0 (TID 101)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 102.0 (TID 101)\n",
      "[Executor task launch worker for task 1.0 in stage 102.0 (TID 102)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 102.0 (TID 101)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 102.0 (TID 101)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 102.0 (TID 101). 2917 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 102.0 (TID 102)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 102.0 (TID 102). 2917 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 102.0 (TID 101) in 13 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 102.0 (TID 102) in 13 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 102.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 102 (showString at <unknown>:0) finished in 0.017 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(30), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_79_piece0 on 192.168.1.24:39111 in memory (size: 20.4 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 72 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 104 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 103)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 104 (MapPartitionsRDD[505] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_80 stored as values in memory (estimated size 63.3 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_80_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_80_piece0 in memory on 192.168.1.24:39111 (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 80 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[505] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 104.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 104.0 (TID 103) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 104.0 (TID 103)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 104.0 (TID 103)\n",
      "[Executor task launch worker for task 0.0 in stage 104.0 (TID 103)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (288.0 B) non-empty blocks including 2 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 104.0 (TID 103)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 104.0 (TID 103)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 104.0 (TID 103). 5330 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 104.0 (TID 103) in 9 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 104.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 104 (showString at <unknown>:0) finished in 0.012 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 72 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 104: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 72 finished: showString at <unknown>:0, took 0.013341 s\n"
     ]
    }
   ],
   "source": [
    "for col in columns:\n",
    "    df.select(col).distinct().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93041b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Lặp qua các cột và thay thế giá trị \"yes\" bằng 1, ngược lại bằng 0\n",
    "for col_name in columns:\n",
    "    df = df.withColumn(col_name, when(col(col_name) == \"yes\", 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "994a2fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+------------------+----------+---------+---------------+---------------------+\n",
      "|school_support|family_support|extra_paid_classes|activities|higher_ed|internet_access|romantic_relationship|\n",
      "+--------------+--------------+------------------+----------+---------+---------------+---------------------+\n",
      "|             1|             0|                 0|         0|        1|              0|                    0|\n",
      "|             0|             1|                 0|         0|        1|              1|                    0|\n",
      "|             1|             0|                 1|         0|        1|              1|                    0|\n",
      "|             0|             1|                 1|         1|        1|              1|                    1|\n",
      "|             0|             1|                 1|         0|        1|              0|                    0|\n",
      "|             0|             1|                 1|         1|        1|              1|                    0|\n",
      "|             0|             0|                 0|         0|        1|              1|                    0|\n",
      "|             1|             1|                 0|         0|        1|              0|                    0|\n",
      "|             0|             1|                 1|         0|        1|              1|                    0|\n",
      "|             0|             1|                 1|         1|        1|              1|                    0|\n",
      "|             0|             1|                 1|         0|        1|              1|                    0|\n",
      "|             0|             1|                 0|         1|        1|              1|                    0|\n",
      "|             0|             1|                 1|         1|        1|              1|                    0|\n",
      "|             0|             1|                 1|         0|        1|              1|                    0|\n",
      "|             0|             1|                 0|         0|        1|              1|                    1|\n",
      "|             0|             1|                 0|         0|        1|              1|                    0|\n",
      "|             0|             1|                 1|         1|        1|              1|                    0|\n",
      "|             1|             1|                 0|         1|        1|              0|                    0|\n",
      "|             0|             1|                 0|         1|        1|              1|                    0|\n",
      "|             0|             0|                 1|         1|        1|              1|                    0|\n",
      "+--------------+--------------+------------------+----------+---------+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.938769 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.204843 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 73 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 105 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 105 (MapPartitionsRDD[515] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_81 stored as values in memory (estimated size 50.8 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_81_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_81_piece0 in memory on 192.168.1.24:39111 (size: 19.7 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 81 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[515] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 105.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 105.0 (TID 104) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 105.0 (TID 104)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 105.0 (TID 104)\n",
      "[Executor task launch worker for task 0.0 in stage 105.0 (TID 104)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 105.0 (TID 104)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.606796 ms\n",
      "[Executor task launch worker for task 0.0 in stage 105.0 (TID 104)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 105.0 (TID 104)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 105.0 (TID 104)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 105.0 (TID 104). 2320 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 105.0 (TID 104) in 15 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 105.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 105 (showString at <unknown>:0) finished in 0.018 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 73 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 105: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 73 finished: showString at <unknown>:0, took 0.019941 s\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị DataFrame kết quả\n",
    "df.select(columns).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ece9d5",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffeead94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 524 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 31\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 74 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 106 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 106 (MapPartitionsRDD[524] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_82 stored as values in memory (estimated size 39.3 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_82_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_82_piece0 in memory on 192.168.1.24:39111 (size: 16.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 82 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 106 (MapPartitionsRDD[524] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 106.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 106.0 (TID 105) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 106.0 (TID 106) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 106.0 (TID 106)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 106.0 (TID 106)\n",
      "[Executor task launch worker for task 0.0 in stage 106.0 (TID 105)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 106.0 (TID 105)\n",
      "[Executor task launch worker for task 0.0 in stage 106.0 (TID 105)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 106.0 (TID 106)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 106.0 (TID 105)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 106.0 (TID 105). 2354 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 106.0 (TID 105) in 7 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 106.0 (TID 106)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 106.0 (TID 106). 2354 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 106.0 (TID 106) in 7 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 106.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 106 (count at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 75 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 108 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 107)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 108 (MapPartitionsRDD[527] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_83 stored as values in memory (estimated size 12.5 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_83_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_83_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 83 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 108 (MapPartitionsRDD[527] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 108.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 108.0 (TID 107) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 108.0 (TID 107)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 108.0 (TID 107)\n",
      "[Executor task launch worker for task 0.0 in stage 108.0 (TID 107)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 108.0 (TID 107)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 108.0 (TID 107)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 108.0 (TID 107). 3995 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 108.0 (TID 107) in 4 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 108.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 108 (count at NativeMethodAccessorImpl.java:0) finished in 0.007 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 75 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 108: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 75 finished: count at NativeMethodAccessorImpl.java:0, took 0.009180 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số dòng: 1044\n",
      "Số cột: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.955799 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 539 (showString at <unknown>:0) as input to shuffle 32\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 76 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 110 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 109)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 109)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 109 (MapPartitionsRDD[539] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_84 stored as values in memory (estimated size 79.8 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_82_piece0 on 192.168.1.24:39111 in memory (size: 16.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_84_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_84_piece0 in memory on 192.168.1.24:39111 (size: 28.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 84 from broadcast at DAGScheduler.scala:1585\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_83_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 109 (MapPartitionsRDD[539] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 109.0 with 2 tasks resource profile 0\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_80_piece0 on 192.168.1.24:39111 in memory (size: 26.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 109.0 (TID 108) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 109.0 (TID 109) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_81_piece0 on 192.168.1.24:39111 in memory (size: 19.7 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 109.0 (TID 109)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 109.0 (TID 109)\n",
      "[Executor task launch worker for task 0.0 in stage 109.0 (TID 108)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 109.0 (TID 108)\n",
      "[Executor task launch worker for task 0.0 in stage 109.0 (TID 108)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 109.0 (TID 109)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 109.0 (TID 108)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 109.0 (TID 108). 3654 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 109.0 (TID 108) in 10 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 109.0 (TID 109)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 109.0 (TID 109). 3654 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 109.0 (TID 109) in 13 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 109.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 109 (showString at <unknown>:0) finished in 0.027 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 110)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 110 (MapPartitionsRDD[544] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_85 stored as values in memory (estimated size 84.6 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_85_piece0 stored as bytes in memory (estimated size 30.8 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_85_piece0 in memory on 192.168.1.24:39111 (size: 30.8 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 85 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 110 (MapPartitionsRDD[544] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 110.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 110.0 (TID 110) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 110.0 (TID 110)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 110.0 (TID 110)\n",
      "[Executor task launch worker for task 0.0 in stage 110.0 (TID 110)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (980.0 B) non-empty blocks including 2 (980.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 110.0 (TID 110)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "|age|class_failures|school_support|family_support|extra_paid_classes|activities|higher_ed|internet_access|romantic_relationship|family_relationship|free_time|social|weekday_alcohol|weekend_alcohol|health|absences|grade_1|grade_2|final_grade|subject_code| id|mother_education_is_5th_to_9th_grade|mother_education_is_none|mother_education_is_secondary_education|mother_education_is_primary_education_(4th_grade)|mother_education_is_higher_education|father_education_is_5th_to_9th_grade|father_education_is_none|father_education_is_secondary_education|father_education_is_primary_education_(4th_grade)|father_education_is_higher_education|reputation_school_choice|course_school_choice|other_school_choice|home_school_choice|father_guardian|mother_guardian|other_guardian|study_time_encoded|sex_is_M|address_type_is_Urban|\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "| 18|             0|             1|             0|                 0|         0|        1|              0|                    0|                  4|        3|     4|              1|              1|     3|       6|      5|      6|          6|           1|  1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   1|                  0|                 0|              0|              1|             1|                 1|       0|                    1|\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- class_failures: integer (nullable = true)\n",
      " |-- school_support: integer (nullable = false)\n",
      " |-- family_support: integer (nullable = false)\n",
      " |-- extra_paid_classes: integer (nullable = false)\n",
      " |-- activities: integer (nullable = false)\n",
      " |-- higher_ed: integer (nullable = false)\n",
      " |-- internet_access: integer (nullable = false)\n",
      " |-- romantic_relationship: integer (nullable = false)\n",
      " |-- family_relationship: integer (nullable = true)\n",
      " |-- free_time: integer (nullable = true)\n",
      " |-- social: integer (nullable = true)\n",
      " |-- weekday_alcohol: integer (nullable = true)\n",
      " |-- weekend_alcohol: integer (nullable = true)\n",
      " |-- health: integer (nullable = true)\n",
      " |-- absences: integer (nullable = true)\n",
      " |-- grade_1: integer (nullable = true)\n",
      " |-- grade_2: integer (nullable = true)\n",
      " |-- final_grade: integer (nullable = true)\n",
      " |-- subject_code: integer (nullable = false)\n",
      " |-- id: integer (nullable = false)\n",
      " |-- mother_education_is_5th_to_9th_grade: integer (nullable = true)\n",
      " |-- mother_education_is_none: integer (nullable = true)\n",
      " |-- mother_education_is_secondary_education: integer (nullable = true)\n",
      " |-- mother_education_is_primary_education_(4th_grade): integer (nullable = true)\n",
      " |-- mother_education_is_higher_education: integer (nullable = true)\n",
      " |-- father_education_is_5th_to_9th_grade: integer (nullable = true)\n",
      " |-- father_education_is_none: integer (nullable = true)\n",
      " |-- father_education_is_secondary_education: integer (nullable = true)\n",
      " |-- father_education_is_primary_education_(4th_grade): integer (nullable = true)\n",
      " |-- father_education_is_higher_education: integer (nullable = true)\n",
      " |-- reputation_school_choice: integer (nullable = true)\n",
      " |-- course_school_choice: integer (nullable = true)\n",
      " |-- other_school_choice: integer (nullable = true)\n",
      " |-- home_school_choice: integer (nullable = true)\n",
      " |-- father_guardian: integer (nullable = true)\n",
      " |-- mother_guardian: integer (nullable = true)\n",
      " |-- other_guardian: integer (nullable = true)\n",
      " |-- study_time_encoded: integer (nullable = false)\n",
      " |-- sex_is_M: integer (nullable = false)\n",
      " |-- address_type_is_Urban: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 110.0 (TID 110)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.986166 ms\n",
      "[Executor task launch worker for task 0.0 in stage 110.0 (TID 110)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 110.0 (TID 110). 4858 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 110.0 (TID 110) in 40 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 110.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 110 (showString at <unknown>:0) finished in 0.044 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 76 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 110: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 76 finished: showString at <unknown>:0, took 0.075036 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.488353 ms\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị thông tin dữ liệu\n",
    "num_rows = df.count()\n",
    "print(f\"Số dòng: {num_rows}\")\n",
    "num_columns = len(df.columns)\n",
    "print(f\"Số cột: {num_columns}\")\n",
    "\n",
    "df.show(1)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d12ee8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 555 (toPandas at /tmp/ipykernel_14509/373494526.py:7) as input to shuffle 33\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 77 (toPandas at /tmp/ipykernel_14509/373494526.py:7) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 111 (toPandas at /tmp/ipykernel_14509/373494526.py:7)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 111 (MapPartitionsRDD[555] at toPandas at /tmp/ipykernel_14509/373494526.py:7), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_86 stored as values in memory (estimated size 66.7 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_86_piece0 stored as bytes in memory (estimated size 22.5 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_86_piece0 in memory on 192.168.1.24:39111 (size: 22.5 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 86 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 111 (MapPartitionsRDD[555] at toPandas at /tmp/ipykernel_14509/373494526.py:7) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 111.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 111.0 (TID 111) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 111.0 (TID 112) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 111.0 (TID 111)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 111.0 (TID 111)\n",
      "[Executor task launch worker for task 1.0 in stage 111.0 (TID 112)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 111.0 (TID 112)\n",
      "[Executor task launch worker for task 0.0 in stage 111.0 (TID 111)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 111.0 (TID 112)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 111.0 (TID 111)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 111.0 (TID 111). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 111.0 (TID 111) in 10 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 111.0 (TID 112)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 111.0 (TID 112). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 111.0 (TID 112) in 10 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 111.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 111 (toPandas at /tmp/ipykernel_14509/373494526.py:7) finished in 0.015 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_86_piece0 on 192.168.1.24:39111 in memory (size: 22.5 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_85_piece0 on 192.168.1.24:39111 in memory (size: 30.8 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_84_piece0 on 192.168.1.24:39111 in memory (size: 28.3 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.expand_doConsume_0$ is 17996 bytes\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 57.892923 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.331999 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 560 (toPandas at /tmp/ipykernel_14509/373494526.py:7) as input to shuffle 34\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 78 (toPandas at /tmp/ipykernel_14509/373494526.py:7) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 113 (toPandas at /tmp/ipykernel_14509/373494526.py:7)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 112)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 113 (MapPartitionsRDD[560] at toPandas at /tmp/ipykernel_14509/373494526.py:7), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_87 stored as values in memory (estimated size 288.2 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_87_piece0 stored as bytes in memory (estimated size 65.2 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_87_piece0 in memory on 192.168.1.24:39111 (size: 65.2 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 87 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 113 (MapPartitionsRDD[560] at toPandas at /tmp/ipykernel_14509/373494526.py:7) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 113.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 113.0 (TID 113) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 113.0 (TID 113)\n",
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (133.4 KiB) non-empty blocks including 2 (133.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.271178 ms\n",
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.567074 ms\n",
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.expand_doConsume_0$ is 17996 bytes\n",
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 56.976219 ms\n",
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.328412 ms\n",
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.25352 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 113.0 (TID 113)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 113.0 (TID 113). 5781 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 113.0 (TID 113) in 263 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 113.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 113 (toPandas at /tmp/ipykernel_14509/373494526.py:7) finished in 0.272 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(34), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_87_piece0 on 192.168.1.24:39111 in memory (size: 65.2 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 30.094355 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 563 (toPandas at /tmp/ipykernel_14509/373494526.py:7) as input to shuffle 35\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 79 (toPandas at /tmp/ipykernel_14509/373494526.py:7) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 116 (toPandas at /tmp/ipykernel_14509/373494526.py:7)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 115)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 116 (MapPartitionsRDD[563] at toPandas at /tmp/ipykernel_14509/373494526.py:7), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_88 stored as values in memory (estimated size 259.6 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_88_piece0 stored as bytes in memory (estimated size 70.3 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_88_piece0 in memory on 192.168.1.24:39111 (size: 70.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 88 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 116 (MapPartitionsRDD[563] at toPandas at /tmp/ipykernel_14509/373494526.py:7) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 116.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 116.0 (TID 114) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 116.0 (TID 114)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 116.0 (TID 114)\n",
      "[Executor task launch worker for task 0.0 in stage 116.0 (TID 114)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (27.9 KiB) non-empty blocks including 1 (27.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 116.0 (TID 114)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 116.0 (TID 114)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 28.650074 ms\n",
      "[Executor task launch worker for task 0.0 in stage 116.0 (TID 114)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 116.0 (TID 114). 7539 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 116.0 (TID 114) in 53 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 116.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 116 (toPandas at /tmp/ipykernel_14509/373494526.py:7) finished in 0.060 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.529009 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: toPandas at /tmp/ipykernel_14509/373494526.py:7\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 80 (toPandas at /tmp/ipykernel_14509/373494526.py:7) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 120 (toPandas at /tmp/ipykernel_14509/373494526.py:7)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 119)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 120 (MapPartitionsRDD[566] at toPandas at /tmp/ipykernel_14509/373494526.py:7), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_89 stored as values in memory (estimated size 36.3 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_89_piece0 stored as bytes in memory (estimated size 11.4 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_89_piece0 in memory on 192.168.1.24:39111 (size: 11.4 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 89 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[566] at toPandas at /tmp/ipykernel_14509/373494526.py:7) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 120.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 120.0 (TID 115) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 120.0 (TID 115)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 120.0 (TID 115)\n",
      "[Executor task launch worker for task 0.0 in stage 120.0 (TID 115)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (117.0 B) non-empty blocks including 1 (117.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 120.0 (TID 115)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 120.0 (TID 115)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.077535 ms\n",
      "[Executor task launch worker for task 0.0 in stage 120.0 (TID 115)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 120.0 (TID 115). 4045 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 120.0 (TID 115) in 15 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 120.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 120 (toPandas at /tmp/ipykernel_14509/373494526.py:7) finished in 0.018 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 80 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 120: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 80 finished: toPandas at /tmp/ipykernel_14509/373494526.py:7, took 0.019929 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>distinct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>address_type_is_Urban</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>subject_code</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sex_is_M</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mother_education_is_5th_to_9th_grade</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mother_education_is_none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mother_education_is_secondary_education</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mother_education_is_primary_education_(4th_grade)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>father_guardian</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mother_education_is_higher_education</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>father_education_is_none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>father_education_is_secondary_education</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>father_education_is_primary_education_(4th_grade)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>father_education_is_higher_education</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>reputation_school_choice</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>course_school_choice</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>father_education_is_5th_to_9th_grade</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>other_school_choice</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>mother_guardian</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>internet_access</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>school_support</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family_support</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>extra_paid_classes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>activities</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>higher_ed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>other_guardian</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>home_school_choice</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>romantic_relationship</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class_failures</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>study_time_encoded</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>family_relationship</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>free_time</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>social</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>weekday_alcohol</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>weekend_alcohol</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>health</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>grade_2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>grade_1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>final_grade</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>absences</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>id</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               column  distinct_count\n",
       "40                              address_type_is_Urban               2\n",
       "19                                       subject_code               2\n",
       "39                                           sex_is_M               2\n",
       "21               mother_education_is_5th_to_9th_grade               2\n",
       "22                           mother_education_is_none               2\n",
       "23            mother_education_is_secondary_education               2\n",
       "24  mother_education_is_primary_education_(4th_grade)               2\n",
       "35                                    father_guardian               2\n",
       "25               mother_education_is_higher_education               2\n",
       "27                           father_education_is_none               2\n",
       "28            father_education_is_secondary_education               2\n",
       "29  father_education_is_primary_education_(4th_grade)               2\n",
       "30               father_education_is_higher_education               2\n",
       "31                           reputation_school_choice               2\n",
       "32                               course_school_choice               2\n",
       "26               father_education_is_5th_to_9th_grade               2\n",
       "33                                other_school_choice               2\n",
       "36                                    mother_guardian               2\n",
       "7                                     internet_access               2\n",
       "2                                      school_support               2\n",
       "3                                      family_support               2\n",
       "4                                  extra_paid_classes               2\n",
       "5                                          activities               2\n",
       "6                                           higher_ed               2\n",
       "37                                     other_guardian               2\n",
       "34                                 home_school_choice               2\n",
       "8                               romantic_relationship               2\n",
       "1                                      class_failures               4\n",
       "38                                 study_time_encoded               4\n",
       "9                                 family_relationship               5\n",
       "10                                          free_time               5\n",
       "11                                             social               5\n",
       "12                                    weekday_alcohol               5\n",
       "13                                    weekend_alcohol               5\n",
       "14                                             health               5\n",
       "0                                                 age               8\n",
       "17                                            grade_2              17\n",
       "16                                            grade_1              18\n",
       "18                                        final_grade              19\n",
       "15                                           absences              35\n",
       "20                                                 id            1044"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distinct = df\n",
    "\n",
    "# Tính số giá trị duy nhất của từng cột hiện tại\n",
    "distinct_counts = df_distinct.agg(*(countDistinct(col(c)).alias(c) for c in df_distinct.columns))\n",
    "\n",
    "# Chuyển kết quả sang Pandas DataFrame\n",
    "distinct_counts_pd = distinct_counts.toPandas().T.reset_index()\n",
    "distinct_counts_pd.columns = ['column', 'distinct_count']\n",
    "\n",
    "# Sắp xếp kết quả từ ít giá trị nhất đến nhiều giá trị nhất\n",
    "sorted_distinct_counts_pd = distinct_counts_pd.sort_values(by='distinct_count')\n",
    "\n",
    "sorted_distinct_counts_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b3cd16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.888225 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.589578 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: toPandas at /tmp/ipykernel_14509/1261834516.py:5\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 81 (toPandas at /tmp/ipykernel_14509/1261834516.py:5) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 121 (toPandas at /tmp/ipykernel_14509/1261834516.py:5)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 121 (MapPartitionsRDD[576] at toPandas at /tmp/ipykernel_14509/1261834516.py:5), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_90 stored as values in memory (estimated size 49.1 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_90_piece0 stored as bytes in memory (estimated size 19.0 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_90_piece0 in memory on 192.168.1.24:39111 (size: 19.0 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 90 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 121 (MapPartitionsRDD[576] at toPandas at /tmp/ipykernel_14509/1261834516.py:5) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 121.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 121.0 (TID 116) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 121.0 (TID 117) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8376 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 121.0 (TID 116)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 121.0 (TID 116)\n",
      "[Executor task launch worker for task 1.0 in stage 121.0 (TID 117)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 121.0 (TID 117)\n",
      "[Executor task launch worker for task 0.0 in stage 121.0 (TID 116)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 121.0 (TID 117)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 121.0 (TID 116)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.622689 ms\n",
      "[Executor task launch worker for task 0.0 in stage 121.0 (TID 116)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.210999 ms\n",
      "[Executor task launch worker for task 1.0 in stage 121.0 (TID 117)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.633401 ms\n",
      "[Executor task launch worker for task 0.0 in stage 121.0 (TID 116)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 121.0 (TID 116). 15890 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 121.0 (TID 116) in 18 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 121.0 (TID 117)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 121.0 (TID 117). 24222 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 121.0 (TID 117) in 22 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 121.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 121 (toPandas at /tmp/ipykernel_14509/1261834516.py:5) finished in 0.026 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 81 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 121: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 81 finished: toPandas at /tmp/ipykernel_14509/1261834516.py:5, took 0.027407 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chuyển đổi DataFrame PySpark thành DataFrame pandas\n",
    "pandas_df = df.select(\"class_failures\", \"study_time_encoded\", \"free_time\", \n",
    "                      \"family_relationship\", \"weekday_alcohol\", \"weekend_alcohol\",\n",
    "                      \"health\", \"social\", \"age\", \"absences\",\n",
    "                      \"grade_1\", \"grade_2\", \"final_grade\").toPandas()\n",
    "\n",
    "# Vẽ biểu đồ ma trận tương quan bằng seaborn\n",
    "correlation_matrix = pandas_df.corr()\n",
    "plt.figure(figsize=(10, 8)) \n",
    "\n",
    "# Vẽ heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e0d9b7",
   "metadata": {},
   "source": [
    "Các thuộc tính đa phần không có mối tương quan cao. Nhưng còn 1 cặp có độ tương quan là 0.63: \n",
    "+ weekday_alcohol - weekdend_alcohol\n",
    "\n",
    "Nếu cân nhắc loại bỏ 1 trong 2. Nhưng nhóm chọn bỏ weekend_alcohol vì có sự tương quan hơn với social - học sinh có mức xã giao cao. \n",
    "\n",
    "-> Loại weekdend_alcohol\n",
    "\n",
    "+ grade_1 - grade_2: cần giữ lại cho bước Phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ba4cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loại bỏ thuộc tính weekdend_alcohol\n",
    "df = df.drop(\"weekend_alcohol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0200f1f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.757541 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.663051 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: toPandas at /tmp/ipykernel_14509/2035232886.py:5\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 82 (toPandas at /tmp/ipykernel_14509/2035232886.py:5) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 122 (toPandas at /tmp/ipykernel_14509/2035232886.py:5)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 122 (MapPartitionsRDD[586] at toPandas at /tmp/ipykernel_14509/2035232886.py:5), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_91 stored as values in memory (estimated size 48.5 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_91_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_89_piece0 on 192.168.1.24:39111 in memory (size: 11.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_91_piece0 in memory on 192.168.1.24:39111 (size: 18.8 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 91 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 122 (MapPartitionsRDD[586] at toPandas at /tmp/ipykernel_14509/2035232886.py:5) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 122.0 with 2 tasks resource profile 0\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_90_piece0 on 192.168.1.24:39111 in memory (size: 19.0 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_88_piece0 on 192.168.1.24:39111 in memory (size: 70.3 KiB, free: 434.3 MiB)\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 122.0 (TID 118) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 122.0 (TID 119) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8376 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 122.0 (TID 118)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 122.0 (TID 118)\n",
      "[Executor task launch worker for task 1.0 in stage 122.0 (TID 119)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 122.0 (TID 119)\n",
      "[Executor task launch worker for task 0.0 in stage 122.0 (TID 118)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 122.0 (TID 119)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 122.0 (TID 118)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.991363 ms\n",
      "[Executor task launch worker for task 0.0 in stage 122.0 (TID 118)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.213325 ms\n",
      "[Executor task launch worker for task 1.0 in stage 122.0 (TID 119)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.298304 ms\n",
      "[Executor task launch worker for task 0.0 in stage 122.0 (TID 118)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 122.0 (TID 118). 14863 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 122.0 (TID 118) in 19 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 122.0 (TID 119)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 122.0 (TID 119). 22538 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 122.0 (TID 119) in 21 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 122.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 122 (toPandas at /tmp/ipykernel_14509/2035232886.py:5) finished in 0.031 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 82 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 122: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 82 finished: toPandas at /tmp/ipykernel_14509/2035232886.py:5, took 0.032515 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chuyển đổi DataFrame PySpark thành DataFrame pandas\n",
    "pandas_df = df.select(\"class_failures\", \"study_time_encoded\", \"free_time\", \n",
    "                      \"family_relationship\", \"weekday_alcohol\", \n",
    "                      \"health\", \"social\", \"age\", \"absences\",\n",
    "                      \"grade_1\", \"grade_2\", \"final_grade\").toPandas()\n",
    "\n",
    "# Vẽ biểu đồ ma trận tương quan bằng seaborn\n",
    "correlation_matrix = pandas_df.corr()\n",
    "plt.figure(figsize=(10, 8)) \n",
    "\n",
    "# Vẽ heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54dd3a",
   "metadata": {},
   "source": [
    "# Model trainning and testing (using KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ca8f7",
   "metadata": {},
   "source": [
    "## KNN Regression: Đối với bài toán dự đoán ta sẽ tiến hành chọn biến mục tiêu là grade_final (điểm trung bình năm học của học sinh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2045a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 595 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 36\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 83 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 123 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 123 (MapPartitionsRDD[595] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_92 stored as values in memory (estimated size 39.3 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_92_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_92_piece0 in memory on 192.168.1.24:39111 (size: 16.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 92 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 123 (MapPartitionsRDD[595] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 123.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 123.0 (TID 120) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 123.0 (TID 121) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 123.0 (TID 121)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 123.0 (TID 121)\n",
      "[Executor task launch worker for task 0.0 in stage 123.0 (TID 120)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 123.0 (TID 120)\n",
      "[Executor task launch worker for task 0.0 in stage 123.0 (TID 120)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 123.0 (TID 121)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 123.0 (TID 120)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 123.0 (TID 120). 2354 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 123.0 (TID 120) in 6 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 123.0 (TID 121)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 123.0 (TID 121). 2354 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 123.0 (TID 121) in 7 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 123.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 123 (count at NativeMethodAccessorImpl.java:0) finished in 0.010 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 84 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 125 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 124)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 125 (MapPartitionsRDD[598] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_93 stored as values in memory (estimated size 12.5 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_93_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_93_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 93 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[598] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 125.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 125.0 (TID 122) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 125.0 (TID 122)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 125.0 (TID 122)\n",
      "[Executor task launch worker for task 0.0 in stage 125.0 (TID 122)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 125.0 (TID 122)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 125.0 (TID 122)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 125.0 (TID 122). 3995 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 125.0 (TID 122) in 4 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 125.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 125 (count at NativeMethodAccessorImpl.java:0) finished in 0.006 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 84 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 125: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 84 finished: count at NativeMethodAccessorImpl.java:0, took 0.007095 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.652578 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.09044 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.415245 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.153509 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.32251 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 610 (showString at <unknown>:0) as input to shuffle 37\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 85 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 127 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 126)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 126)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 126 (MapPartitionsRDD[610] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_94 stored as values in memory (estimated size 78.8 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_94_piece0 stored as bytes in memory (estimated size 28.1 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_94_piece0 in memory on 192.168.1.24:39111 (size: 28.1 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 94 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 126 (MapPartitionsRDD[610] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 126.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 126.0 (TID 123) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 126.0 (TID 124) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 126.0 (TID 123)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 126.0 (TID 123)\n",
      "[Executor task launch worker for task 1.0 in stage 126.0 (TID 124)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 126.0 (TID 124)\n",
      "[Executor task launch worker for task 1.0 in stage 126.0 (TID 124)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.31179 ms\n",
      "[Executor task launch worker for task 1.0 in stage 126.0 (TID 124)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 126.0 (TID 123)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 126.0 (TID 124)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.813556 ms\n",
      "[Executor task launch worker for task 1.0 in stage 126.0 (TID 124)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.147084 ms\n",
      "[Executor task launch worker for task 0.0 in stage 126.0 (TID 123)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.446491 ms\n",
      "[Executor task launch worker for task 1.0 in stage 126.0 (TID 124)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.078353 ms\n",
      "[Executor task launch worker for task 1.0 in stage 126.0 (TID 124)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 126.0 (TID 124). 3654 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 126.0 (TID 123)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 126.0 (TID 123). 3654 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 126.0 (TID 124) in 32 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 126.0 (TID 123) in 32 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 126.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 126 (showString at <unknown>:0) finished in 0.035 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 127)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 127 (MapPartitionsRDD[615] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_95 stored as values in memory (estimated size 84.1 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_95_piece0 stored as bytes in memory (estimated size 30.9 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_95_piece0 in memory on 192.168.1.24:39111 (size: 30.9 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 95 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 127 (MapPartitionsRDD[615] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 127.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 127.0 (TID 125) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 127.0 (TID 125)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 127.0 (TID 125)\n",
      "[Executor task launch worker for task 0.0 in stage 127.0 (TID 125)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (1186.0 B) non-empty blocks including 2 (1186.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 127.0 (TID 125)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số dòng: 1044\n",
      "Số cột: 40\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "|age|class_failures|school_support|family_support|extra_paid_classes|activities|higher_ed|internet_access|romantic_relationship|family_relationship|free_time|social|weekday_alcohol|health|absences|grade_1|grade_2|final_grade|subject_code| id|mother_education_is_5th_to_9th_grade|mother_education_is_none|mother_education_is_secondary_education|mother_education_is_primary_education_(4th_grade)|mother_education_is_higher_education|father_education_is_5th_to_9th_grade|father_education_is_none|father_education_is_secondary_education|father_education_is_primary_education_(4th_grade)|father_education_is_higher_education|reputation_school_choice|course_school_choice|other_school_choice|home_school_choice|father_guardian|mother_guardian|other_guardian|study_time_encoded|sex_is_M|address_type_is_Urban|\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "| 18|             0|             1|             0|                 0|         0|        1|              0|                    0|                  4|        3|     4|              1|     3|       6|      5|      6|          6|           1|  1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   1|                  0|                 0|              0|              1|             1|                 1|       0|                    1|\n",
      "| 17|             0|             0|             1|                 0|         0|        1|              1|                    0|                  5|        3|     3|              1|     3|       4|      5|      5|          6|           1|  2|                                   0|                       0|                                      0|                                                1|                                   0|                                   0|                       0|                                      0|                                                1|                                   0|                       0|                   1|                  0|                 0|              1|              0|             0|                 1|       0|                    1|\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- class_failures: integer (nullable = true)\n",
      " |-- school_support: integer (nullable = false)\n",
      " |-- family_support: integer (nullable = false)\n",
      " |-- extra_paid_classes: integer (nullable = false)\n",
      " |-- activities: integer (nullable = false)\n",
      " |-- higher_ed: integer (nullable = false)\n",
      " |-- internet_access: integer (nullable = false)\n",
      " |-- romantic_relationship: integer (nullable = false)\n",
      " |-- family_relationship: integer (nullable = true)\n",
      " |-- free_time: integer (nullable = true)\n",
      " |-- social: integer (nullable = true)\n",
      " |-- weekday_alcohol: integer (nullable = true)\n",
      " |-- health: integer (nullable = true)\n",
      " |-- absences: integer (nullable = true)\n",
      " |-- grade_1: integer (nullable = true)\n",
      " |-- grade_2: integer (nullable = true)\n",
      " |-- final_grade: integer (nullable = true)\n",
      " |-- subject_code: integer (nullable = false)\n",
      " |-- id: integer (nullable = false)\n",
      " |-- mother_education_is_5th_to_9th_grade: integer (nullable = true)\n",
      " |-- mother_education_is_none: integer (nullable = true)\n",
      " |-- mother_education_is_secondary_education: integer (nullable = true)\n",
      " |-- mother_education_is_primary_education_(4th_grade): integer (nullable = true)\n",
      " |-- mother_education_is_higher_education: integer (nullable = true)\n",
      " |-- father_education_is_5th_to_9th_grade: integer (nullable = true)\n",
      " |-- father_education_is_none: integer (nullable = true)\n",
      " |-- father_education_is_secondary_education: integer (nullable = true)\n",
      " |-- father_education_is_primary_education_(4th_grade): integer (nullable = true)\n",
      " |-- father_education_is_higher_education: integer (nullable = true)\n",
      " |-- reputation_school_choice: integer (nullable = true)\n",
      " |-- course_school_choice: integer (nullable = true)\n",
      " |-- other_school_choice: integer (nullable = true)\n",
      " |-- home_school_choice: integer (nullable = true)\n",
      " |-- father_guardian: integer (nullable = true)\n",
      " |-- mother_guardian: integer (nullable = true)\n",
      " |-- other_guardian: integer (nullable = true)\n",
      " |-- study_time_encoded: integer (nullable = false)\n",
      " |-- sex_is_M: integer (nullable = false)\n",
      " |-- address_type_is_Urban: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 127.0 (TID 125)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.582883 ms\n",
      "[Executor task launch worker for task 0.0 in stage 127.0 (TID 125)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.963696 ms\n",
      "[Executor task launch worker for task 0.0 in stage 127.0 (TID 125)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 127.0 (TID 125). 4955 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 127.0 (TID 125) in 35 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 127.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 127 (showString at <unknown>:0) finished in 0.038 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 85 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 127: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 85 finished: showString at <unknown>:0, took 0.076552 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.837943 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Số dòng: {df.count()}\")\n",
    "print(f\"Số cột: {len(df.columns)}\")\n",
    "\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc9eef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn feature, tất cả của df ngoài \"id\" và target\n",
    "selected_columns = [col for col in df.columns if col not in [\"id\", \"final_grade\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ac877f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_94_piece0 on 192.168.1.24:39111 in memory (size: 28.1 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_93_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_92_piece0 on 192.168.1.24:39111 in memory (size: 16.3 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_91_piece0 on 192.168.1.24:39111 in memory (size: 18.8 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_95_piece0 on 192.168.1.24:39111 in memory (size: 30.9 KiB, free: 434.3 MiB)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.47501 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|id |final_grade|feature                                                                                                               |\n",
      "+---+-----------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |6          |[18, 0, 1, 0, 0, 0, 1, 0, 0, 4, 3, 4, 1, 3, 6, 5, 6, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]   |\n",
      "|2  |6          |[17, 0, 0, 1, 0, 0, 1, 1, 0, 5, 3, 3, 1, 3, 4, 5, 5, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1]   |\n",
      "|3  |10         |[15, 3, 1, 0, 1, 0, 1, 1, 0, 4, 3, 2, 2, 3, 10, 7, 8, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1]  |\n",
      "|4  |15         |[15, 0, 0, 1, 1, 1, 1, 1, 1, 3, 2, 2, 1, 5, 2, 15, 14, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 1] |\n",
      "|5  |10         |[16, 0, 0, 1, 1, 0, 1, 0, 0, 4, 3, 2, 1, 5, 4, 6, 10, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1]  |\n",
      "|6  |15         |[16, 0, 0, 1, 1, 1, 1, 1, 0, 5, 4, 2, 1, 5, 10, 15, 15, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]|\n",
      "|7  |11         |[16, 0, 0, 0, 0, 0, 1, 1, 0, 4, 4, 4, 1, 3, 0, 12, 12, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1] |\n",
      "|8  |6          |[17, 0, 1, 1, 0, 0, 1, 0, 0, 4, 1, 4, 1, 1, 6, 6, 5, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1]   |\n",
      "|9  |19         |[15, 0, 0, 1, 1, 0, 1, 1, 0, 4, 2, 2, 1, 1, 0, 16, 18, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1] |\n",
      "|10 |15         |[15, 0, 0, 1, 1, 1, 1, 1, 0, 5, 5, 1, 1, 5, 0, 14, 15, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1] |\n",
      "|11 |9          |[15, 0, 0, 1, 1, 0, 1, 1, 0, 3, 3, 3, 1, 2, 0, 10, 8, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]  |\n",
      "|12 |12         |[15, 0, 0, 1, 0, 1, 1, 1, 0, 5, 2, 2, 1, 4, 4, 10, 12, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1] |\n",
      "|13 |14         |[15, 0, 0, 1, 1, 1, 1, 1, 0, 4, 3, 3, 1, 5, 2, 14, 14, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1] |\n",
      "|14 |11         |[15, 0, 0, 1, 1, 0, 1, 1, 0, 5, 4, 3, 1, 3, 2, 10, 10, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1] |\n",
      "|15 |16         |[15, 0, 0, 1, 0, 0, 1, 1, 1, 4, 5, 2, 1, 3, 0, 14, 16, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 1, 1] |\n",
      "|16 |14         |[16, 0, 0, 1, 0, 0, 1, 1, 0, 4, 4, 4, 1, 2, 4, 14, 14, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1] |\n",
      "|17 |14         |[16, 0, 0, 1, 1, 1, 1, 1, 0, 3, 2, 3, 1, 2, 6, 13, 14, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 2, 0, 1] |\n",
      "|18 |10         |[16, 0, 1, 1, 0, 1, 1, 0, 0, 5, 3, 2, 1, 4, 4, 8, 10, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]  |\n",
      "|19 |5          |[17, 3, 0, 1, 0, 1, 1, 1, 0, 5, 5, 5, 2, 5, 16, 6, 5, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1]  |\n",
      "|20 |10         |[16, 0, 0, 0, 1, 1, 1, 1, 0, 3, 1, 3, 1, 5, 4, 8, 10, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1]  |\n",
      "+---+-----------+----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 627 (showString at <unknown>:0) as input to shuffle 38\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 86 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 129 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 128)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 128)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 128 (MapPartitionsRDD[627] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_96 stored as values in memory (estimated size 78.8 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_96_piece0 stored as bytes in memory (estimated size 28.1 KiB, free 433.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_96_piece0 in memory on 192.168.1.24:39111 (size: 28.1 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 96 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 128 (MapPartitionsRDD[627] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 128.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 128.0 (TID 126) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 128.0 (TID 127) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 128.0 (TID 126)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 128.0 (TID 126)\n",
      "[Executor task launch worker for task 1.0 in stage 128.0 (TID 127)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 128.0 (TID 127)\n",
      "[Executor task launch worker for task 0.0 in stage 128.0 (TID 126)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 128.0 (TID 127)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 128.0 (TID 127)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 128.0 (TID 127). 3654 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 128.0 (TID 126)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 128.0 (TID 126). 3654 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 128.0 (TID 127) in 15 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 128.0 (TID 126) in 15 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 128.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 128 (showString at <unknown>:0) finished in 0.021 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 129)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 129 (MapPartitionsRDD[632] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_97 stored as values in memory (estimated size 79.6 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_97_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_97_piece0 in memory on 192.168.1.24:39111 (size: 30.5 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 97 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[632] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 129.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 129.0 (TID 128) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 129.0 (TID 128)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 129.0 (TID 128)\n",
      "[Executor task launch worker for task 0.0 in stage 129.0 (TID 128)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (5.9 KiB) non-empty blocks including 2 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 129.0 (TID 128)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 129.0 (TID 128)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.727657 ms\n",
      "[Executor task launch worker for task 0.0 in stage 129.0 (TID 128)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 129.0 (TID 128). 5914 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 129.0 (TID 128) in 25 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 129.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 129 (showString at <unknown>:0) finished in 0.028 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 86 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 129: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 86 finished: showString at <unknown>:0, took 0.052106 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.7915 ms\n"
     ]
    }
   ],
   "source": [
    "df_regression = df.select(\"id\", \"final_grade\", f.array(*selected_columns).alias(\"feature\"))\n",
    "df_regression.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ba24d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- final_grade: integer (nullable = true)\n",
      " |-- feature: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_regression.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "127a813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_regression.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f07ddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 643 (take at /tmp/ipykernel_14509/2715825711.py:3) as input to shuffle 39\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 87 (take at /tmp/ipykernel_14509/2715825711.py:3) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 130 (take at /tmp/ipykernel_14509/2715825711.py:3)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 130 (MapPartitionsRDD[643] at take at /tmp/ipykernel_14509/2715825711.py:3), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_98 stored as values in memory (estimated size 65.8 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_98_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_98_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 98 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 130 (MapPartitionsRDD[643] at take at /tmp/ipykernel_14509/2715825711.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 130.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 130.0 (TID 129) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 130.0 (TID 130) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 130.0 (TID 130)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 130.0 (TID 130)\n",
      "[Executor task launch worker for task 0.0 in stage 130.0 (TID 129)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 130.0 (TID 129)\n",
      "[Executor task launch worker for task 1.0 in stage 130.0 (TID 130)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 130.0 (TID 129)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 130.0 (TID 130)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 130.0 (TID 130). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 130.0 (TID 130) in 11 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 130.0 (TID 129)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 130.0 (TID 129). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 130.0 (TID 129) in 12 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 130.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 130 (take at /tmp/ipykernel_14509/2715825711.py:3) finished in 0.015 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.686145 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: take at /tmp/ipykernel_14509/2715825711.py:3\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 88 (take at /tmp/ipykernel_14509/2715825711.py:3) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 132 (take at /tmp/ipykernel_14509/2715825711.py:3)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 131)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 132 (MapPartitionsRDD[648] at take at /tmp/ipykernel_14509/2715825711.py:3), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_99 stored as values in memory (estimated size 97.8 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_99_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_99_piece0 in memory on 192.168.1.24:39111 (size: 35.5 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 99 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 132 (MapPartitionsRDD[648] at take at /tmp/ipykernel_14509/2715825711.py:3) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 132.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 132.0 (TID 131) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 132.0 (TID 131)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 132.0 (TID 131)\n",
      "[Executor task launch worker for task 0.0 in stage 132.0 (TID 131)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 132.0 (TID 131)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 132.0 (TID 131)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.871216 ms\n",
      "[Executor task launch worker for task 0.0 in stage 132.0 (TID 131)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.487015 ms\n",
      "[Executor task launch worker for task 0.0 in stage 132.0 (TID 131)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.836726 ms\n",
      "[Executor task launch worker for task 0.0 in stage 132.0 (TID 131)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.072057 ms\n",
      "[Executor task launch worker for task 0.0 in stage 132.0 (TID 131)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 132.0 (TID 131). 5209 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 132.0 (TID 131) in 50 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 132.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 132 (take at /tmp/ipykernel_14509/2715825711.py:3) finished in 0.054 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 88 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 132: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 88 finished: take at /tmp/ipykernel_14509/2715825711.py:3, took 0.055290 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 659 (take at /tmp/ipykernel_14509/2715825711.py:7) as input to shuffle 40\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 89 (take at /tmp/ipykernel_14509/2715825711.py:7) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 133 (take at /tmp/ipykernel_14509/2715825711.py:7)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 133 (MapPartitionsRDD[659] at take at /tmp/ipykernel_14509/2715825711.py:7), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_100 stored as values in memory (estimated size 65.8 KiB, free 433.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_100_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 433.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_100_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 100 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 133 (MapPartitionsRDD[659] at take at /tmp/ipykernel_14509/2715825711.py:7) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 133.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 133.0 (TID 132) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 133.0 (TID 133) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 133.0 (TID 133)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 133.0 (TID 133)\n",
      "[Executor task launch worker for task 0.0 in stage 133.0 (TID 132)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 133.0 (TID 132)\n",
      "[Executor task launch worker for task 0.0 in stage 133.0 (TID 132)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 133.0 (TID 133)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 133.0 (TID 132)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 133.0 (TID 132). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 133.0 (TID 132) in 11 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 133.0 (TID 133)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 133.0 (TID 133). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 133.0 (TID 133) in 13 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 133.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 133 (take at /tmp/ipykernel_14509/2715825711.py:7) finished in 0.017 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_96_piece0 on 192.168.1.24:39111 in memory (size: 28.1 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_97_piece0 on 192.168.1.24:39111 in memory (size: 30.5 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_100_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_98_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_99_piece0 on 192.168.1.24:39111 in memory (size: 35.5 KiB, free: 434.3 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.258069 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: take at /tmp/ipykernel_14509/2715825711.py:7\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 90 (take at /tmp/ipykernel_14509/2715825711.py:7) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 135 (take at /tmp/ipykernel_14509/2715825711.py:7)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 134)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 135 (MapPartitionsRDD[664] at take at /tmp/ipykernel_14509/2715825711.py:7), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_101 stored as values in memory (estimated size 97.8 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_101_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_101_piece0 in memory on 192.168.1.24:39111 (size: 35.5 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 101 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[664] at take at /tmp/ipykernel_14509/2715825711.py:7) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 135.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 135.0 (TID 134) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 135.0 (TID 134)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 135.0 (TID 134)\n",
      "[Executor task launch worker for task 0.0 in stage 135.0 (TID 134)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 135.0 (TID 134)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 135.0 (TID 134)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.081897 ms\n",
      "[Executor task launch worker for task 0.0 in stage 135.0 (TID 134)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 135.0 (TID 134). 5215 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 135.0 (TID 134) in 34 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 135.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 135 (take at /tmp/ipykernel_14509/2715825711.py:7) finished in 0.039 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 90 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 135: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 90 finished: take at /tmp/ipykernel_14509/2715825711.py:7, took 0.039598 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(test_feature=[17, 0, 0, 1, 0, 0, 1, 1, 0, 5, 3, 3, 1, 3, 4, 5, 5, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1], test_id=2)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "train = train.select(col(\"feature\"), col(\"final_grade\"))\n",
    "train.take(1)\n",
    "\n",
    "# Test data\n",
    "test = test.select(col(\"feature\").alias(\"test_feature\"), col(\"id\").alias(\"test_id\")).dropDuplicates()\n",
    "test.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f44105",
   "metadata": {},
   "source": [
    "### Mô hình hồi quy K-Nearest Neighbors (KNN) sử dụng Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47a6c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(x,y):\n",
    "    s = 0\n",
    "    for i,j in zip(x,y):\n",
    "        s = s + i*j\n",
    "    return s/(norm(x)*norm(y))\n",
    "\n",
    "class KNearestNeighborsRegression:\n",
    "    #khởi tạo và nhận đầu vào\n",
    "    def __init__(self, train, test, k):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.k = k\n",
    "        \n",
    "    def get_predict(self):\n",
    "        k = self.k\n",
    "        tr = self.train\n",
    "        td = self.test\n",
    "        join_df = tr.crossJoin(td)\n",
    "        cos_map_df = join_df\\\n",
    "                    .rdd.map(lambda x: (float(cos(x.feature,x.test_feature)), x.final_grade, x.test_id))\\\n",
    "                    .toDF([\"cos\", \"final_grade\", \"test_id\"])\n",
    "        windowDept = Window.partitionBy(\"test_id\").orderBy(col(\"cos\").desc(), col(\"final_grade\").desc())\n",
    "    \n",
    "        top_k_nearest_items_df = cos_map_df\\\n",
    "                                .withColumn(\"row\",row_number().over(windowDept)).filter(col(\"row\") <= k)\n",
    "        \n",
    "        final_grade_predict_df = top_k_nearest_items_df.rdd.map(lambda x: (x.test_id, x.cos*x.final_grade))\\\n",
    "                                                    .reduceByKey(add)\\\n",
    "                                                    .map(lambda x: (x[0], (x[1]/k)))\\\n",
    "                                                    .toDF([\"id\", \"final_grade_predict\"])\\\n",
    "                                                    .orderBy(col(\"final_grade_predict\").desc())\\\n",
    "                                                    \n",
    "        return final_grade_predict_df\n",
    "    \n",
    "    def evaluate(self):\n",
    "        k = self.k\n",
    "        w = self.train.orderBy(f.rand())\n",
    "        test_size = int(w.count()/4)\n",
    "        test_df = w.limit(test_size)\n",
    "        train_df = w.subtract(test_df)\n",
    "        test_df = test_df.withColumnRenamed(\"feature\", \"test_feature\")\\\n",
    "                                    .withColumn(\"test_id\", monotonically_increasing_id())\n",
    "        \n",
    "        actual_final_grade = test_df.select(col(\"test_id\"), col(\"final_grade\").alias(\"actual_final_grade\"))\n",
    "        \n",
    "        join_df = train_df.crossJoin(test_df)\n",
    "    \n",
    "        cos_map_df = join_df\\\n",
    "                    .rdd.map(lambda x: (float(cos(x.feature,x.test_feature)), x.final_grade, x.test_id))\\\n",
    "                    .toDF([\"cosine_score\", \"final_grade\", \"test_id\"])\n",
    "    \n",
    "        windowDept = Window.partitionBy(\"test_id\").orderBy(col(\"cosine_score\").desc(), col(\"final_grade\").desc())\n",
    "    \n",
    "        top_k_nearest_items_df = cos_map_df\\\n",
    "                                .withColumn(\"row\",row_number().over(windowDept)).filter(col(\"row\") <= k)\n",
    "    \n",
    "        final_grade_predict_df = top_k_nearest_items_df.rdd.map(lambda x: (x.test_id, x.cosine_score*x.final_grade))\\\n",
    "                                                    .reduceByKey(add)\\\n",
    "                                                    .map(lambda x: (x[0], x[1]/k))\\\n",
    "                                                    .toDF([\"test_id\", \"final_grade_predict\"])\n",
    "        \n",
    "        join_df = final_grade_predict_df.join(actual_final_grade, [\"test_id\"])\n",
    "\n",
    "        # Calculate RMSE, MSE, and MAE\n",
    "        result_rdd = join_df.rdd.map(lambda x: ((x.final_grade_predict - x.actual_final_grade) ** 2))\n",
    "        rmse = (result_rdd.reduce(add) / result_rdd.count()) ** 0.5\n",
    "\n",
    "        mse = (result_rdd.reduce(add) / result_rdd.count())\n",
    "\n",
    "        result_rdd1 = join_df.select(abs(col(\"final_grade_predict\") - col(\"actual_final_grade\")).alias(\"mae\"))\n",
    "        mae = result_rdd1.rdd.map(lambda x: x.mae).reduce(add) / result_rdd1.count()\n",
    "\n",
    "        return {\"RMSE\": rmse, \"MAE\": mae, \"MSE\": mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "651d44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to find the best k\n",
    "def find_best_k(train, test, k_range):\n",
    "    best_k = None\n",
    "    best_metrics = {\"RMSE\": float(\"inf\"), \"MAE\": float(\"inf\"), \"MSE\": float(\"inf\")}\n",
    "    \n",
    "    for k in k_range:\n",
    "        knn = KNearestNeighborsRegression(train, test, k)\n",
    "        metrics = knn.evaluate()\n",
    "        print(f\"Evaluating k={k}: RMSE={metrics['RMSE']}, MAE={metrics['MAE']}, MSE={metrics['MSE']}\")\n",
    "        \n",
    "        if metrics[\"RMSE\"] < best_metrics[\"RMSE\"]:\n",
    "            best_metrics[\"RMSE\"] = metrics[\"RMSE\"]\n",
    "            best_k_rmse = k\n",
    "            \n",
    "        if metrics[\"MAE\"] < best_metrics[\"MAE\"]:\n",
    "            best_metrics[\"MAE\"] = metrics[\"MAE\"]\n",
    "            best_k_mae = k\n",
    "            \n",
    "        if metrics[\"MSE\"] < best_metrics[\"MSE\"]:\n",
    "            best_metrics[\"MSE\"] = metrics[\"MSE\"]\n",
    "            best_k_mse = k\n",
    "\n",
    "    return {\n",
    "        \"Best k for RMSE\": best_k_rmse,\n",
    "        \"Best k for MAE\": best_k_mae,\n",
    "        \"Best k for MSE\": best_k_mse,\n",
    "        \"Best metrics\": best_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d91bc7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 675 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 41\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 91 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 136 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 136 (MapPartitionsRDD[675] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_102 stored as values in memory (estimated size 65.8 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_102_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_102_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 102 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 136 (MapPartitionsRDD[675] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 136.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 136.0 (TID 135) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 136.0 (TID 136) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 136.0 (TID 135)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 136.0 (TID 135)\n",
      "[Executor task launch worker for task 1.0 in stage 136.0 (TID 136)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 136.0 (TID 136)\n",
      "[Executor task launch worker for task 0.0 in stage 136.0 (TID 135)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 136.0 (TID 136)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 136.0 (TID 135)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 136.0 (TID 135). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 136.0 (TID 135) in 11 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 136.0 (TID 136)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 136.0 (TID 136). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 136.0 (TID 136) in 13 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 136.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 136 (count at NativeMethodAccessorImpl.java:0) finished in 0.016 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.413411 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 92 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 138 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 137)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 138 (MapPartitionsRDD[680] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_103 stored as values in memory (estimated size 102.5 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_103_piece0 stored as bytes in memory (estimated size 37.8 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_103_piece0 in memory on 192.168.1.24:39111 (size: 37.8 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 103 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 138 (MapPartitionsRDD[680] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 138.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 138.0 (TID 137) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 138.0 (TID 137)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 138.0 (TID 137)\n",
      "[Executor task launch worker for task 0.0 in stage 138.0 (TID 137)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 138.0 (TID 137)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 138.0 (TID 137)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.31026 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_102_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 138.0 (TID 137)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 138.0 (TID 137). 6025 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 138.0 (TID 137) in 54 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 138.0, whose tasks have all completed, from pool \n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_101_piece0 on 192.168.1.24:39111 in memory (size: 35.5 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 138 (count at NativeMethodAccessorImpl.java:0) finished in 0.057 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 92 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 138: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 92 finished: count at NativeMethodAccessorImpl.java:0, took 0.058331 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 691 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 42\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 93 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 139 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 139 (MapPartitionsRDD[691] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_104 stored as values in memory (estimated size 65.8 KiB, free 433.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_104_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_104_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 104 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 139 (MapPartitionsRDD[691] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 139.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 139.0 (TID 138) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 139.0 (TID 139) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 139.0 (TID 139)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 139.0 (TID 139)\n",
      "[Executor task launch worker for task 0.0 in stage 139.0 (TID 138)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 139.0 (TID 138)\n",
      "[Executor task launch worker for task 1.0 in stage 139.0 (TID 139)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 139.0 (TID 138)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.149897 ms\n",
      "[Executor task launch worker for task 1.0 in stage 139.0 (TID 139)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 139.0 (TID 139). 2325 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 139.0 (TID 138)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 139.0 (TID 138). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 139.0 (TID 139) in 13 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 139.0 (TID 138) in 13 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 139.0, whose tasks have all completed, from pool \n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.194102 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 139 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.018 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.643584 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 702 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 43\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 94 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 140 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 140 (MapPartitionsRDD[702] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_105 stored as values in memory (estimated size 76.0 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_105_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_105_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 105 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 140 (MapPartitionsRDD[702] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 140.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 140.0 (TID 140) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 140.0 (TID 141) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 140.0 (TID 141)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 140.0 (TID 141)\n",
      "[Executor task launch worker for task 0.0 in stage 140.0 (TID 140)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 140.0 (TID 140)\n",
      "[Executor task launch worker for task 1.0 in stage 140.0 (TID 141)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 140.0 (TID 140)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.216266 ms\n",
      "[Executor task launch worker for task 1.0 in stage 140.0 (TID 141)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.385746 ms\n",
      "[Executor task launch worker for task 0.0 in stage 140.0 (TID 140)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.072452 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.332123 ms\n",
      "[Executor task launch worker for task 1.0 in stage 140.0 (TID 141)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.926942 ms\n",
      "[Executor task launch worker for task 1.0 in stage 140.0 (TID 141)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 140.0 (TID 141). 2325 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 140.0 (TID 140)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 140.0 (TID 140). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 140.0 (TID 141) in 24 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 140.0 (TID 140) in 24 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 140.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 140 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.029 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.615077 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 713 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 44\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 95 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 141 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 141 (MapPartitionsRDD[713] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_106 stored as values in memory (estimated size 76.0 KiB, free 433.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_106_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_106_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 106 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 141 (MapPartitionsRDD[713] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 141.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 141.0 (TID 142) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 141.0 (TID 143) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 141.0 (TID 143)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 141.0 (TID 143)\n",
      "[Executor task launch worker for task 0.0 in stage 141.0 (TID 142)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 141.0 (TID 142)\n",
      "[Executor task launch worker for task 1.0 in stage 141.0 (TID 143)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 141.0 (TID 142)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 1.0 in stage 141.0 (TID 143)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.109057 ms\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 141.0 (TID 142)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.732003 ms\n",
      "[Executor task launch worker for task 1.0 in stage 141.0 (TID 143)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.565265 ms\n",
      "[Executor task launch worker for task 0.0 in stage 141.0 (TID 142)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 141.0 (TID 142). 2325 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 141.0 (TID 143)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 141.0 (TID 143). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 141.0 (TID 142) in 28 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 141.0 (TID 143) in 28 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 141.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 141 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.033 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[broadcast-exchange-0] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.986315 ms\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-1] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.231342 ms\n",
      "[broadcast-exchange-0] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.023391 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_106_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_104_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_105_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_103_piece0 on 192.168.1.24:39111 in memory (size: 37.8 KiB, free: 434.3 MiB)\n",
      "[broadcast-exchange-0] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.712436 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[broadcast-exchange-1] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.915622 ms\n",
      "[broadcast-exchange-1] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.077774 ms\n",
      "[broadcast-exchange-0] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 96 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 143 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 142)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 143 (MapPartitionsRDD[719] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_107 stored as values in memory (estimated size 113.1 KiB, free 433.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_107_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 433.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_107_piece0 in memory on 192.168.1.24:39111 (size: 39.9 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 107 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 143 (MapPartitionsRDD[719] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 143.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 143.0 (TID 144) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 143.0 (TID 144)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 143.0 (TID 144)\n",
      "[broadcast-exchange-1] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 97 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 145 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 144)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 145 (MapPartitionsRDD[726] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[Executor task launch worker for task 0.0 in stage 143.0 (TID 144)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.85043 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_108 stored as values in memory (estimated size 116.6 KiB, free 433.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_108_piece0 stored as bytes in memory (estimated size 40.9 KiB, free 433.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_108_piece0 in memory on 192.168.1.24:39111 (size: 40.9 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 108 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 145 (MapPartitionsRDD[726] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[Executor task launch worker for task 0.0 in stage 143.0 (TID 144)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 145.0 with 1 tasks resource profile 0\n",
      "[Executor task launch worker for task 0.0 in stage 143.0 (TID 144)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 145.0 (TID 145) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 145.0 (TID 145)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 145.0 (TID 145)\n",
      "[Executor task launch worker for task 0.0 in stage 143.0 (TID 144)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.72431 ms\n",
      "[Executor task launch worker for task 0.0 in stage 145.0 (TID 145)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 145.0 (TID 145)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 145.0 (TID 145)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.769142 ms\n",
      "[Executor task launch worker for task 0.0 in stage 145.0 (TID 145)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.06939 ms\n",
      "[Executor task launch worker for task 0.0 in stage 143.0 (TID 144)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.543406 ms\n",
      "[Executor task launch worker for task 0.0 in stage 145.0 (TID 145)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.266418 ms\n",
      "[Executor task launch worker for task 0.0 in stage 143.0 (TID 144)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 143.0 (TID 144). 18955 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 143.0 (TID 144) in 82 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 143.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 143 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.086 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 96 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 143: Stage finished\n",
      "[broadcast-exchange-0] INFO org.apache.spark.scheduler.DAGScheduler - Job 96 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.087623 s\n",
      "[Executor task launch worker for task 0.0 in stage 145.0 (TID 145)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.00774 ms\n",
      "[Executor task launch worker for task 0.0 in stage 145.0 (TID 145)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 145.0 (TID 145). 19931 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 145.0 (TID 145) in 64 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 145.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 145 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.071 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 97 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 145: Stage finished\n",
      "[broadcast-exchange-1] INFO org.apache.spark.scheduler.DAGScheduler - Job 97 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.074287 s\n",
      "[broadcast-exchange-1] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_109 stored as values in memory (estimated size 47.0 KiB, free 433.5 MiB)\n",
      "[broadcast-exchange-1] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_109_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 433.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_109_piece0 in memory on 192.168.1.24:39111 (size: 14.7 KiB, free: 434.2 MiB)\n",
      "[broadcast-exchange-1] INFO org.apache.spark.SparkContext - Created broadcast 109 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[broadcast-exchange-0] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.810413 ms\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-0] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_110 stored as values in memory (estimated size 2.0 MiB, free 431.5 MiB)\n",
      "[broadcast-exchange-0] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_110_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 431.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_110_piece0 in memory on 192.168.1.24:39111 (size: 17.4 KiB, free: 434.2 MiB)\n",
      "[broadcast-exchange-0] INFO org.apache.spark.SparkContext - Created broadcast 110 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.775063 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.538021 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 98 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 147 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 146)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 147 (PythonRDD[734] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_111 stored as values in memory (estimated size 200.6 KiB, free 431.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_111_piece0 stored as bytes in memory (estimated size 65.7 KiB, free 431.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_111_piece0 in memory on 192.168.1.24:39111 (size: 65.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 111 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 147 (PythonRDD[734] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 147.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 147.0 (TID 146) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 147.0 (TID 146)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 147.0 (TID 146)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 147.0 (TID 146)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 147.0 (TID 146)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 147.0 (TID 146)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.926096 ms\n",
      "[Executor task launch worker for task 0.0 in stage 147.0 (TID 146)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.965454 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.780371 ms\n",
      "[Executor task launch worker for task 0.0 in stage 147.0 (TID 146)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 112, boot = -6027, init = 6139, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 147.0 (TID 146)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 147.0 (TID 146). 11064 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 147.0 (TID 146) in 200 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 147.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 147 (runJob at PythonRDD.scala:181) finished in 0.207 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 98 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 147: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 98 finished: runJob at PythonRDD.scala:181, took 0.207701 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.584694 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 741 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 45\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 99 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 149 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 148)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 149 (MapPartitionsRDD[741] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_112 stored as values in memory (estimated size 210.1 KiB, free 431.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_112_piece0 stored as bytes in memory (estimated size 70.0 KiB, free 431.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_112_piece0 in memory on 192.168.1.24:39111 (size: 70.0 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 112 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 149 (MapPartitionsRDD[741] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 149.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 149.0 (TID 147) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 149.0 (TID 147)\n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.833411 ms\n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.647286 ms\n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.30015 ms\n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.105758 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_111_piece0 on 192.168.1.24:39111 in memory (size: 65.7 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_108_piece0 on 192.168.1.24:39111 in memory (size: 40.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_107_piece0 on 192.168.1.24:39111 in memory (size: 39.9 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 2379, boot = 3, init = 93, finish = 2283\n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.080724 ms\n",
      "[Executor task launch worker for task 0.0 in stage 149.0 (TID 147)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 149.0 (TID 147). 12027 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 149.0 (TID 147) in 2539 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 149.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 149 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 2.553 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(45), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.511373 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.556047 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 751 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) as input to shuffle 46\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 100 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 153 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 152)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 152)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 152 (PairwiseRDD[751] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_113 stored as values in memory (estimated size 221.8 KiB, free 431.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_113_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 431.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_113_piece0 in memory on 192.168.1.24:39111 (size: 74.1 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 113 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 152 (PairwiseRDD[751] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 152.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 152.0 (TID 148) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 152.0 (TID 148)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 152.0 (TID 148)\n",
      "[Executor task launch worker for task 0.0 in stage 152.0 (TID 148)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (16.0 KiB) non-empty blocks including 1 (16.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 152.0 (TID 148)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "[Executor task launch worker for task 0.0 in stage 152.0 (TID 148)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.277248 ms\n",
      "[Executor task launch worker for task 0.0 in stage 152.0 (TID 148)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.81128 ms\n",
      "[Executor task launch worker for task 0.0 in stage 152.0 (TID 148)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.984904 ms\n",
      "[Executor task launch worker for task 0.0 in stage 152.0 (TID 148)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 87, boot = -228, init = 311, finish = 4\n",
      "[Executor task launch worker for task 0.0 in stage 152.0 (TID 148)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 152.0 (TID 148). 13238 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 152.0 (TID 148) in 261 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 152.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 152 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) finished in 0.272 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 153)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 153 (PythonRDD[754] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_114 stored as values in memory (estimated size 11.4 KiB, free 431.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_114_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 431.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_114_piece0 in memory on 192.168.1.24:39111 (size: 6.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 114 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 153 (PythonRDD[754] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 153.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 153.0 (TID 149) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 153.0 (TID 149)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 153.0 (TID 149)\n",
      "[Executor task launch worker for task 0.0 in stage 153.0 (TID 149)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 153.0 (TID 149)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 153.0 (TID 149)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 81, boot = -149, init = 230, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 153.0 (TID 149)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 153.0 (TID 149). 2065 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 153.0 (TID 149) in 91 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 153.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 153 (runJob at PythonRDD.scala:181) finished in 0.095 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 100 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 153: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 100 finished: runJob at PythonRDD.scala:181, took 0.371164 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 769 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 47\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 101 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 154 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 154 (MapPartitionsRDD[769] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_115 stored as values in memory (estimated size 65.8 KiB, free 431.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_115_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 431.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_115_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 115 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 154 (MapPartitionsRDD[769] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 154.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 154.0 (TID 150) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 154.0 (TID 151) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 154.0 (TID 150)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 154.0 (TID 150)\n",
      "[Executor task launch worker for task 1.0 in stage 154.0 (TID 151)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 154.0 (TID 151)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_114_piece0 on 192.168.1.24:39111 in memory (size: 6.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_113_piece0 on 192.168.1.24:39111 in memory (size: 74.1 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 154.0 (TID 150)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 154.0 (TID 151)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 154.0 (TID 150)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 154.0 (TID 150). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 154.0 (TID 150) in 12 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 154.0 (TID 151)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 154.0 (TID 151). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 154.0 (TID 151) in 15 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 154.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 154 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.018 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-2] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.627775 ms\n",
      "[broadcast-exchange-2] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.676507 ms\n",
      "[broadcast-exchange-2] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.95629 ms\n",
      "[broadcast-exchange-2] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 102 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 156 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 155)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 156 (MapPartitionsRDD[776] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_116 stored as values in memory (estimated size 100.6 KiB, free 431.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_116_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 431.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_116_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 116 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 156 (MapPartitionsRDD[776] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 156.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 156.0 (TID 152) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 156.0 (TID 152)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 156.0 (TID 152)\n",
      "[Executor task launch worker for task 0.0 in stage 156.0 (TID 152)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.873734 ms\n",
      "[Executor task launch worker for task 0.0 in stage 156.0 (TID 152)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 156.0 (TID 152)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 156.0 (TID 152)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.832512 ms\n",
      "[Executor task launch worker for task 0.0 in stage 156.0 (TID 152)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.182087 ms\n",
      "[Executor task launch worker for task 0.0 in stage 156.0 (TID 152)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 1.981739 ms\n",
      "[Executor task launch worker for task 0.0 in stage 156.0 (TID 152)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 156.0 (TID 152). 6802 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 156.0 (TID 152) in 63 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 156.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 156 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.067 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 102 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 156: Stage finished\n",
      "[broadcast-exchange-2] INFO org.apache.spark.scheduler.DAGScheduler - Job 102 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.067860 s\n",
      "[broadcast-exchange-2] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.113314 ms\n",
      "[broadcast-exchange-2] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_117 stored as values in memory (estimated size 1025.5 KiB, free 430.3 MiB)\n",
      "[broadcast-exchange-2] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_117_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 430.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_117_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.1 MiB)\n",
      "[broadcast-exchange-2] INFO org.apache.spark.SparkContext - Created broadcast 117 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.314275 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 103 (reduce at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 160 (reduce at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 159)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 160 (PythonRDD[781] at reduce at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_118 stored as values in memory (estimated size 27.5 KiB, free 430.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_118_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 430.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_118_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 118 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 160 (PythonRDD[781] at reduce at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 160.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 160.0 (TID 153) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 160.0 (TID 153)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 160.0 (TID 153)\n",
      "[Executor task launch worker for task 0.0 in stage 160.0 (TID 153)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 160.0 (TID 153)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 160.0 (TID 153)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.300545 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 95, boot = -320, init = 414, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 160.0 (TID 153)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 97, boot = 5, init = 90, finish = 2\n",
      "[Executor task launch worker for task 0.0 in stage 160.0 (TID 153)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 160.0 (TID 153). 2538 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 160.0 (TID 153) in 119 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 160.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 160 (reduce at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.123 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 103 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 160: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 103 finished: reduce at /tmp/ipykernel_14509/1633301326.py:66, took 0.124830 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 104 (count at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 164 (count at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 163)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 164 (PythonRDD[782] at count at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_119 stored as values in memory (estimated size 28.3 KiB, free 430.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_119_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 430.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_115_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_119_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 119 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 164 (PythonRDD[782] at count at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 164.0 with 1 tasks resource profile 0\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_116_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_118_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.2 MiB)\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 164.0 (TID 154) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 164.0 (TID 154)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 164.0 (TID 154)\n",
      "[Executor task launch worker for task 0.0 in stage 164.0 (TID 154)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 164.0 (TID 154)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_112_piece0 on 192.168.1.24:39111 in memory (size: 70.0 KiB, free: 434.2 MiB)\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 112, boot = -33, init = 144, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 164.0 (TID 154)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 114, boot = -19, init = 132, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 164.0 (TID 154)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 164.0 (TID 154). 2531 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 164.0 (TID 154) in 124 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 164.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 164 (count at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.133 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 104 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 164: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 104 finished: count at /tmp/ipykernel_14509/1633301326.py:66, took 0.134950 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 105 (reduce at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 168 (reduce at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 167)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 168 (PythonRDD[783] at reduce at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_120 stored as values in memory (estimated size 27.5 KiB, free 430.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_120_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 430.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_120_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 120 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 168 (PythonRDD[783] at reduce at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 168.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 168.0 (TID 155) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 168.0 (TID 155)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 168.0 (TID 155)\n",
      "[Executor task launch worker for task 0.0 in stage 168.0 (TID 155)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 168.0 (TID 155)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 83, boot = -15, init = 98, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 168.0 (TID 155)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 85, boot = -15, init = 99, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 168.0 (TID 155)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 168.0 (TID 155). 2538 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 168.0 (TID 155) in 93 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 168.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 168 (reduce at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.096 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 105 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 168: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 105 finished: reduce at /tmp/ipykernel_14509/1633301326.py:68, took 0.098151 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 106 (count at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 172 (count at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 171)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 172 (PythonRDD[784] at count at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_121 stored as values in memory (estimated size 28.3 KiB, free 430.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_121_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 430.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_121_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 121 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 172 (PythonRDD[784] at count at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 172.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 172.0 (TID 156) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 172.0 (TID 156)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 172.0 (TID 156)\n",
      "[Executor task launch worker for task 0.0 in stage 172.0 (TID 156)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 172.0 (TID 156)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 88, boot = -16, init = 103, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 172.0 (TID 156)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 89, boot = -16, init = 104, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 172.0 (TID 156)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 172.0 (TID 156). 2531 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 172.0 (TID 156) in 98 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 172.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 172 (count at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.100 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 106 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 172: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 106 finished: count at /tmp/ipykernel_14509/1633301326.py:68, took 0.102036 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 795 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 48\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 107 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 173 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 173 (MapPartitionsRDD[795] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_122 stored as values in memory (estimated size 65.8 KiB, free 430.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_122_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 430.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_122_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 122 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 173 (MapPartitionsRDD[795] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 173.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 173.0 (TID 157) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 173.0 (TID 158) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 173.0 (TID 158)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 173.0 (TID 158)\n",
      "[Executor task launch worker for task 0.0 in stage 173.0 (TID 157)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 173.0 (TID 157)\n",
      "[Executor task launch worker for task 0.0 in stage 173.0 (TID 157)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 173.0 (TID 158)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 173.0 (TID 157)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 173.0 (TID 157). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 173.0 (TID 157) in 8 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 173.0 (TID 158)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 173.0 (TID 158). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 173.0 (TID 158) in 8 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 173.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 173 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.012 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-3] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 108 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 175 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 174)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 175 (MapPartitionsRDD[802] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_123 stored as values in memory (estimated size 100.6 KiB, free 430.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_123_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 430.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_123_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 123 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 175 (MapPartitionsRDD[802] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 175.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 175.0 (TID 159) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 175.0 (TID 159)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 175.0 (TID 159)\n",
      "[Executor task launch worker for task 0.0 in stage 175.0 (TID 159)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 175.0 (TID 159)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 175.0 (TID 159)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 175.0 (TID 159). 6802 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 175.0 (TID 159) in 23 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 175.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 175 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.026 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 108 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 175: Stage finished\n",
      "[broadcast-exchange-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 108 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.027862 s\n",
      "[broadcast-exchange-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_124 stored as values in memory (estimated size 1025.5 KiB, free 429.4 MiB)\n",
      "[broadcast-exchange-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_124_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 429.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_124_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.1 MiB)\n",
      "[broadcast-exchange-3] INFO org.apache.spark.SparkContext - Created broadcast 124 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.67407 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:71\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 109 (reduce at /tmp/ipykernel_14509/1633301326.py:71) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 179 (reduce at /tmp/ipykernel_14509/1633301326.py:71)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 178)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 179 (PythonRDD[807] at reduce at /tmp/ipykernel_14509/1633301326.py:71), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_125 stored as values in memory (estimated size 27.6 KiB, free 429.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_125_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 429.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_125_piece0 in memory on 192.168.1.24:39111 (size: 12.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 125 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 179 (PythonRDD[807] at reduce at /tmp/ipykernel_14509/1633301326.py:71) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 179.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 179.0 (TID 160) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 179.0 (TID 160)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 179.0 (TID 160)\n",
      "[Executor task launch worker for task 0.0 in stage 179.0 (TID 160)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 179.0 (TID 160)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 179.0 (TID 160)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.925359 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 85, boot = -178, init = 263, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 179.0 (TID 160)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 90, boot = -183, init = 273, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 179.0 (TID 160)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 179.0 (TID 160). 2538 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 179.0 (TID 160) in 104 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 179.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 179 (reduce at /tmp/ipykernel_14509/1633301326.py:71) finished in 0.107 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 109 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 179: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 109 finished: reduce at /tmp/ipykernel_14509/1633301326.py:71, took 0.109159 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 818 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 49\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 110 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 180 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 180 (MapPartitionsRDD[818] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_126 stored as values in memory (estimated size 65.8 KiB, free 429.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_126_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 429.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_126_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 126 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 180 (MapPartitionsRDD[818] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 180.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 180.0 (TID 161) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 180.0 (TID 162) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 180.0 (TID 162)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 180.0 (TID 162)\n",
      "[Executor task launch worker for task 0.0 in stage 180.0 (TID 161)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 180.0 (TID 161)\n",
      "[Executor task launch worker for task 1.0 in stage 180.0 (TID 162)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 180.0 (TID 161)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 180.0 (TID 162)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 180.0 (TID 162). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 180.0 (TID 162) in 8 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 180.0 (TID 161)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 180.0 (TID 161). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 180.0 (TID 161) in 9 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 180.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 180 (count at NativeMethodAccessorImpl.java:0) finished in 0.013 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-4] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.379753 ms\n",
      "[broadcast-exchange-4] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.643537 ms\n",
      "[broadcast-exchange-4] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.808538 ms\n",
      "[broadcast-exchange-4] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 111 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 182 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 182 (MapPartitionsRDD[825] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_127 stored as values in memory (estimated size 101.4 KiB, free 429.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_127_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 429.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_127_piece0 in memory on 192.168.1.24:39111 (size: 37.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 127 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 182 (MapPartitionsRDD[825] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 182.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 182.0 (TID 163) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 182.0 (TID 163)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 182.0 (TID 163)\n",
      "[Executor task launch worker for task 0.0 in stage 182.0 (TID 163)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.688749 ms\n",
      "[Executor task launch worker for task 0.0 in stage 182.0 (TID 163)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 182.0 (TID 163)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_119_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_126_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_125_piece0 on 192.168.1.24:39111 in memory (size: 12.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_123_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_121_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_122_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_120_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 182.0 (TID 163)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.609956 ms\n",
      "[Executor task launch worker for task 0.0 in stage 182.0 (TID 163)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.324993 ms\n",
      "[Executor task launch worker for task 0.0 in stage 182.0 (TID 163)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 182.0 (TID 163). 8163 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 182.0 (TID 163) in 64 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 182.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 182 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.069 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 111 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 182: Stage finished\n",
      "[broadcast-exchange-4] INFO org.apache.spark.scheduler.DAGScheduler - Job 111 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.070630 s\n",
      "[broadcast-exchange-4] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_128 stored as values in memory (estimated size 1025.5 KiB, free 428.6 MiB)\n",
      "[broadcast-exchange-4] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_128_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 428.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_128_piece0 in memory on 192.168.1.24:39111 (size: 2.3 KiB, free: 434.2 MiB)\n",
      "[broadcast-exchange-4] INFO org.apache.spark.SparkContext - Created broadcast 128 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.721242 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 827 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 50\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 112 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 186 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 185)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 186 (MapPartitionsRDD[827] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_129 stored as values in memory (estimated size 24.0 KiB, free 428.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_129_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 428.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_129_piece0 in memory on 192.168.1.24:39111 (size: 12.0 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 129 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 186 (MapPartitionsRDD[827] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 186.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 186.0 (TID 164) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7422 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 186.0 (TID 164)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 186.0 (TID 164)\n",
      "[Executor task launch worker for task 0.0 in stage 186.0 (TID 164)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 186.0 (TID 164)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 186.0 (TID 164)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.864578 ms\n",
      "[Executor task launch worker for task 0.0 in stage 186.0 (TID 164)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 81, boot = -243, init = 324, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 186.0 (TID 164)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 186.0 (TID 164). 3162 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 186.0 (TID 164) in 89 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 186.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 186 (count at NativeMethodAccessorImpl.java:0) finished in 0.092 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.913029 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 113 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 191 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 190)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 191 (MapPartitionsRDD[830] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_130 stored as values in memory (estimated size 12.5 KiB, free 428.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_130_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 428.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_130_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 130 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 191 (MapPartitionsRDD[830] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 191.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 191.0 (TID 165) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 191.0 (TID 165)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 191.0 (TID 165)\n",
      "[Executor task launch worker for task 0.0 in stage 191.0 (TID 165)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 191.0 (TID 165)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 191.0 (TID 165)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.801457 ms\n",
      "[Executor task launch worker for task 0.0 in stage 191.0 (TID 165)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 191.0 (TID 165). 3952 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 191.0 (TID 165) in 7 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 191.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 191 (count at NativeMethodAccessorImpl.java:0) finished in 0.008 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 113 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 191: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 113 finished: count at NativeMethodAccessorImpl.java:0, took 0.009932 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating k=3: RMSE=1.5693130923218828, MAE=1.216659035873881, MSE=2.4627435817328704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 841 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 51\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 114 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 192 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 192 (MapPartitionsRDD[841] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_131 stored as values in memory (estimated size 65.8 KiB, free 428.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_131_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 428.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_131_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 131 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 192 (MapPartitionsRDD[841] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 192.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 192.0 (TID 166) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 192.0 (TID 167) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 192.0 (TID 167)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 192.0 (TID 167)\n",
      "[Executor task launch worker for task 0.0 in stage 192.0 (TID 166)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 192.0 (TID 166)\n",
      "[Executor task launch worker for task 1.0 in stage 192.0 (TID 167)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 192.0 (TID 166)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 192.0 (TID 167)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 192.0 (TID 167). 2325 bytes result sent to driver\n",
      "[Executor task launch worker for task 0.0 in stage 192.0 (TID 166)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 192.0 (TID 166). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 192.0 (TID 167) in 9 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 192.0 (TID 166) in 10 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 192.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 192 (count at NativeMethodAccessorImpl.java:0) finished in 0.013 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 115 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 194 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 193)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 194 (MapPartitionsRDD[846] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_132 stored as values in memory (estimated size 102.5 KiB, free 428.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_132_piece0 stored as bytes in memory (estimated size 37.8 KiB, free 428.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_132_piece0 in memory on 192.168.1.24:39111 (size: 37.8 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 132 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 194 (MapPartitionsRDD[846] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 194.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 194.0 (TID 168) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 194.0 (TID 168)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 194.0 (TID 168)\n",
      "[Executor task launch worker for task 0.0 in stage 194.0 (TID 168)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 194.0 (TID 168)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 194.0 (TID 168)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 194.0 (TID 168). 5939 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 194.0 (TID 168) in 22 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 194.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 194 (count at NativeMethodAccessorImpl.java:0) finished in 0.025 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 115 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 194: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 115 finished: count at NativeMethodAccessorImpl.java:0, took 0.026661 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 857 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 52\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 116 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 195 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 195 (MapPartitionsRDD[857] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_133 stored as values in memory (estimated size 65.8 KiB, free 428.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_133_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 428.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_133_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 133 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 195 (MapPartitionsRDD[857] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 195.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 195.0 (TID 169) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 195.0 (TID 170) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_130_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 195.0 (TID 170)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 195.0 (TID 170)\n",
      "[Executor task launch worker for task 0.0 in stage 195.0 (TID 169)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 195.0 (TID 169)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_131_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_129_piece0 on 192.168.1.24:39111 in memory (size: 12.0 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_132_piece0 on 192.168.1.24:39111 in memory (size: 37.8 KiB, free: 434.2 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 195.0 (TID 170)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 195.0 (TID 169)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 195.0 (TID 169)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 195.0 (TID 169). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 195.0 (TID 169) in 8 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 195.0 (TID 170)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 195.0 (TID 170). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 195.0 (TID 170) in 9 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 195.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 195 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.017 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 868 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 53\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 117 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 196 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 196 (MapPartitionsRDD[868] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_134 stored as values in memory (estimated size 76.0 KiB, free 428.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_134_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 428.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_134_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 134 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 196 (MapPartitionsRDD[868] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 196.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 196.0 (TID 171) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 196.0 (TID 172) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 196.0 (TID 171)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 196.0 (TID 171)\n",
      "[Executor task launch worker for task 1.0 in stage 196.0 (TID 172)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 196.0 (TID 172)\n",
      "[Executor task launch worker for task 0.0 in stage 196.0 (TID 171)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 196.0 (TID 172)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 196.0 (TID 171)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 196.0 (TID 171). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 196.0 (TID 171) in 8 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 879 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 54\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 118 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 197 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 197 (MapPartitionsRDD[879] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_135 stored as values in memory (estimated size 76.0 KiB, free 428.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_135_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 428.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_135_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 196.0 (TID 172)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 196.0 (TID 172). 2325 bytes result sent to driver\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 135 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 197 (MapPartitionsRDD[879] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 197.0 with 2 tasks resource profile 0\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 196.0 (TID 172) in 13 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 196.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 196 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.017 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 197)\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 197.0 (TID 173) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 197.0 (TID 174) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 197.0 (TID 173)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 197.0 (TID 173)\n",
      "[Executor task launch worker for task 1.0 in stage 197.0 (TID 174)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 197.0 (TID 174)\n",
      "[Executor task launch worker for task 0.0 in stage 197.0 (TID 173)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 197.0 (TID 174)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 197.0 (TID 173)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 197.0 (TID 173). 2325 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 197.0 (TID 174)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 197.0 (TID 174). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 197.0 (TID 173) in 10 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 197.0 (TID 174) in 10 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 197.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 197 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.014 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-5] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.317385 ms\n",
      "[broadcast-exchange-6] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.817175 ms\n",
      "[broadcast-exchange-5] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 119 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 199 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 198)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 199 (MapPartitionsRDD[885] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_136 stored as values in memory (estimated size 113.1 KiB, free 428.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_136_piece0 stored as bytes in memory (estimated size 40.0 KiB, free 428.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_136_piece0 in memory on 192.168.1.24:39111 (size: 40.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 136 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 199 (MapPartitionsRDD[885] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 199.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 199.0 (TID 175) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 199.0 (TID 175)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 199.0 (TID 175)\n",
      "[broadcast-exchange-6] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 120 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 201 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 200)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 201 (MapPartitionsRDD[892] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_137 stored as values in memory (estimated size 116.6 KiB, free 428.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 199.0 (TID 175)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 199.0 (TID 175)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_134_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_137_piece0 stored as bytes in memory (estimated size 40.9 KiB, free 428.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_137_piece0 in memory on 192.168.1.24:39111 (size: 40.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 137 from broadcast at DAGScheduler.scala:1585\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_133_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 201 (MapPartitionsRDD[892] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 201.0 with 1 tasks resource profile 0\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_135_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 201.0 (TID 176) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 201.0 (TID 176)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 201.0 (TID 176)\n",
      "[Executor task launch worker for task 0.0 in stage 201.0 (TID 176)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 201.0 (TID 176)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 201.0 (TID 176)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.780735 ms\n",
      "[Executor task launch worker for task 0.0 in stage 199.0 (TID 175)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.135686 ms\n",
      "[Executor task launch worker for task 0.0 in stage 201.0 (TID 176)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 201.0 (TID 176). 19982 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 201.0 (TID 176) in 38 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 201.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 201 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.048 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 120 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 201: Stage finished\n",
      "[broadcast-exchange-6] INFO org.apache.spark.scheduler.DAGScheduler - Job 120 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.049878 s\n",
      "[broadcast-exchange-6] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_138 stored as values in memory (estimated size 47.0 KiB, free 426.2 MiB)\n",
      "[broadcast-exchange-6] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_138_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 426.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_138_piece0 in memory on 192.168.1.24:39111 (size: 14.7 KiB, free: 434.1 MiB)\n",
      "[broadcast-exchange-6] INFO org.apache.spark.SparkContext - Created broadcast 138 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 199.0 (TID 175)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 199.0 (TID 175). 19149 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 199.0 (TID 175) in 61 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 199.0, whose tasks have all completed, from pool \n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 199 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.065 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 119 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 199: Stage finished\n",
      "[broadcast-exchange-5] INFO org.apache.spark.scheduler.DAGScheduler - Job 119 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.066414 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[broadcast-exchange-5] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_139 stored as values in memory (estimated size 2.0 MiB, free 426.3 MiB)\n",
      "[broadcast-exchange-5] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_139_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 426.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_139_piece0 in memory on 192.168.1.24:39111 (size: 17.5 KiB, free: 434.1 MiB)\n",
      "[broadcast-exchange-5] INFO org.apache.spark.SparkContext - Created broadcast 139 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_127_piece0 on 192.168.1.24:39111 in memory (size: 37.4 KiB, free: 434.1 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 121 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 203 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 202)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 203 (PythonRDD[900] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_140 stored as values in memory (estimated size 200.6 KiB, free 426.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_140_piece0 stored as bytes in memory (estimated size 65.6 KiB, free 426.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_140_piece0 in memory on 192.168.1.24:39111 (size: 65.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 140 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 203 (PythonRDD[900] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 203.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 203.0 (TID 177) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 203.0 (TID 177)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 203.0 (TID 177)\n",
      "[Executor task launch worker for task 0.0 in stage 203.0 (TID 177)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 203.0 (TID 177)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 203.0 (TID 177)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 86, boot = -1002, init = 1088, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 203.0 (TID 177)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 203.0 (TID 177). 11064 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 203.0 (TID 177) in 114 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 203.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 203 (runJob at PythonRDD.scala:181) finished in 0.121 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 121 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 203: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 121 finished: runJob at PythonRDD.scala:181, took 0.123399 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 907 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 55\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 122 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 205 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 204)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 205 (MapPartitionsRDD[907] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_141 stored as values in memory (estimated size 210.1 KiB, free 425.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_141_piece0 stored as bytes in memory (estimated size 70.0 KiB, free 425.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_141_piece0 in memory on 192.168.1.24:39111 (size: 70.0 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 141 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 205 (MapPartitionsRDD[907] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 205.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 205.0 (TID 178) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 205.0 (TID 178)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 205.0 (TID 178)\n",
      "[Executor task launch worker for task 0.0 in stage 205.0 (TID 178)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 205.0 (TID 178)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_140_piece0 on 192.168.1.24:39111 in memory (size: 65.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_136_piece0 on 192.168.1.24:39111 in memory (size: 40.0 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_137_piece0 on 192.168.1.24:39111 in memory (size: 40.9 KiB, free: 434.1 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_124_piece0 on 192.168.1.24:39111 in memory (size: 2.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_128_piece0 on 192.168.1.24:39111 in memory (size: 2.3 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 205.0 (TID 178)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 2328, boot = -827, init = 914, finish = 2241\n",
      "[Executor task launch worker for task 0.0 in stage 205.0 (TID 178)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 205.0 (TID 178). 12027 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 205.0 (TID 178) in 2423 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 205.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 205 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 2.430 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(55), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.942894 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 917 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) as input to shuffle 56\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 123 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 209 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 208)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 208)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 208 (PairwiseRDD[917] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_142 stored as values in memory (estimated size 221.8 KiB, free 428.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_142_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 428.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_142_piece0 in memory on 192.168.1.24:39111 (size: 74.1 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 142 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 208 (PairwiseRDD[917] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 208.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 208.0 (TID 179) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 208.0 (TID 179)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 208.0 (TID 179)\n",
      "[Executor task launch worker for task 0.0 in stage 208.0 (TID 179)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (20.5 KiB) non-empty blocks including 1 (20.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 208.0 (TID 179)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 208.0 (TID 179)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.706807 ms\n",
      "[Executor task launch worker for task 0.0 in stage 208.0 (TID 179)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 100, boot = -144, init = 238, finish = 6\n",
      "[Executor task launch worker for task 0.0 in stage 208.0 (TID 179)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 208.0 (TID 179). 13238 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 208.0 (TID 179) in 121 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 208.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 208 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) finished in 0.126 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 209)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 209 (PythonRDD[920] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_143 stored as values in memory (estimated size 11.4 KiB, free 428.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_143_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 428.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_143_piece0 in memory on 192.168.1.24:39111 (size: 6.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 143 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 209 (PythonRDD[920] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 209.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 209.0 (TID 180) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 209.0 (TID 180)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 209.0 (TID 180)\n",
      "[Executor task launch worker for task 0.0 in stage 209.0 (TID 180)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 209.0 (TID 180)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 209.0 (TID 180)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 83, boot = -2, init = 85, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 209.0 (TID 180)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 209.0 (TID 180). 2022 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 209.0 (TID 180) in 89 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 209.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 209 (runJob at PythonRDD.scala:181) finished in 0.091 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 123 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 209: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 123 finished: runJob at PythonRDD.scala:181, took 0.222532 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 935 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 57\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 124 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 210 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 210 (MapPartitionsRDD[935] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_144 stored as values in memory (estimated size 65.8 KiB, free 428.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_144_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 428.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_144_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 144 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 210 (MapPartitionsRDD[935] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 210.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 210.0 (TID 181) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 210.0 (TID 182) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 210.0 (TID 182)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 210.0 (TID 182)\n",
      "[Executor task launch worker for task 0.0 in stage 210.0 (TID 181)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 210.0 (TID 181)\n",
      "[Executor task launch worker for task 1.0 in stage 210.0 (TID 182)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 210.0 (TID 181)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 210.0 (TID 182)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 210.0 (TID 182). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 210.0 (TID 182) in 8 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 210.0 (TID 181)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 210.0 (TID 181). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 210.0 (TID 181) in 8 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 210.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 210 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-7] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.293313 ms\n",
      "[broadcast-exchange-7] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 125 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 212 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 211)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 212 (MapPartitionsRDD[942] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_145 stored as values in memory (estimated size 100.6 KiB, free 427.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_145_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 427.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_145_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 145 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 212 (MapPartitionsRDD[942] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 212.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 212.0 (TID 183) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 212.0 (TID 183)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 212.0 (TID 183)\n",
      "[Executor task launch worker for task 0.0 in stage 212.0 (TID 183)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 212.0 (TID 183)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 212.0 (TID 183)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.382979 ms\n",
      "[Executor task launch worker for task 0.0 in stage 212.0 (TID 183)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 212.0 (TID 183). 6818 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 212.0 (TID 183) in 47 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 212.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 212 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.051 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 125 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 212: Stage finished\n",
      "[broadcast-exchange-7] INFO org.apache.spark.scheduler.DAGScheduler - Job 125 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.052418 s\n",
      "[broadcast-exchange-7] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_146 stored as values in memory (estimated size 1025.5 KiB, free 426.9 MiB)\n",
      "[broadcast-exchange-7] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_146_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 426.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_146_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-7] INFO org.apache.spark.SparkContext - Created broadcast 146 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.099486 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_143_piece0 on 192.168.1.24:39111 in memory (size: 6.6 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_142_piece0 on 192.168.1.24:39111 in memory (size: 74.1 KiB, free: 434.1 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_145_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 126 (reduce at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 216 (reduce at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 215)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_144_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 216 (PythonRDD[947] at reduce at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_147 stored as values in memory (estimated size 27.5 KiB, free 427.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_147_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 427.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_147_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 147 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 216 (PythonRDD[947] at reduce at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 216.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 216.0 (TID 184) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 216.0 (TID 184)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 216.0 (TID 184)\n",
      "[Executor task launch worker for task 0.0 in stage 216.0 (TID 184)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 216.0 (TID 184)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 216.0 (TID 184)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.529171 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 88, boot = -229, init = 317, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 216.0 (TID 184)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 114, boot = 3, init = 109, finish = 2\n",
      "[Executor task launch worker for task 0.0 in stage 216.0 (TID 184)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 216.0 (TID 184). 2538 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 216.0 (TID 184) in 127 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 216.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 216 (reduce at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.129 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 126 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 216: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 126 finished: reduce at /tmp/ipykernel_14509/1633301326.py:66, took 0.131094 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 127 (count at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 220 (count at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 219)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 220 (PythonRDD[948] at count at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_148 stored as values in memory (estimated size 28.3 KiB, free 427.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_148_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 427.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_148_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 148 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 220 (PythonRDD[948] at count at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 220.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 220.0 (TID 185) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 220.0 (TID 185)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 220.0 (TID 185)\n",
      "[Executor task launch worker for task 0.0 in stage 220.0 (TID 185)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 220.0 (TID 185)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 110, boot = -48, init = 158, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 220.0 (TID 185)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 112, boot = -9, init = 120, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 220.0 (TID 185)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 220.0 (TID 185). 2531 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 220.0 (TID 185) in 121 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 220.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 220 (count at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.124 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 127 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 220: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 127 finished: count at /tmp/ipykernel_14509/1633301326.py:66, took 0.125415 s\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_148_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_147_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 128 (reduce at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 224 (reduce at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 223)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 224 (PythonRDD[949] at reduce at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_149 stored as values in memory (estimated size 27.5 KiB, free 427.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_149_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 427.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_149_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 149 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 224 (PythonRDD[949] at reduce at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 224.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 224.0 (TID 186) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 224.0 (TID 186)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 224.0 (TID 186)\n",
      "[Executor task launch worker for task 0.0 in stage 224.0 (TID 186)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 224.0 (TID 186)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 86, boot = -17, init = 102, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 224.0 (TID 186)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 89, boot = -18, init = 106, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 224.0 (TID 186)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 224.0 (TID 186). 2538 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 224.0 (TID 186) in 98 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 224.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 224 (reduce at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.101 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 128 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 224: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 128 finished: reduce at /tmp/ipykernel_14509/1633301326.py:68, took 0.102433 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 129 (count at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 228 (count at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 227)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 228 (PythonRDD[950] at count at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_150 stored as values in memory (estimated size 28.3 KiB, free 427.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_150_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 427.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_150_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 150 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 228 (PythonRDD[950] at count at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 228.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 228.0 (TID 187) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 228.0 (TID 187)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 228.0 (TID 187)\n",
      "[Executor task launch worker for task 0.0 in stage 228.0 (TID 187)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 228.0 (TID 187)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 85, boot = -21, init = 106, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 228.0 (TID 187)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 87, boot = -19, init = 104, finish = 2\n",
      "[Executor task launch worker for task 0.0 in stage 228.0 (TID 187)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 228.0 (TID 187). 2531 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 228.0 (TID 187) in 97 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 228.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 228 (count at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.103 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 129 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 228: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 129 finished: count at /tmp/ipykernel_14509/1633301326.py:68, took 0.104602 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 961 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 58\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 130 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 229 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 229 (MapPartitionsRDD[961] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_151 stored as values in memory (estimated size 65.8 KiB, free 427.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_151_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 427.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_151_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 151 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 229 (MapPartitionsRDD[961] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 229.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 229.0 (TID 188) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 229.0 (TID 189) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 229.0 (TID 188)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 229.0 (TID 188)\n",
      "[Executor task launch worker for task 1.0 in stage 229.0 (TID 189)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 229.0 (TID 189)\n",
      "[Executor task launch worker for task 1.0 in stage 229.0 (TID 189)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 229.0 (TID 188)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 229.0 (TID 188)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 229.0 (TID 188). 2325 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 229.0 (TID 189)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 229.0 (TID 189). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 229.0 (TID 188) in 7 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 229.0 (TID 189) in 7 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 229.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 229 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.009 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-8] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 131 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 231 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 230)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 231 (MapPartitionsRDD[968] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_152 stored as values in memory (estimated size 100.6 KiB, free 427.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_152_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 427.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_152_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 152 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 231 (MapPartitionsRDD[968] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 231.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 231.0 (TID 190) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 231.0 (TID 190)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 231.0 (TID 190)\n",
      "[Executor task launch worker for task 0.0 in stage 231.0 (TID 190)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 231.0 (TID 190)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 231.0 (TID 190)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 231.0 (TID 190). 6818 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 231.0 (TID 190) in 20 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 231.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 231 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.024 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 131 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 231: Stage finished\n",
      "[broadcast-exchange-8] INFO org.apache.spark.scheduler.DAGScheduler - Job 131 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.025058 s\n",
      "[broadcast-exchange-8] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_153 stored as values in memory (estimated size 1025.5 KiB, free 426.1 MiB)\n",
      "[broadcast-exchange-8] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_153_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 426.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_153_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-8] INFO org.apache.spark.SparkContext - Created broadcast 153 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.014697 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:71\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 132 (reduce at /tmp/ipykernel_14509/1633301326.py:71) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 235 (reduce at /tmp/ipykernel_14509/1633301326.py:71)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 234)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 235 (PythonRDD[973] at reduce at /tmp/ipykernel_14509/1633301326.py:71), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_154 stored as values in memory (estimated size 27.6 KiB, free 426.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_154_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 426.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_154_piece0 in memory on 192.168.1.24:39111 (size: 12.6 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 154 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 235 (PythonRDD[973] at reduce at /tmp/ipykernel_14509/1633301326.py:71) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 235.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 235.0 (TID 191) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 235.0 (TID 191)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 235.0 (TID 191)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 235.0 (TID 191)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 235.0 (TID 191)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 136, boot = -194, init = 329, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 235.0 (TID 191)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 135, boot = -196, init = 331, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 235.0 (TID 191)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 235.0 (TID 191). 2538 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 235.0 (TID 191) in 162 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 235.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 235 (reduce at /tmp/ipykernel_14509/1633301326.py:71) finished in 0.170 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 132 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 235: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 132 finished: reduce at /tmp/ipykernel_14509/1633301326.py:71, took 0.173804 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 984 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 59\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 133 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 236 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 236 (MapPartitionsRDD[984] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_155 stored as values in memory (estimated size 65.8 KiB, free 426.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_155_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 426.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_155_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 155 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 236 (MapPartitionsRDD[984] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 236.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 236.0 (TID 192) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 236.0 (TID 193) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 236.0 (TID 192)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 236.0 (TID 192)\n",
      "[Executor task launch worker for task 1.0 in stage 236.0 (TID 193)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 236.0 (TID 193)\n",
      "[Executor task launch worker for task 0.0 in stage 236.0 (TID 192)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 236.0 (TID 193)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 236.0 (TID 192)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 236.0 (TID 192). 2325 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 236.0 (TID 193)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 236.0 (TID 193). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 236.0 (TID 192) in 9 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 236.0 (TID 193) in 8 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 236.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 236 (count at NativeMethodAccessorImpl.java:0) finished in 0.012 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-9] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.726195 ms\n",
      "[broadcast-exchange-9] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 134 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 238 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 237)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 238 (MapPartitionsRDD[991] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_156 stored as values in memory (estimated size 101.4 KiB, free 425.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_156_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 425.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_156_piece0 in memory on 192.168.1.24:39111 (size: 37.4 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 156 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 238 (MapPartitionsRDD[991] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 238.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 238.0 (TID 194) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 238.0 (TID 194)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 238.0 (TID 194)\n",
      "[Executor task launch worker for task 0.0 in stage 238.0 (TID 194)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 238.0 (TID 194)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 238.0 (TID 194)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.41193 ms\n",
      "[Executor task launch worker for task 0.0 in stage 238.0 (TID 194)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 238.0 (TID 194). 8120 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 238.0 (TID 194) in 35 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 238.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 238 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.039 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 134 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 238: Stage finished\n",
      "[broadcast-exchange-9] INFO org.apache.spark.scheduler.DAGScheduler - Job 134 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.040620 s\n",
      "[broadcast-exchange-9] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_157 stored as values in memory (estimated size 1025.5 KiB, free 424.9 MiB)\n",
      "[broadcast-exchange-9] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_157_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 424.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_157_piece0 in memory on 192.168.1.24:39111 (size: 2.3 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-9] INFO org.apache.spark.SparkContext - Created broadcast 157 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 993 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 60\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 135 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 242 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 241)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 242 (MapPartitionsRDD[993] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_158 stored as values in memory (estimated size 24.0 KiB, free 424.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_158_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 424.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_158_piece0 in memory on 192.168.1.24:39111 (size: 12.0 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 158 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 242 (MapPartitionsRDD[993] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 242.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 242.0 (TID 195) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7422 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 242.0 (TID 195)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 242.0 (TID 195)\n",
      "[Executor task launch worker for task 0.0 in stage 242.0 (TID 195)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 242.0 (TID 195)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 242.0 (TID 195)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 90, boot = -235, init = 324, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 242.0 (TID 195)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 242.0 (TID 195). 3162 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 242.0 (TID 195) in 101 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 242.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 242 (count at NativeMethodAccessorImpl.java:0) finished in 0.106 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_150_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_151_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_155_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_152_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_149_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_156_piece0 on 192.168.1.24:39111 in memory (size: 37.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_154_piece0 on 192.168.1.24:39111 in memory (size: 12.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_158_piece0 on 192.168.1.24:39111 in memory (size: 12.0 KiB, free: 434.1 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 136 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 247 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 246)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 247 (MapPartitionsRDD[996] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_159 stored as values in memory (estimated size 12.5 KiB, free 425.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_159_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 425.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_159_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 159 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 247 (MapPartitionsRDD[996] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 247.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 247.0 (TID 196) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 247.0 (TID 196)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 247.0 (TID 196)\n",
      "[Executor task launch worker for task 0.0 in stage 247.0 (TID 196)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 247.0 (TID 196)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 247.0 (TID 196)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 247.0 (TID 196). 3995 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 247.0 (TID 196) in 3 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 247.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 247 (count at NativeMethodAccessorImpl.java:0) finished in 0.006 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 136 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 247: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 136 finished: count at NativeMethodAccessorImpl.java:0, took 0.006968 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1007 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 61\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 137 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 248 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 248 (MapPartitionsRDD[1007] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_160 stored as values in memory (estimated size 65.8 KiB, free 425.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_160_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 425.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_160_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 160 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 248 (MapPartitionsRDD[1007] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 248.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 248.0 (TID 197) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 248.0 (TID 198) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 248.0 (TID 198)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 248.0 (TID 198)\n",
      "[Executor task launch worker for task 0.0 in stage 248.0 (TID 197)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 248.0 (TID 197)\n",
      "[Executor task launch worker for task 0.0 in stage 248.0 (TID 197)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 248.0 (TID 198)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 248.0 (TID 197)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 248.0 (TID 197). 2325 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 248.0 (TID 198)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 248.0 (TID 198). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 248.0 (TID 198) in 7 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 248.0 (TID 197) in 7 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 248.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 248 (count at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 138 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 250 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 249)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 250 (MapPartitionsRDD[1012] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_161 stored as values in memory (estimated size 102.5 KiB, free 425.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_161_piece0 stored as bytes in memory (estimated size 37.8 KiB, free 425.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_161_piece0 in memory on 192.168.1.24:39111 (size: 37.8 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 161 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 250 (MapPartitionsRDD[1012] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 250.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 250.0 (TID 199) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 250.0 (TID 199)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 250.0 (TID 199)\n",
      "[Executor task launch worker for task 0.0 in stage 250.0 (TID 199)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 250.0 (TID 199)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 250.0 (TID 199)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 250.0 (TID 199). 5939 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 250.0 (TID 199) in 20 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 250.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 250 (count at NativeMethodAccessorImpl.java:0) finished in 0.024 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 138 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 250: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 138 finished: count at NativeMethodAccessorImpl.java:0, took 0.026024 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating k=5: RMSE=1.5366776795611328, MAE=1.1251934247056476, MSE=2.361378290861387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1023 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 62\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 139 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 251 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 251 (MapPartitionsRDD[1023] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_162 stored as values in memory (estimated size 65.8 KiB, free 425.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_162_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 425.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_160_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_162_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 162 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 251 (MapPartitionsRDD[1023] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 251.0 with 2 tasks resource profile 0\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_157_piece0 on 192.168.1.24:39111 in memory (size: 2.3 KiB, free: 434.1 MiB)\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 251.0 (TID 200) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 251.0 (TID 201) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_161_piece0 on 192.168.1.24:39111 in memory (size: 37.8 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 251.0 (TID 200)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 251.0 (TID 200)\n",
      "[Executor task launch worker for task 1.0 in stage 251.0 (TID 201)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 251.0 (TID 201)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_159_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 251.0 (TID 200)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 251.0 (TID 201)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 251.0 (TID 200)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 251.0 (TID 200). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 251.0 (TID 200) in 8 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 251.0 (TID 201)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 251.0 (TID 201). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 251.0 (TID 201) in 10 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 251.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 251 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.021 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1034 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 63\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 140 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 252 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 252 (MapPartitionsRDD[1034] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_163 stored as values in memory (estimated size 76.0 KiB, free 426.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_163_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 426.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_163_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 163 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 252 (MapPartitionsRDD[1034] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 252.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 252.0 (TID 202) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 252.0 (TID 203) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 252.0 (TID 203)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 252.0 (TID 203)\n",
      "[Executor task launch worker for task 0.0 in stage 252.0 (TID 202)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 252.0 (TID 202)\n",
      "[Executor task launch worker for task 0.0 in stage 252.0 (TID 202)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 252.0 (TID 203)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1045 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 64\n",
      "[Executor task launch worker for task 0.0 in stage 252.0 (TID 202)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 252.0 (TID 202). 2325 bytes result sent to driver\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 141 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 253 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 252.0 (TID 202) in 19 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 253 (MapPartitionsRDD[1045] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[Executor task launch worker for task 1.0 in stage 252.0 (TID 203)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 252.0 (TID 203). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 252.0 (TID 203) in 20 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 252.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_164 stored as values in memory (estimated size 76.0 KiB, free 426.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_164_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 426.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_164_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 164 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 253 (MapPartitionsRDD[1045] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 253.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 253.0 (TID 204) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 252 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.029 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 253.0 (TID 205) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 253)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Executor task launch worker for task 1.0 in stage 253.0 (TID 205)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 253.0 (TID 205)\n",
      "[Executor task launch worker for task 0.0 in stage 253.0 (TID 204)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 253.0 (TID 204)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 1.0 in stage 253.0 (TID 205)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 253.0 (TID 204)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 253.0 (TID 204)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 253.0 (TID 204). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 253.0 (TID 204) in 22 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 253.0 (TID 205)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 253.0 (TID 205). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 253.0 (TID 205) in 24 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 253.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 253 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.030 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_141_piece0 on 192.168.1.24:39111 in memory (size: 70.0 KiB, free: 434.1 MiB)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[broadcast-exchange-10] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.953885 ms\n",
      "[broadcast-exchange-11] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.237403 ms\n",
      "[broadcast-exchange-10] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[broadcast-exchange-11] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 142 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 255 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 254)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 255 (MapPartitionsRDD[1053] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_165 stored as values in memory (estimated size 113.1 KiB, free 426.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_165_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 426.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_165_piece0 in memory on 192.168.1.24:39111 (size: 39.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 165 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 255 (MapPartitionsRDD[1053] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 255.0 with 1 tasks resource profile 0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 143 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 257 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 256)\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 255.0 (TID 206) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 257 (MapPartitionsRDD[1058] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[Executor task launch worker for task 0.0 in stage 255.0 (TID 206)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 255.0 (TID 206)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_166 stored as values in memory (estimated size 116.6 KiB, free 426.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_166_piece0 stored as bytes in memory (estimated size 41.0 KiB, free 426.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_166_piece0 in memory on 192.168.1.24:39111 (size: 41.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 166 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 257 (MapPartitionsRDD[1058] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 257.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 257.0 (TID 207) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 257.0 (TID 207)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 257.0 (TID 207)\n",
      "[Executor task launch worker for task 0.0 in stage 255.0 (TID 206)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 255.0 (TID 206)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 257.0 (TID 207)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 257.0 (TID 207)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 257.0 (TID 207)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.104617 ms\n",
      "[Executor task launch worker for task 0.0 in stage 255.0 (TID 206)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.566581 ms\n",
      "[Executor task launch worker for task 0.0 in stage 257.0 (TID 207)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 257.0 (TID 207). 19841 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 257.0 (TID 207) in 32 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 257.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 257 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.036 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 143 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 257: Stage finished\n",
      "[broadcast-exchange-11] INFO org.apache.spark.scheduler.DAGScheduler - Job 143 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.041645 s\n",
      "[Executor task launch worker for task 0.0 in stage 255.0 (TID 206)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 255.0 (TID 206). 18918 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 255.0 (TID 206) in 38 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[broadcast-exchange-11] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_167 stored as values in memory (estimated size 47.0 KiB, free 426.1 MiB)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 255.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 255 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.042 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 142 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 255: Stage finished\n",
      "[broadcast-exchange-10] INFO org.apache.spark.scheduler.DAGScheduler - Job 142 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.043976 s\n",
      "[broadcast-exchange-11] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_167_piece0 stored as bytes in memory (estimated size 14.5 KiB, free 426.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_167_piece0 in memory on 192.168.1.24:39111 (size: 14.5 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-11] INFO org.apache.spark.SparkContext - Created broadcast 167 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[broadcast-exchange-10] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_168 stored as values in memory (estimated size 2.0 MiB, free 424.0 MiB)\n",
      "[broadcast-exchange-10] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_168_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 424.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_168_piece0 in memory on 192.168.1.24:39111 (size: 17.2 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-10] INFO org.apache.spark.SparkContext - Created broadcast 168 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 144 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 259 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 258)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 259 (PythonRDD[1066] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_169 stored as values in memory (estimated size 200.6 KiB, free 423.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_169_piece0 stored as bytes in memory (estimated size 65.7 KiB, free 423.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_169_piece0 in memory on 192.168.1.24:39111 (size: 65.7 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 169 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 259 (PythonRDD[1066] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 259.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 259.0 (TID 208) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 259.0 (TID 208)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 259.0 (TID 208)\n",
      "[Executor task launch worker for task 0.0 in stage 259.0 (TID 208)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 259.0 (TID 208)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_164_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_165_piece0 on 192.168.1.24:39111 in memory (size: 39.9 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_163_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_166_piece0 on 192.168.1.24:39111 in memory (size: 41.0 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_162_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 259.0 (TID 208)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 108, boot = -1035, init = 1142, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 259.0 (TID 208)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 259.0 (TID 208). 11107 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 259.0 (TID 208) in 144 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 259.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 259 (runJob at PythonRDD.scala:181) finished in 0.149 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 144 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 259: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 144 finished: runJob at PythonRDD.scala:181, took 0.150288 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1073 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 65\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 145 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 261 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 260)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 261 (MapPartitionsRDD[1073] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_170 stored as values in memory (estimated size 210.1 KiB, free 424.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_170_piece0 stored as bytes in memory (estimated size 70.0 KiB, free 424.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_170_piece0 in memory on 192.168.1.24:39111 (size: 70.0 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 170 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 261 (MapPartitionsRDD[1073] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 261.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 261.0 (TID 209) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 261.0 (TID 209)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 261.0 (TID 209)\n",
      "[Executor task launch worker for task 0.0 in stage 261.0 (TID 209)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 261.0 (TID 209)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_169_piece0 on 192.168.1.24:39111 in memory (size: 65.7 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 261.0 (TID 209)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 2459, boot = -879, init = 975, finish = 2363\n",
      "[Executor task launch worker for task 0.0 in stage 261.0 (TID 209)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 261.0 (TID 209). 12027 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 261.0 (TID 209) in 2549 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 261.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 261 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 2.559 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(65), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.983028 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1083 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) as input to shuffle 66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 146 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 265 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 264 (PairwiseRDD[1083] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_171 stored as values in memory (estimated size 221.8 KiB, free 424.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_171_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 424.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_171_piece0 in memory on 192.168.1.24:39111 (size: 74.1 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 171 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 264 (PairwiseRDD[1083] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 264.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 264.0 (TID 210) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 264.0 (TID 210)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 264.0 (TID 210)\n",
      "[Executor task launch worker for task 0.0 in stage 264.0 (TID 210)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (25.0 KiB) non-empty blocks including 1 (25.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 264.0 (TID 210)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 264.0 (TID 210)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.604854 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 264.0 (TID 210)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 98, boot = -130, init = 219, finish = 9\n",
      "[Executor task launch worker for task 0.0 in stage 264.0 (TID 210)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 264.0 (TID 210). 13238 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 264.0 (TID 210) in 118 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 264.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 264 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) finished in 0.124 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 265)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 265 (PythonRDD[1086] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_172 stored as values in memory (estimated size 11.4 KiB, free 424.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_172_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 424.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_172_piece0 in memory on 192.168.1.24:39111 (size: 6.6 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 172 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 265 (PythonRDD[1086] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 265.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 265.0 (TID 211) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 265.0 (TID 211)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 265.0 (TID 211)\n",
      "[Executor task launch worker for task 0.0 in stage 265.0 (TID 211)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 265.0 (TID 211)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 265.0 (TID 211)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 84, boot = -2, init = 86, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 265.0 (TID 211)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 265.0 (TID 211). 2065 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 265.0 (TID 211) in 91 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 265.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 265 (runJob at PythonRDD.scala:181) finished in 0.093 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 146 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 265: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 146 finished: runJob at PythonRDD.scala:181, took 0.221605 s\n",
      "\r",
      "                                                                                \r",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1101 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 67\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 147 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 266 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 266 (MapPartitionsRDD[1101] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_173 stored as values in memory (estimated size 65.8 KiB, free 424.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_173_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 423.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_173_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 173 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 266 (MapPartitionsRDD[1101] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 266.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 266.0 (TID 212) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 266.0 (TID 213) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 266.0 (TID 213)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 266.0 (TID 213)\n",
      "[Executor task launch worker for task 0.0 in stage 266.0 (TID 212)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 266.0 (TID 212)\n",
      "[Executor task launch worker for task 1.0 in stage 266.0 (TID 213)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 266.0 (TID 212)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 266.0 (TID 212)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 266.0 (TID 212). 2325 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 266.0 (TID 213)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 266.0 (TID 213). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 266.0 (TID 212) in 6 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 266.0 (TID 213) in 6 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 266.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 266 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.010 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_171_piece0 on 192.168.1.24:39111 in memory (size: 74.1 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_173_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_170_piece0 on 192.168.1.24:39111 in memory (size: 70.0 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_172_piece0 on 192.168.1.24:39111 in memory (size: 6.6 KiB, free: 434.2 MiB)\n",
      "[broadcast-exchange-12] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.434847 ms\n",
      "[broadcast-exchange-12] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 148 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 268 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 267)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 268 (MapPartitionsRDD[1108] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_174 stored as values in memory (estimated size 100.6 KiB, free 424.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_174_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 424.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_174_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 174 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 268 (MapPartitionsRDD[1108] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 268.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 268.0 (TID 214) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 268.0 (TID 214)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 268.0 (TID 214)\n",
      "[Executor task launch worker for task 0.0 in stage 268.0 (TID 214)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 268.0 (TID 214)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 268.0 (TID 214)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.839784 ms\n",
      "[Executor task launch worker for task 0.0 in stage 268.0 (TID 214)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 268.0 (TID 214). 6795 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 268.0 (TID 214) in 37 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 268.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 268 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.044 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 148 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 268: Stage finished\n",
      "[broadcast-exchange-12] INFO org.apache.spark.scheduler.DAGScheduler - Job 148 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.045315 s\n",
      "[broadcast-exchange-12] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_175 stored as values in memory (estimated size 1025.5 KiB, free 423.5 MiB)\n",
      "[broadcast-exchange-12] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_175_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 423.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_175_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.1 MiB)\n",
      "[broadcast-exchange-12] INFO org.apache.spark.SparkContext - Created broadcast 175 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 149 (reduce at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 272 (reduce at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 271)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 272 (PythonRDD[1113] at reduce at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_176 stored as values in memory (estimated size 27.5 KiB, free 423.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_176_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 423.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_176_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 176 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 272 (PythonRDD[1113] at reduce at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 272.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 272.0 (TID 215) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 272.0 (TID 215)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 272.0 (TID 215)\n",
      "[Executor task launch worker for task 0.0 in stage 272.0 (TID 215)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 272.0 (TID 215)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 84, boot = -198, init = 281, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 272.0 (TID 215)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 93, boot = 3, init = 88, finish = 2\n",
      "[Executor task launch worker for task 0.0 in stage 272.0 (TID 215)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 272.0 (TID 215). 2538 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 272.0 (TID 215) in 99 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 272.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 272 (reduce at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.102 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 149 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 272: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 149 finished: reduce at /tmp/ipykernel_14509/1633301326.py:66, took 0.102687 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 150 (count at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 276 (count at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 275)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 276 (PythonRDD[1114] at count at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_177 stored as values in memory (estimated size 28.3 KiB, free 423.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_177_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 423.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_177_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 177 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 276 (PythonRDD[1114] at count at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 276.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 276.0 (TID 216) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 276.0 (TID 216)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 276.0 (TID 216)\n",
      "[Executor task launch worker for task 0.0 in stage 276.0 (TID 216)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 276.0 (TID 216)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 93, boot = -31, init = 124, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 276.0 (TID 216)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 95, boot = -8, init = 102, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 276.0 (TID 216)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 276.0 (TID 216). 2531 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 276.0 (TID 216) in 102 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 276.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 276 (count at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.105 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 150 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 276: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 150 finished: count at /tmp/ipykernel_14509/1633301326.py:66, took 0.106865 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 151 (reduce at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 280 (reduce at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 279)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 280 (PythonRDD[1115] at reduce at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_178 stored as values in memory (estimated size 27.5 KiB, free 423.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_178_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 423.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_178_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 178 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 280 (PythonRDD[1115] at reduce at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 280.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 280.0 (TID 217) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 280.0 (TID 217)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 280.0 (TID 217)\n",
      "[Executor task launch worker for task 0.0 in stage 280.0 (TID 217)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 280.0 (TID 217)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 101, boot = -18, init = 118, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 280.0 (TID 217)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 102, boot = -10, init = 111, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 280.0 (TID 217)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 280.0 (TID 217). 2538 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 280.0 (TID 217) in 110 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 280.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 280 (reduce at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.112 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 151 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 280: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 151 finished: reduce at /tmp/ipykernel_14509/1633301326.py:68, took 0.114022 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 152 (count at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 284 (count at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 283)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 284 (PythonRDD[1116] at count at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_179 stored as values in memory (estimated size 28.3 KiB, free 423.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_179_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 423.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_179_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 179 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 284 (PythonRDD[1116] at count at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 284.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 284.0 (TID 218) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 284.0 (TID 218)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 284.0 (TID 218)\n",
      "[Executor task launch worker for task 0.0 in stage 284.0 (TID 218)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 284.0 (TID 218)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 94, boot = -18, init = 111, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 284.0 (TID 218)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 94, boot = -18, init = 111, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 284.0 (TID 218)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 284.0 (TID 218). 2531 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 284.0 (TID 218) in 102 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 284.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 284 (count at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.108 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 152 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 284: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 152 finished: count at /tmp/ipykernel_14509/1633301326.py:68, took 0.110374 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1127 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 153 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 285 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 285 (MapPartitionsRDD[1127] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_180 stored as values in memory (estimated size 65.8 KiB, free 423.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_180_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 423.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_180_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 180 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 285 (MapPartitionsRDD[1127] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 285.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 285.0 (TID 219) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 285.0 (TID 220) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 285.0 (TID 220)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 285.0 (TID 220)\n",
      "[Executor task launch worker for task 0.0 in stage 285.0 (TID 219)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 285.0 (TID 219)\n",
      "[Executor task launch worker for task 0.0 in stage 285.0 (TID 219)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 285.0 (TID 220)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 285.0 (TID 219)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 285.0 (TID 219). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 285.0 (TID 219) in 6 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 285.0 (TID 220)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 285.0 (TID 220). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 285.0 (TID 220) in 6 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 285.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 285 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.015 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-13] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 154 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 287 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 286)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 287 (MapPartitionsRDD[1134] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_181 stored as values in memory (estimated size 100.6 KiB, free 423.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_181_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 423.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_181_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 181 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 287 (MapPartitionsRDD[1134] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 287.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 287.0 (TID 221) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 287.0 (TID 221)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 287.0 (TID 221)\n",
      "[Executor task launch worker for task 0.0 in stage 287.0 (TID 221)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 287.0 (TID 221)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 287.0 (TID 221)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 287.0 (TID 221). 6795 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 287.0 (TID 221) in 21 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 287.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 287 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.024 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 154 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 287: Stage finished\n",
      "[broadcast-exchange-13] INFO org.apache.spark.scheduler.DAGScheduler - Job 154 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.025293 s\n",
      "[broadcast-exchange-13] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_182 stored as values in memory (estimated size 1025.5 KiB, free 422.1 MiB)\n",
      "[broadcast-exchange-13] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_182_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 422.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_182_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-13] INFO org.apache.spark.SparkContext - Created broadcast 182 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:71\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 155 (reduce at /tmp/ipykernel_14509/1633301326.py:71) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 291 (reduce at /tmp/ipykernel_14509/1633301326.py:71)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 290)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 291 (PythonRDD[1139] at reduce at /tmp/ipykernel_14509/1633301326.py:71), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_183 stored as values in memory (estimated size 27.6 KiB, free 422.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_183_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 422.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_183_piece0 in memory on 192.168.1.24:39111 (size: 12.6 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 183 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 291 (PythonRDD[1139] at reduce at /tmp/ipykernel_14509/1633301326.py:71) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 291.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 291.0 (TID 222) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 291.0 (TID 222)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 291.0 (TID 222)\n",
      "[Executor task launch worker for task 0.0 in stage 291.0 (TID 222)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 291.0 (TID 222)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 89, boot = -161, init = 250, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 291.0 (TID 222)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 90, boot = -159, init = 248, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 291.0 (TID 222)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 291.0 (TID 222). 2538 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 291.0 (TID 222) in 97 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 291.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 291 (reduce at /tmp/ipykernel_14509/1633301326.py:71) finished in 0.099 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 155 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 291: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 155 finished: reduce at /tmp/ipykernel_14509/1633301326.py:71, took 0.100355 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1150 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 69\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 156 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 292 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 292 (MapPartitionsRDD[1150] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_184 stored as values in memory (estimated size 65.8 KiB, free 422.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_184_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 422.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_184_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 184 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 292 (MapPartitionsRDD[1150] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 292.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 292.0 (TID 223) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 292.0 (TID 224) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 292.0 (TID 223)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 292.0 (TID 223)\n",
      "[Executor task launch worker for task 1.0 in stage 292.0 (TID 224)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 292.0 (TID 224)\n",
      "[Executor task launch worker for task 0.0 in stage 292.0 (TID 223)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 292.0 (TID 224)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 292.0 (TID 223)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 292.0 (TID 223). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 292.0 (TID 223) in 8 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 292.0 (TID 224)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 292.0 (TID 224). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 292.0 (TID 224) in 9 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 292.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 292 (count at NativeMethodAccessorImpl.java:0) finished in 0.012 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_184_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_174_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_183_piece0 on 192.168.1.24:39111 in memory (size: 12.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_179_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_181_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_178_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_180_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_177_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_176_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.2 MiB)\n",
      "[broadcast-exchange-14] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.719079 ms\n",
      "[broadcast-exchange-14] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 157 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 294 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 293)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 294 (MapPartitionsRDD[1157] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_185 stored as values in memory (estimated size 101.4 KiB, free 422.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_185_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 422.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_185_piece0 in memory on 192.168.1.24:39111 (size: 37.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 185 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 294 (MapPartitionsRDD[1157] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 294.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 294.0 (TID 225) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 294.0 (TID 225)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 294.0 (TID 225)\n",
      "[Executor task launch worker for task 0.0 in stage 294.0 (TID 225)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 294.0 (TID 225)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 294.0 (TID 225)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.278525 ms\n",
      "[Executor task launch worker for task 0.0 in stage 294.0 (TID 225)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 294.0 (TID 225). 8120 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 294.0 (TID 225) in 32 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 294.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 294 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.037 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 157 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 294: Stage finished\n",
      "[broadcast-exchange-14] INFO org.apache.spark.scheduler.DAGScheduler - Job 157 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.037523 s\n",
      "[broadcast-exchange-14] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_186 stored as values in memory (estimated size 1025.5 KiB, free 421.5 MiB)\n",
      "[broadcast-exchange-14] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_186_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 421.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_186_piece0 in memory on 192.168.1.24:39111 (size: 2.3 KiB, free: 434.1 MiB)\n",
      "[broadcast-exchange-14] INFO org.apache.spark.SparkContext - Created broadcast 186 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1159 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 70\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 158 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 298 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 297)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 298 (MapPartitionsRDD[1159] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_187 stored as values in memory (estimated size 24.0 KiB, free 421.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_187_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 421.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_187_piece0 in memory on 192.168.1.24:39111 (size: 12.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 187 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 298 (MapPartitionsRDD[1159] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 298.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 298.0 (TID 226) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7422 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 298.0 (TID 226)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 298.0 (TID 226)\n",
      "[Executor task launch worker for task 0.0 in stage 298.0 (TID 226)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 298.0 (TID 226)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 298.0 (TID 226)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 82, boot = -184, init = 266, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 298.0 (TID 226)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 298.0 (TID 226). 3162 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 298.0 (TID 226) in 87 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 298.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 298 (count at NativeMethodAccessorImpl.java:0) finished in 0.090 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 159 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 303 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 302)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 303 (MapPartitionsRDD[1162] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_188 stored as values in memory (estimated size 12.5 KiB, free 421.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_188_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 421.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_188_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 188 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 303 (MapPartitionsRDD[1162] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 303.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 303.0 (TID 227) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 303.0 (TID 227)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 303.0 (TID 227)\n",
      "[Executor task launch worker for task 0.0 in stage 303.0 (TID 227)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 303.0 (TID 227)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_185_piece0 on 192.168.1.24:39111 in memory (size: 37.4 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 303.0 (TID 227)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 303.0 (TID 227). 3995 bytes result sent to driver\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_187_piece0 on 192.168.1.24:39111 in memory (size: 12.0 KiB, free: 434.2 MiB)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 303.0 (TID 227) in 8 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 303.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 303 (count at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 159 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 303: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 159 finished: count at NativeMethodAccessorImpl.java:0, took 0.011247 s\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_153_piece0 on 192.168.1.24:39111 in memory (size: 2.6 KiB, free: 434.2 MiB)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1173 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 71\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 160 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 304 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 304 (MapPartitionsRDD[1173] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_189 stored as values in memory (estimated size 65.8 KiB, free 422.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_189_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 422.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_189_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 189 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 304 (MapPartitionsRDD[1173] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 304.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 304.0 (TID 228) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 304.0 (TID 229) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 304.0 (TID 228)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 304.0 (TID 228)\n",
      "[Executor task launch worker for task 1.0 in stage 304.0 (TID 229)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 304.0 (TID 229)\n",
      "[Executor task launch worker for task 1.0 in stage 304.0 (TID 229)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 304.0 (TID 228)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 304.0 (TID 228)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 304.0 (TID 228). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 304.0 (TID 228) in 10 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 304.0 (TID 229)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 304.0 (TID 229). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 304.0 (TID 229) in 9 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 304.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 304 (count at NativeMethodAccessorImpl.java:0) finished in 0.014 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.830036 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 161 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 306 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 305)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 306 (MapPartitionsRDD[1178] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_190 stored as values in memory (estimated size 102.5 KiB, free 422.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_190_piece0 stored as bytes in memory (estimated size 37.8 KiB, free 422.4 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating k=7: RMSE=2.0703973784636527, MAE=1.2973181444214417, MSE=4.286545304749166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_190_piece0 in memory on 192.168.1.24:39111 (size: 37.8 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 190 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 306 (MapPartitionsRDD[1178] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 306.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 306.0 (TID 230) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 306.0 (TID 230)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 306.0 (TID 230)\n",
      "[Executor task launch worker for task 0.0 in stage 306.0 (TID 230)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 306.0 (TID 230)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 306.0 (TID 230)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 306.0 (TID 230). 5939 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 306.0 (TID 230) in 23 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 306.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 306 (count at NativeMethodAccessorImpl.java:0) finished in 0.029 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 161 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 306: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 161 finished: count at NativeMethodAccessorImpl.java:0, took 0.030863 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_188_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1189 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 72\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 162 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 307 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_189_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 307 (MapPartitionsRDD[1189] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_186_piece0 on 192.168.1.24:39111 in memory (size: 2.3 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_190_piece0 on 192.168.1.24:39111 in memory (size: 37.8 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_191 stored as values in memory (estimated size 65.8 KiB, free 423.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_191_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 423.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_191_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 191 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 307 (MapPartitionsRDD[1189] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 307.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 307.0 (TID 231) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 307.0 (TID 232) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 307.0 (TID 231)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 307.0 (TID 231)\n",
      "[Executor task launch worker for task 1.0 in stage 307.0 (TID 232)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 307.0 (TID 232)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 307.0 (TID 231)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 1.0 in stage 307.0 (TID 232)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 307.0 (TID 231)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 307.0 (TID 231). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 307.0 (TID 231) in 11 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 307.0 (TID 232)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 307.0 (TID 232). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 307.0 (TID 232) in 19 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 307.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 307 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.027 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.967398 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1200 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 73\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 163 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 308 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 308 (MapPartitionsRDD[1200] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_192 stored as values in memory (estimated size 76.0 KiB, free 423.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_192_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 423.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_192_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 192 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 308 (MapPartitionsRDD[1200] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 308.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 308.0 (TID 233) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 308.0 (TID 234) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 308.0 (TID 234)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 308.0 (TID 234)\n",
      "[Executor task launch worker for task 0.0 in stage 308.0 (TID 233)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 308.0 (TID 233)\n",
      "[Executor task launch worker for task 1.0 in stage 308.0 (TID 234)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 308.0 (TID 233)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.394158 ms\n",
      "[Executor task launch worker for task 0.0 in stage 308.0 (TID 233)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 308.0 (TID 233). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 308.0 (TID 233) in 14 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1211 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 74\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 164 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 309 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[Executor task launch worker for task 1.0 in stage 308.0 (TID 234)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 308.0 (TID 234). 2325 bytes result sent to driver\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 309 (MapPartitionsRDD[1211] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 308.0 (TID 234) in 15 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 308.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_193 stored as values in memory (estimated size 76.0 KiB, free 423.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_193_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 423.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_193_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 193 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 309 (MapPartitionsRDD[1211] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 309.0 with 2 tasks resource profile 0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 308 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.028 s\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 309.0 (TID 235) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 309)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 309.0 (TID 236) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 309.0 (TID 236)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 309.0 (TID 236)\n",
      "[Executor task launch worker for task 0.0 in stage 309.0 (TID 235)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 309.0 (TID 235)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 309.0 (TID 235)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 1.0 in stage 309.0 (TID 236)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 309.0 (TID 235)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 309.0 (TID 235). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 309.0 (TID 235) in 8 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 309.0 (TID 236)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 309.0 (TID 236). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 309.0 (TID 236) in 10 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 309.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 309 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.013 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-16] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.85064 ms\n",
      "[broadcast-exchange-15] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.707077 ms\n",
      "[broadcast-exchange-16] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.043409 ms\n",
      "[broadcast-exchange-15] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 165 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 311 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 310)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 311 (MapPartitionsRDD[1217] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_194 stored as values in memory (estimated size 113.1 KiB, free 423.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_194_piece0 stored as bytes in memory (estimated size 40.0 KiB, free 423.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_194_piece0 in memory on 192.168.1.24:39111 (size: 40.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 194 from broadcast at DAGScheduler.scala:1585\n",
      "[broadcast-exchange-16] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.355885 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 311 (MapPartitionsRDD[1217] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 311.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 311.0 (TID 237) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 311.0 (TID 237)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 311.0 (TID 237)\n",
      "[Executor task launch worker for task 0.0 in stage 311.0 (TID 237)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 311.0 (TID 237)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 311.0 (TID 237)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.361375 ms\n",
      "[broadcast-exchange-16] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 166 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 313 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 312)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 313 (MapPartitionsRDD[1224] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_195 stored as values in memory (estimated size 116.6 KiB, free 421.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_195_piece0 stored as bytes in memory (estimated size 40.9 KiB, free 421.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_195_piece0 in memory on 192.168.1.24:39111 (size: 40.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 195 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 313 (MapPartitionsRDD[1224] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 313.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 313.0 (TID 238) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 313.0 (TID 238)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 313.0 (TID 238)\n",
      "[Executor task launch worker for task 0.0 in stage 313.0 (TID 238)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 313.0 (TID 238)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 311.0 (TID 237)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.139583 ms\n",
      "[Executor task launch worker for task 0.0 in stage 313.0 (TID 238)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.701522 ms\n",
      "[Executor task launch worker for task 0.0 in stage 311.0 (TID 237)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 311.0 (TID 237). 18726 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 311.0 (TID 237) in 44 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 311.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 311 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.049 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 165 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 311: Stage finished\n",
      "[broadcast-exchange-15] INFO org.apache.spark.scheduler.DAGScheduler - Job 165 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.050392 s\n",
      "[broadcast-exchange-15] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_196 stored as values in memory (estimated size 2.0 MiB, free 419.0 MiB)\n",
      "[broadcast-exchange-15] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_196_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 418.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_196_piece0 in memory on 192.168.1.24:39111 (size: 17.1 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-15] INFO org.apache.spark.SparkContext - Created broadcast 196 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Executor task launch worker for task 0.0 in stage 313.0 (TID 238)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.580137 ms\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 313.0 (TID 238)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 313.0 (TID 238). 19723 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 313.0 (TID 238) in 47 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 313.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 313 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.052 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 166 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 313: Stage finished\n",
      "[broadcast-exchange-16] INFO org.apache.spark.scheduler.DAGScheduler - Job 166 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.053429 s\n",
      "[broadcast-exchange-16] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_197 stored as values in memory (estimated size 47.0 KiB, free 421.0 MiB)\n",
      "[broadcast-exchange-16] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_197_piece0 stored as bytes in memory (estimated size 14.4 KiB, free 420.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_197_piece0 in memory on 192.168.1.24:39111 (size: 14.4 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-16] INFO org.apache.spark.SparkContext - Created broadcast 197 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 167 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 315 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 314)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 315 (PythonRDD[1232] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_198 stored as values in memory (estimated size 200.6 KiB, free 420.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_198_piece0 stored as bytes in memory (estimated size 65.6 KiB, free 420.7 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_198_piece0 in memory on 192.168.1.24:39111 (size: 65.6 KiB, free: 433.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 198 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 315 (PythonRDD[1232] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 315.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 315.0 (TID 239) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 315.0 (TID 239)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 315.0 (TID 239)\n",
      "[Executor task launch worker for task 0.0 in stage 315.0 (TID 239)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 315.0 (TID 239)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_193_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 433.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_191_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_195_piece0 on 192.168.1.24:39111 in memory (size: 40.9 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_194_piece0 on 192.168.1.24:39111 in memory (size: 40.0 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_192_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 434.1 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 315.0 (TID 239)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 95, boot = -1033, init = 1128, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 315.0 (TID 239)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 315.0 (TID 239). 11107 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 315.0 (TID 239) in 119 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 315.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 315 (runJob at PythonRDD.scala:181) finished in 0.128 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 167 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 315: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 167 finished: runJob at PythonRDD.scala:181, took 0.128976 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1239 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 75\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 168 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 317 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 316)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 317 (MapPartitionsRDD[1239] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_199 stored as values in memory (estimated size 210.1 KiB, free 421.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_199_piece0 stored as bytes in memory (estimated size 69.9 KiB, free 421.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_199_piece0 in memory on 192.168.1.24:39111 (size: 69.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 199 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 317 (MapPartitionsRDD[1239] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 317.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 317.0 (TID 240) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 317.0 (TID 240)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 317.0 (TID 240)\n",
      "[Executor task launch worker for task 0.0 in stage 317.0 (TID 240)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 317.0 (TID 240)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_198_piece0 on 192.168.1.24:39111 in memory (size: 65.6 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 317.0 (TID 240)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 2359, boot = -908, init = 994, finish = 2273\n",
      "[Executor task launch worker for task 0.0 in stage 317.0 (TID 240)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 317.0 (TID 240). 12027 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 317.0 (TID 240) in 2437 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 317.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 317 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 2.444 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(75), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.578109 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.562043 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1249 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) as input to shuffle 76\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 169 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 321 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 320)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 320)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 320 (PairwiseRDD[1249] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_200 stored as values in memory (estimated size 221.8 KiB, free 421.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_200_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 421.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_200_piece0 in memory on 192.168.1.24:39111 (size: 74.1 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 200 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 320 (PairwiseRDD[1249] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 320.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 320.0 (TID 241) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 320.0 (TID 241)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 320.0 (TID 241)\n",
      "[Executor task launch worker for task 0.0 in stage 320.0 (TID 241)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (29.2 KiB) non-empty blocks including 1 (29.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 320.0 (TID 241)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 320.0 (TID 241)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.559548 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 320.0 (TID 241)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 107, boot = -128, init = 225, finish = 10\n",
      "[Executor task launch worker for task 0.0 in stage 320.0 (TID 241)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 320.0 (TID 241). 13238 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 320.0 (TID 241) in 128 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 320.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 320 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) finished in 0.134 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 321)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 321 (PythonRDD[1252] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_201 stored as values in memory (estimated size 11.4 KiB, free 420.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_201_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 420.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_201_piece0 in memory on 192.168.1.24:39111 (size: 6.6 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 201 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 321 (PythonRDD[1252] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 321.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 321.0 (TID 242) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 321.0 (TID 242)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 321.0 (TID 242)\n",
      "[Executor task launch worker for task 0.0 in stage 321.0 (TID 242)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 321.0 (TID 242)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 321.0 (TID 242)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 83, boot = -1, init = 84, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 321.0 (TID 242)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 321.0 (TID 242). 2022 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 321.0 (TID 242) in 88 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 321.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 321 (runJob at PythonRDD.scala:181) finished in 0.091 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 169 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 321: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 169 finished: runJob at PythonRDD.scala:181, took 0.228434 s\n",
      "\r",
      "                                                                                \r",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1267 (javaToPython at <unknown>:0) as input to shuffle 77\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 170 (javaToPython at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 322 (javaToPython at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 322 (MapPartitionsRDD[1267] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_202 stored as values in memory (estimated size 65.8 KiB, free 420.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_202_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 420.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_202_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 202 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 322 (MapPartitionsRDD[1267] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 322.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 322.0 (TID 243) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 322.0 (TID 244) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 322.0 (TID 243)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 322.0 (TID 243)\n",
      "[Executor task launch worker for task 1.0 in stage 322.0 (TID 244)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 322.0 (TID 244)\n",
      "[Executor task launch worker for task 1.0 in stage 322.0 (TID 244)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 322.0 (TID 243)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 322.0 (TID 243)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 322.0 (TID 243). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 322.0 (TID 243) in 6 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 322.0 (TID 244)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 322.0 (TID 244). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 322.0 (TID 244) in 7 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 322.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 322 (javaToPython at <unknown>:0) finished in 0.009 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[broadcast-exchange-17] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.538763 ms\n",
      "[broadcast-exchange-17] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 171 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 324 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 323)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 324 (MapPartitionsRDD[1274] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_203 stored as values in memory (estimated size 100.6 KiB, free 420.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_202_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_203_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 420.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_200_piece0 on 192.168.1.24:39111 in memory (size: 74.1 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_203_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 203 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 324 (MapPartitionsRDD[1274] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 324.0 with 1 tasks resource profile 0\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_201_piece0 on 192.168.1.24:39111 in memory (size: 6.6 KiB, free: 434.0 MiB)\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 324.0 (TID 245) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 324.0 (TID 245)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 324.0 (TID 245)\n",
      "[Executor task launch worker for task 0.0 in stage 324.0 (TID 245)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 324.0 (TID 245)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 324.0 (TID 245)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.523299 ms\n",
      "[Executor task launch worker for task 0.0 in stage 324.0 (TID 245)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 324.0 (TID 245). 6819 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 324.0 (TID 245) in 30 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 324.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 324 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.037 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 171 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 324: Stage finished\n",
      "[broadcast-exchange-17] INFO org.apache.spark.scheduler.DAGScheduler - Job 171 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.038191 s\n",
      "[broadcast-exchange-17] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_204 stored as values in memory (estimated size 1025.5 KiB, free 420.1 MiB)\n",
      "[broadcast-exchange-17] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_204_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 420.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_204_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-17] INFO org.apache.spark.SparkContext - Created broadcast 204 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.901605 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 172 (reduce at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 328 (reduce at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 327)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 328 (PythonRDD[1279] at reduce at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_205 stored as values in memory (estimated size 27.5 KiB, free 420.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_205_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 420.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_205_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 205 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 328 (PythonRDD[1279] at reduce at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 328.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 328.0 (TID 246) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 328.0 (TID 246)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 328.0 (TID 246)\n",
      "[Executor task launch worker for task 0.0 in stage 328.0 (TID 246)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 328.0 (TID 246)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 328.0 (TID 246)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.853956 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 106, boot = -181, init = 287, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 328.0 (TID 246)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 115, boot = 3, init = 110, finish = 2\n",
      "[Executor task launch worker for task 0.0 in stage 328.0 (TID 246)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 328.0 (TID 246). 2538 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 328.0 (TID 246) in 125 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 328.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 328 (reduce at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.127 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 172 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 328: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 172 finished: reduce at /tmp/ipykernel_14509/1633301326.py:66, took 0.128612 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 173 (count at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 332 (count at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 331)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 332 (PythonRDD[1280] at count at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_206 stored as values in memory (estimated size 28.3 KiB, free 420.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_206_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 420.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_206_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 206 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 332 (PythonRDD[1280] at count at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 332.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 332.0 (TID 247) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 332.0 (TID 247)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 332.0 (TID 247)\n",
      "[Executor task launch worker for task 0.0 in stage 332.0 (TID 247)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 332.0 (TID 247)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 96, boot = -24, init = 119, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 332.0 (TID 247)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 98, boot = -8, init = 105, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 332.0 (TID 247)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 332.0 (TID 247). 2531 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 332.0 (TID 247) in 106 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 332.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 332 (count at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.109 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 173 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 332: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 173 finished: count at /tmp/ipykernel_14509/1633301326.py:66, took 0.110344 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 174 (reduce at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 336 (reduce at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 335)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 336 (PythonRDD[1281] at reduce at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_207 stored as values in memory (estimated size 27.5 KiB, free 420.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_207_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 420.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_207_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 207 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 336 (PythonRDD[1281] at reduce at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 336.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 336.0 (TID 248) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 336.0 (TID 248)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 336.0 (TID 248)\n",
      "[Executor task launch worker for task 0.0 in stage 336.0 (TID 248)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 336.0 (TID 248)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 100, boot = -16, init = 115, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 336.0 (TID 248)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 101, boot = -9, init = 109, finish = 1\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_203_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.0 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 336.0 (TID 248)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 336.0 (TID 248). 2624 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 336.0 (TID 248) in 116 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 336.0, whose tasks have all completed, from pool \n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_205_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_206_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 336 (reduce at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.120 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 174 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 336: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 174 finished: reduce at /tmp/ipykernel_14509/1633301326.py:68, took 0.122478 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 175 (count at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 340 (count at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 339)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 340 (PythonRDD[1282] at count at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_208 stored as values in memory (estimated size 28.3 KiB, free 420.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_208_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 420.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_208_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 208 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 340 (PythonRDD[1282] at count at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 340.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 340.0 (TID 249) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 340.0 (TID 249)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 340.0 (TID 249)\n",
      "[Executor task launch worker for task 0.0 in stage 340.0 (TID 249)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 340.0 (TID 249)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_199_piece0 on 192.168.1.24:39111 in memory (size: 69.9 KiB, free: 434.1 MiB)\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 121, boot = -13, init = 133, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 340.0 (TID 249)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 122, boot = -12, init = 133, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 340.0 (TID 249)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 340.0 (TID 249). 2531 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 340.0 (TID 249) in 131 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 340.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 340 (count at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.134 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 175 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 340: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 175 finished: count at /tmp/ipykernel_14509/1633301326.py:68, took 0.135446 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1293 (javaToPython at <unknown>:0) as input to shuffle 78\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 176 (javaToPython at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 341 (javaToPython at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 341 (MapPartitionsRDD[1293] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_209 stored as values in memory (estimated size 65.8 KiB, free 420.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_209_piece0 stored as bytes in memory (estimated size 22.3 KiB, free 420.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_209_piece0 in memory on 192.168.1.24:39111 (size: 22.3 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 209 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 341 (MapPartitionsRDD[1293] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 341.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 341.0 (TID 250) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 341.0 (TID 251) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 341.0 (TID 251)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 341.0 (TID 251)\n",
      "[Executor task launch worker for task 0.0 in stage 341.0 (TID 250)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 341.0 (TID 250)\n",
      "[Executor task launch worker for task 0.0 in stage 341.0 (TID 250)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 341.0 (TID 251)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 341.0 (TID 250)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 341.0 (TID 250). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 341.0 (TID 250) in 7 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 341.0 (TID 251)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 341.0 (TID 251). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 341.0 (TID 251) in 7 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 341.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 341 (javaToPython at <unknown>:0) finished in 0.010 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-18] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 177 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 343 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 342)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 343 (MapPartitionsRDD[1300] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_210 stored as values in memory (estimated size 100.6 KiB, free 420.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_210_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 420.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_210_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 210 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 343 (MapPartitionsRDD[1300] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 343.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 343.0 (TID 252) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 343.0 (TID 252)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 343.0 (TID 252)\n",
      "[Executor task launch worker for task 0.0 in stage 343.0 (TID 252)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 343.0 (TID 252)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 343.0 (TID 252)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 343.0 (TID 252). 6819 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 343.0 (TID 252) in 19 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 343.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 343 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.025 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 177 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 343: Stage finished\n",
      "[broadcast-exchange-18] INFO org.apache.spark.scheduler.DAGScheduler - Job 177 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.026946 s\n",
      "[broadcast-exchange-18] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_211 stored as values in memory (estimated size 1025.5 KiB, free 419.2 MiB)\n",
      "[broadcast-exchange-18] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_211_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 419.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_211_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-18] INFO org.apache.spark.SparkContext - Created broadcast 211 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:71\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 178 (reduce at /tmp/ipykernel_14509/1633301326.py:71) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 347 (reduce at /tmp/ipykernel_14509/1633301326.py:71)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 346)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 347 (PythonRDD[1305] at reduce at /tmp/ipykernel_14509/1633301326.py:71), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_212 stored as values in memory (estimated size 27.6 KiB, free 419.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_212_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 419.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_212_piece0 in memory on 192.168.1.24:39111 (size: 12.6 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 212 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 347 (PythonRDD[1305] at reduce at /tmp/ipykernel_14509/1633301326.py:71) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 347.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 347.0 (TID 253) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 347.0 (TID 253)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 347.0 (TID 253)\n",
      "[Executor task launch worker for task 0.0 in stage 347.0 (TID 253)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 347.0 (TID 253)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 94, boot = -167, init = 260, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 347.0 (TID 253)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 102, boot = -166, init = 267, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 347.0 (TID 253)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 347.0 (TID 253). 2538 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 347.0 (TID 253) in 107 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 347.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 347 (reduce at /tmp/ipykernel_14509/1633301326.py:71) finished in 0.110 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 178 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 347: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 178 finished: reduce at /tmp/ipykernel_14509/1633301326.py:71, took 0.110954 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1316 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 79\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 179 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 348 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 348 (MapPartitionsRDD[1316] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_213 stored as values in memory (estimated size 65.8 KiB, free 419.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_213_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 419.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_213_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 213 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 348 (MapPartitionsRDD[1316] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 348.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 348.0 (TID 254) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 348.0 (TID 255) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 348.0 (TID 254)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 348.0 (TID 254)\n",
      "[Executor task launch worker for task 1.0 in stage 348.0 (TID 255)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 348.0 (TID 255)\n",
      "[Executor task launch worker for task 1.0 in stage 348.0 (TID 255)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 348.0 (TID 254)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 348.0 (TID 254)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 348.0 (TID 254). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 348.0 (TID 254) in 7 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 348.0 (TID 255)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 348.0 (TID 255). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 348.0 (TID 255) in 6 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 348.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 348 (count at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_208_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_212_piece0 on 192.168.1.24:39111 in memory (size: 12.6 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_207_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_213_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_210_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_209_piece0 on 192.168.1.24:39111 in memory (size: 22.3 KiB, free: 434.1 MiB)\n",
      "[broadcast-exchange-19] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.825826 ms\n",
      "[broadcast-exchange-19] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 180 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 350 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 349)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 350 (MapPartitionsRDD[1323] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_214 stored as values in memory (estimated size 101.4 KiB, free 419.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_214_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 419.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_214_piece0 in memory on 192.168.1.24:39111 (size: 37.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 214 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 350 (MapPartitionsRDD[1323] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 350.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 350.0 (TID 256) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 350.0 (TID 256)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 350.0 (TID 256)\n",
      "[Executor task launch worker for task 0.0 in stage 350.0 (TID 256)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 350.0 (TID 256)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 350.0 (TID 256)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.423025 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 350.0 (TID 256)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 350.0 (TID 256). 8120 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 350.0 (TID 256) in 46 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 350.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 350 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.050 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 180 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 350: Stage finished\n",
      "[broadcast-exchange-19] INFO org.apache.spark.scheduler.DAGScheduler - Job 180 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.050865 s\n",
      "[broadcast-exchange-19] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_215 stored as values in memory (estimated size 1025.5 KiB, free 418.4 MiB)\n",
      "[broadcast-exchange-19] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_215_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 418.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_215_piece0 in memory on 192.168.1.24:39111 (size: 2.3 KiB, free: 434.1 MiB)\n",
      "[broadcast-exchange-19] INFO org.apache.spark.SparkContext - Created broadcast 215 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1325 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 80\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 181 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 354 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 353)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 354 (MapPartitionsRDD[1325] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_216 stored as values in memory (estimated size 24.0 KiB, free 418.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_216_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 418.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_216_piece0 in memory on 192.168.1.24:39111 (size: 12.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 216 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 354 (MapPartitionsRDD[1325] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 354.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 354.0 (TID 257) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7422 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 354.0 (TID 257)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 354.0 (TID 257)\n",
      "[Executor task launch worker for task 0.0 in stage 354.0 (TID 257)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.7 KiB) non-empty blocks including 1 (2.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 354.0 (TID 257)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 354.0 (TID 257)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 100, boot = -214, init = 313, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 354.0 (TID 257)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 354.0 (TID 257). 3162 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 354.0 (TID 257) in 109 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 354.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 354 (count at NativeMethodAccessorImpl.java:0) finished in 0.113 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 182 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 359 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 358)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 359 (MapPartitionsRDD[1328] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_217 stored as values in memory (estimated size 12.5 KiB, free 418.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_217_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 418.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_217_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 217 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 359 (MapPartitionsRDD[1328] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 359.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 359.0 (TID 258) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 359.0 (TID 258)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 359.0 (TID 258)\n",
      "[Executor task launch worker for task 0.0 in stage 359.0 (TID 258)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 359.0 (TID 258)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 359.0 (TID 258)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 359.0 (TID 258). 3952 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 359.0 (TID 258) in 5 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 359.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 359 (count at NativeMethodAccessorImpl.java:0) finished in 0.008 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 182 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 359: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 182 finished: count at NativeMethodAccessorImpl.java:0, took 0.009666 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating k=9: RMSE=2.013853026696256, MAE=1.2446992153766103, MSE=4.055604013133671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1339 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 81\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 183 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 360 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 360 (MapPartitionsRDD[1339] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_218 stored as values in memory (estimated size 65.8 KiB, free 418.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_218_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 418.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_218_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 218 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 360 (MapPartitionsRDD[1339] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 360.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 360.0 (TID 259) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 360.0 (TID 260) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 360.0 (TID 259)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 360.0 (TID 259)\n",
      "[Executor task launch worker for task 1.0 in stage 360.0 (TID 260)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 360.0 (TID 260)\n",
      "[Executor task launch worker for task 1.0 in stage 360.0 (TID 260)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 360.0 (TID 259)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 360.0 (TID 260)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 360.0 (TID 260). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 360.0 (TID 260) in 7 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 360.0 (TID 259)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 360.0 (TID 259). 2325 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 360.0 (TID 259) in 7 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 360.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 360 (count at NativeMethodAccessorImpl.java:0) finished in 0.010 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 184 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 362 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 361)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 362 (MapPartitionsRDD[1344] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_219 stored as values in memory (estimated size 102.5 KiB, free 418.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_219_piece0 stored as bytes in memory (estimated size 37.8 KiB, free 418.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_219_piece0 in memory on 192.168.1.24:39111 (size: 37.8 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 219 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 362 (MapPartitionsRDD[1344] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 362.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 362.0 (TID 261) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 362.0 (TID 261)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 362.0 (TID 261)\n",
      "[Executor task launch worker for task 0.0 in stage 362.0 (TID 261)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 362.0 (TID 261)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 362.0 (TID 261)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 362.0 (TID 261). 5939 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 362.0 (TID 261) in 19 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 362.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 362 (count at NativeMethodAccessorImpl.java:0) finished in 0.022 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 184 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 362: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 184 finished: count at NativeMethodAccessorImpl.java:0, took 0.022833 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_219_piece0 on 192.168.1.24:39111 in memory (size: 37.8 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_216_piece0 on 192.168.1.24:39111 in memory (size: 12.0 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_214_piece0 on 192.168.1.24:39111 in memory (size: 37.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_218_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_217_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1355 (javaToPython at <unknown>:0) as input to shuffle 82\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 185 (javaToPython at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 363 (javaToPython at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 363 (MapPartitionsRDD[1355] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_220 stored as values in memory (estimated size 65.8 KiB, free 418.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_220_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 418.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_220_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 220 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 363 (MapPartitionsRDD[1355] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 363.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 363.0 (TID 262) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 363.0 (TID 263) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 363.0 (TID 263)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 363.0 (TID 263)\n",
      "[Executor task launch worker for task 1.0 in stage 363.0 (TID 263)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 363.0 (TID 262)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 363.0 (TID 262)\n",
      "[Executor task launch worker for task 0.0 in stage 363.0 (TID 262)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1366 (javaToPython at <unknown>:0) as input to shuffle 83\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 186 (javaToPython at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 364 (javaToPython at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 364 (MapPartitionsRDD[1366] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[Executor task launch worker for task 1.0 in stage 363.0 (TID 263)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 363.0 (TID 263). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 363.0 (TID 263) in 14 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 0.0 in stage 363.0 (TID 262)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 363.0 (TID 262). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 363.0 (TID 262) in 17 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 363.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_221 stored as values in memory (estimated size 76.0 KiB, free 418.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_221_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 418.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_221_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 221 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 364 (MapPartitionsRDD[1366] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 364.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 364.0 (TID 264) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 363 (javaToPython at <unknown>:0) finished in 0.027 s\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 364.0 (TID 265) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 364)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Executor task launch worker for task 0.0 in stage 364.0 (TID 264)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 364.0 (TID 264)\n",
      "[Executor task launch worker for task 1.0 in stage 364.0 (TID 265)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 364.0 (TID 265)\n",
      "[Executor task launch worker for task 1.0 in stage 364.0 (TID 265)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 364.0 (TID 264)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 364.0 (TID 264)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 364.0 (TID 264). 2325 bytes result sent to driver\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1377 (javaToPython at <unknown>:0) as input to shuffle 84\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 187 (javaToPython at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 365 (javaToPython at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 364.0 (TID 264) in 15 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 365 (MapPartitionsRDD[1377] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[Executor task launch worker for task 1.0 in stage 364.0 (TID 265)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 364.0 (TID 265). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 364.0 (TID 265) in 20 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 364.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_222 stored as values in memory (estimated size 76.0 KiB, free 418.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_222_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 418.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_222_piece0 in memory on 192.168.1.24:39111 (size: 24.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 222 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 365 (MapPartitionsRDD[1377] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 365.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 365.0 (TID 266) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 364 (javaToPython at <unknown>:0) finished in 0.035 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 365.0 (TID 267) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 365)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Executor task launch worker for task 0.0 in stage 365.0 (TID 266)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 365.0 (TID 266)\n",
      "[Executor task launch worker for task 1.0 in stage 365.0 (TID 267)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 365.0 (TID 267)\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 365.0 (TID 266)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 365.0 (TID 267)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 365.0 (TID 266)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 365.0 (TID 266). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 365.0 (TID 266) in 20 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 365.0 (TID 267)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 365.0 (TID 267). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 365.0 (TID 267) in 22 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 365.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 365 (javaToPython at <unknown>:0) finished in 0.030 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[broadcast-exchange-20] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.070803 ms\n",
      "[broadcast-exchange-21] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.891617 ms\n",
      "[broadcast-exchange-20] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 188 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 367 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 366)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 367 (MapPartitionsRDD[1383] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_223 stored as values in memory (estimated size 113.1 KiB, free 418.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_223_piece0 stored as bytes in memory (estimated size 40.0 KiB, free 418.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_223_piece0 in memory on 192.168.1.24:39111 (size: 40.0 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 223 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 367 (MapPartitionsRDD[1383] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 367.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 367.0 (TID 268) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 367.0 (TID 268)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 367.0 (TID 268)\n",
      "[broadcast-exchange-21] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Executor task launch worker for task 0.0 in stage 367.0 (TID 268)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 367.0 (TID 268)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 189 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 369 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 368)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 369 (MapPartitionsRDD[1390] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_224 stored as values in memory (estimated size 116.6 KiB, free 417.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_224_piece0 stored as bytes in memory (estimated size 40.9 KiB, free 415.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_224_piece0 in memory on 192.168.1.24:39111 (size: 40.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 224 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 369 (MapPartitionsRDD[1390] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 369.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 369.0 (TID 269) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 369.0 (TID 269)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 369.0 (TID 269)\n",
      "[Executor task launch worker for task 0.0 in stage 369.0 (TID 269)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 369.0 (TID 269)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_221_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_222_piece0 on 192.168.1.24:39111 in memory (size: 24.0 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_220_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 367.0 (TID 268)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.93752 ms\n",
      "[Executor task launch worker for task 0.0 in stage 369.0 (TID 269)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.388795 ms\n",
      "[Executor task launch worker for task 0.0 in stage 367.0 (TID 268)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 367.0 (TID 268). 18848 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 367.0 (TID 268) in 39 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 367.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 367 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.044 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 188 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 367: Stage finished\n",
      "[broadcast-exchange-20] INFO org.apache.spark.scheduler.DAGScheduler - Job 188 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.045501 s\n",
      "[broadcast-exchange-20] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_225 stored as values in memory (estimated size 2.0 MiB, free 414.1 MiB)\n",
      "[broadcast-exchange-20] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_225_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 414.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_225_piece0 in memory on 192.168.1.24:39111 (size: 17.3 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-20] INFO org.apache.spark.SparkContext - Created broadcast 225 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Executor task launch worker for task 0.0 in stage 369.0 (TID 269)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 369.0 (TID 269). 19846 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 369.0 (TID 269) in 41 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 369.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 369 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.045 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 189 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 369: Stage finished\n",
      "[broadcast-exchange-21] INFO org.apache.spark.scheduler.DAGScheduler - Job 189 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.046384 s\n",
      "[broadcast-exchange-21] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_226 stored as values in memory (estimated size 47.0 KiB, free 416.1 MiB)\n",
      "[broadcast-exchange-21] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_226_piece0 stored as bytes in memory (estimated size 14.4 KiB, free 416.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_226_piece0 in memory on 192.168.1.24:39111 (size: 14.4 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-21] INFO org.apache.spark.SparkContext - Created broadcast 226 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 190 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 371 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 370)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 371 (PythonRDD[1398] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_227 stored as values in memory (estimated size 200.6 KiB, free 415.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_227_piece0 stored as bytes in memory (estimated size 65.7 KiB, free 415.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_227_piece0 in memory on 192.168.1.24:39111 (size: 65.7 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 227 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 371 (PythonRDD[1398] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 371.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 371.0 (TID 270) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 371.0 (TID 270)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 371.0 (TID 270)\n",
      "[Executor task launch worker for task 0.0 in stage 371.0 (TID 270)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 371.0 (TID 270)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 371.0 (TID 270)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 92, boot = -948, init = 1039, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 371.0 (TID 270)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 371.0 (TID 270). 11064 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 371.0 (TID 270) in 109 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 371.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 371 (runJob at PythonRDD.scala:181) finished in 0.116 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 190 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 371: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 190 finished: runJob at PythonRDD.scala:181, took 0.117675 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1405 (javaToPython at <unknown>:0) as input to shuffle 85\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 191 (javaToPython at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 373 (javaToPython at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 372)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 373 (MapPartitionsRDD[1405] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_228 stored as values in memory (estimated size 210.1 KiB, free 415.7 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_228_piece0 stored as bytes in memory (estimated size 69.9 KiB, free 415.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_228_piece0 in memory on 192.168.1.24:39111 (size: 69.9 KiB, free: 433.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 228 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 373 (MapPartitionsRDD[1405] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 373.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 373.0 (TID 271) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 373.0 (TID 271)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 373.0 (TID 271)\n",
      "[Executor task launch worker for task 0.0 in stage 373.0 (TID 271)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 373.0 (TID 271)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_223_piece0 on 192.168.1.24:39111 in memory (size: 40.0 KiB, free: 433.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_224_piece0 on 192.168.1.24:39111 in memory (size: 40.9 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_227_piece0 on 192.168.1.24:39111 in memory (size: 65.7 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_175_piece0 on 192.168.1.24:39111 in memory (size: 2.6 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_146_piece0 on 192.168.1.24:39111 in memory (size: 2.6 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_139_piece0 on 192.168.1.24:39111 in memory (size: 17.5 KiB, free: 434.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_182_piece0 on 192.168.1.24:39111 in memory (size: 2.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_110_piece0 on 192.168.1.24:39111 in memory (size: 17.4 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_138_piece0 on 192.168.1.24:39111 in memory (size: 14.7 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_117_piece0 on 192.168.1.24:39111 in memory (size: 2.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_215_piece0 on 192.168.1.24:39111 in memory (size: 2.3 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_168_piece0 on 192.168.1.24:39111 in memory (size: 17.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_167_piece0 on 192.168.1.24:39111 in memory (size: 14.5 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_109_piece0 on 192.168.1.24:39111 in memory (size: 14.7 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 373.0 (TID 271)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 2550, boot = -779, init = 878, finish = 2451\n",
      "[Executor task launch worker for task 0.0 in stage 373.0 (TID 271)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 373.0 (TID 271). 12027 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 373.0 (TID 271) in 2623 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 373.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 373 (javaToPython at <unknown>:0) finished in 2.632 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(85), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.748673 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1415 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) as input to shuffle 86\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 192 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 377 (runJob at PythonRDD.scala:181)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 376)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 376)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 376 (PairwiseRDD[1415] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_229 stored as values in memory (estimated size 221.8 KiB, free 427.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_229_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 427.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_229_piece0 in memory on 192.168.1.24:39111 (size: 74.1 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 229 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 376 (PairwiseRDD[1415] at reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 376.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 376.0 (TID 272) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 376.0 (TID 272)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 376.0 (TID 272)\n",
      "[Executor task launch worker for task 0.0 in stage 376.0 (TID 272)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (33.5 KiB) non-empty blocks including 1 (33.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 376.0 (TID 272)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 376.0 (TID 272)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.529442 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 376.0 (TID 272)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 96, boot = -112, init = 192, finish = 16\n",
      "[Executor task launch worker for task 0.0 in stage 376.0 (TID 272)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 376.0 (TID 272). 13238 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 376.0 (TID 272) in 115 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 376.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 376 (reduceByKey at /tmp/ipykernel_14509/1633301326.py:58) finished in 0.120 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 377)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 377 (PythonRDD[1418] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_230 stored as values in memory (estimated size 11.4 KiB, free 427.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_230_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 427.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_230_piece0 in memory on 192.168.1.24:39111 (size: 6.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 230 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 377 (PythonRDD[1418] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 377.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 377.0 (TID 273) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 377.0 (TID 273)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 377.0 (TID 273)\n",
      "[Executor task launch worker for task 0.0 in stage 377.0 (TID 273)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 377.0 (TID 273)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 377.0 (TID 273)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 82, boot = 0, init = 81, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 377.0 (TID 273)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 377.0 (TID 273). 2022 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 377.0 (TID 273) in 86 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 377.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 377 (runJob at PythonRDD.scala:181) finished in 0.089 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 192 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 377: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 192 finished: runJob at PythonRDD.scala:181, took 0.213595 s\n",
      "\r",
      "                                                                                \r",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1433 (javaToPython at <unknown>:0) as input to shuffle 87\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 193 (javaToPython at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 378 (javaToPython at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 378 (MapPartitionsRDD[1433] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_231 stored as values in memory (estimated size 65.8 KiB, free 427.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_231_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 427.0 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_231_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 231 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 378 (MapPartitionsRDD[1433] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 378.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 378.0 (TID 274) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 378.0 (TID 275) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 378.0 (TID 275)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 378.0 (TID 275)\n",
      "[Executor task launch worker for task 0.0 in stage 378.0 (TID 274)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 378.0 (TID 274)\n",
      "[Executor task launch worker for task 1.0 in stage 378.0 (TID 275)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 378.0 (TID 274)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 378.0 (TID 274)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 378.0 (TID 274). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 378.0 (TID 274) in 5 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 378.0 (TID 275)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 378.0 (TID 275). 2325 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 378.0 (TID 275) in 6 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 378.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 378 (javaToPython at <unknown>:0) finished in 0.009 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-22] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.580601 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[broadcast-exchange-22] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 194 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 380 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 379)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 380 (MapPartitionsRDD[1440] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_232 stored as values in memory (estimated size 100.6 KiB, free 426.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_232_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 426.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_232_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 232 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 380 (MapPartitionsRDD[1440] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 380.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 380.0 (TID 276) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 380.0 (TID 276)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 380.0 (TID 276)\n",
      "[Executor task launch worker for task 0.0 in stage 380.0 (TID 276)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 380.0 (TID 276)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 380.0 (TID 276)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.975591 ms\n",
      "[Executor task launch worker for task 0.0 in stage 380.0 (TID 276)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 380.0 (TID 276). 6811 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 380.0 (TID 276) in 31 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 380.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 380 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.033 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 194 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 380: Stage finished\n",
      "[broadcast-exchange-22] INFO org.apache.spark.scheduler.DAGScheduler - Job 194 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.034339 s\n",
      "[broadcast-exchange-22] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_233 stored as values in memory (estimated size 1025.5 KiB, free 425.9 MiB)\n",
      "[broadcast-exchange-22] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_233_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 425.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_233_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-22] INFO org.apache.spark.SparkContext - Created broadcast 233 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 195 (reduce at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 384 (reduce at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 383)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 384 (PythonRDD[1445] at reduce at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_234 stored as values in memory (estimated size 27.5 KiB, free 425.9 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_234_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 425.9 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_234_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 234 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 384 (PythonRDD[1445] at reduce at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 384.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 384.0 (TID 277) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 384.0 (TID 277)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 384.0 (TID 277)\n",
      "[Executor task launch worker for task 0.0 in stage 384.0 (TID 277)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 384.0 (TID 277)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 84, boot = -169, init = 253, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 384.0 (TID 277)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 90, boot = 3, init = 85, finish = 2\n",
      "[Executor task launch worker for task 0.0 in stage 384.0 (TID 277)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 384.0 (TID 277). 2538 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 384.0 (TID 277) in 95 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 384.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 384 (reduce at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.098 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 195 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 384: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 195 finished: reduce at /tmp/ipykernel_14509/1633301326.py:66, took 0.098603 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:66\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 196 (count at /tmp/ipykernel_14509/1633301326.py:66) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 388 (count at /tmp/ipykernel_14509/1633301326.py:66)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 387)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 388 (PythonRDD[1446] at count at /tmp/ipykernel_14509/1633301326.py:66), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_235 stored as values in memory (estimated size 28.3 KiB, free 425.8 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_235_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 425.8 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_235_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 235 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 388 (PythonRDD[1446] at count at /tmp/ipykernel_14509/1633301326.py:66) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 388.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 388.0 (TID 278) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 388.0 (TID 278)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 388.0 (TID 278)\n",
      "[Executor task launch worker for task 0.0 in stage 388.0 (TID 278)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 388.0 (TID 278)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 87, boot = -15, init = 101, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 388.0 (TID 278)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 88, boot = -9, init = 96, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 388.0 (TID 278)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 388.0 (TID 278). 2531 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 388.0 (TID 278) in 100 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 388.0, whose tasks have all completed, from pool \n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_231_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 388 (count at /tmp/ipykernel_14509/1633301326.py:66) finished in 0.105 s\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_232_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 196 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 388: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 196 finished: count at /tmp/ipykernel_14509/1633301326.py:66, took 0.105708 s\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_229_piece0 on 192.168.1.24:39111 in memory (size: 74.1 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_230_piece0 on 192.168.1.24:39111 in memory (size: 6.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_234_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 197 (reduce at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 392 (reduce at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 391)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 392 (PythonRDD[1447] at reduce at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_236 stored as values in memory (estimated size 27.5 KiB, free 426.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_236_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 426.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_236_piece0 in memory on 192.168.1.24:39111 (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 236 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 392 (PythonRDD[1447] at reduce at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 392.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 392.0 (TID 279) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 392.0 (TID 279)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 392.0 (TID 279)\n",
      "[Executor task launch worker for task 0.0 in stage 392.0 (TID 279)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 392.0 (TID 279)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 85, boot = -12, init = 96, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 392.0 (TID 279)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 86, boot = -12, init = 97, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 392.0 (TID 279)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 392.0 (TID 279). 2538 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 392.0 (TID 279) in 91 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 392.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 392 (reduce at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.094 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 197 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 392: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 197 finished: reduce at /tmp/ipykernel_14509/1633301326.py:68, took 0.095207 s\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at /tmp/ipykernel_14509/1633301326.py:68\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 198 (count at /tmp/ipykernel_14509/1633301326.py:68) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 396 (count at /tmp/ipykernel_14509/1633301326.py:68)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 395)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 396 (PythonRDD[1448] at count at /tmp/ipykernel_14509/1633301326.py:68), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_237 stored as values in memory (estimated size 28.3 KiB, free 426.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_237_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 426.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_237_piece0 in memory on 192.168.1.24:39111 (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 237 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 396 (PythonRDD[1448] at count at /tmp/ipykernel_14509/1633301326.py:68) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 396.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 396.0 (TID 280) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 396.0 (TID 280)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 396.0 (TID 280)\n",
      "[Executor task launch worker for task 0.0 in stage 396.0 (TID 280)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 396.0 (TID 280)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 87, boot = -10, init = 96, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 396.0 (TID 280)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 88, boot = -9, init = 96, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 396.0 (TID 280)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 396.0 (TID 280). 2531 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 396.0 (TID 280) in 94 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 396.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 396 (count at /tmp/ipykernel_14509/1633301326.py:68) finished in 0.095 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 198 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 396: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 198 finished: count at /tmp/ipykernel_14509/1633301326.py:68, took 0.097097 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1459 (javaToPython at <unknown>:0) as input to shuffle 88\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 199 (javaToPython at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 397 (javaToPython at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 397 (MapPartitionsRDD[1459] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_238 stored as values in memory (estimated size 65.8 KiB, free 426.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_238_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 426.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_238_piece0 in memory on 192.168.1.24:39111 (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 238 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 397 (MapPartitionsRDD[1459] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 397.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 397.0 (TID 281) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 397.0 (TID 282) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 397.0 (TID 282)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 397.0 (TID 282)\n",
      "[Executor task launch worker for task 0.0 in stage 397.0 (TID 281)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 397.0 (TID 281)\n",
      "[Executor task launch worker for task 1.0 in stage 397.0 (TID 282)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 397.0 (TID 281)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 397.0 (TID 281)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 397.0 (TID 281). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 397.0 (TID 281) in 6 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 397.0 (TID 282)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 397.0 (TID 282). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 397.0 (TID 282) in 5 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 397.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 397 (javaToPython at <unknown>:0) finished in 0.009 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-23] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 200 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 399 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 398)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 399 (MapPartitionsRDD[1466] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_239 stored as values in memory (estimated size 100.6 KiB, free 426.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_239_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 426.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_239_piece0 in memory on 192.168.1.24:39111 (size: 36.9 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 239 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 399 (MapPartitionsRDD[1466] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 399.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 399.0 (TID 283) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 399.0 (TID 283)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 399.0 (TID 283)\n",
      "[Executor task launch worker for task 0.0 in stage 399.0 (TID 283)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 399.0 (TID 283)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 399.0 (TID 283)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 399.0 (TID 283). 6811 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 399.0 (TID 283) in 16 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 399.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 399 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.020 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 200 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 399: Stage finished\n",
      "[broadcast-exchange-23] INFO org.apache.spark.scheduler.DAGScheduler - Job 200 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.020792 s\n",
      "[broadcast-exchange-23] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_240 stored as values in memory (estimated size 1025.5 KiB, free 425.1 MiB)\n",
      "[broadcast-exchange-23] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_240_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 425.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_240_piece0 in memory on 192.168.1.24:39111 (size: 2.6 KiB, free: 434.0 MiB)\n",
      "[broadcast-exchange-23] INFO org.apache.spark.SparkContext - Created broadcast 240 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: reduce at /tmp/ipykernel_14509/1633301326.py:71\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 201 (reduce at /tmp/ipykernel_14509/1633301326.py:71) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 403 (reduce at /tmp/ipykernel_14509/1633301326.py:71)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 402)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 403 (PythonRDD[1471] at reduce at /tmp/ipykernel_14509/1633301326.py:71), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_241 stored as values in memory (estimated size 27.6 KiB, free 425.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_241_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 425.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_239_piece0 on 192.168.1.24:39111 in memory (size: 36.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_241_piece0 in memory on 192.168.1.24:39111 (size: 12.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 241 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 403 (PythonRDD[1471] at reduce at /tmp/ipykernel_14509/1633301326.py:71) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 403.0 with 1 tasks resource profile 0\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_236_piece0 on 192.168.1.24:39111 in memory (size: 12.5 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_237_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 403.0 (TID 284) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_238_piece0 on 192.168.1.24:39111 in memory (size: 22.2 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 403.0 (TID 284)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 403.0 (TID 284)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_235_piece0 on 192.168.1.24:39111 in memory (size: 12.7 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 0.0 in stage 403.0 (TID 284)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 403.0 (TID 284)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_228_piece0 on 192.168.1.24:39111 in memory (size: 69.9 KiB, free: 434.2 MiB)\n",
      "[stdout writer for /usr/bin/python3] INFO org.apache.spark.api.python.PythonRunner - Times: total = 107, boot = -140, init = 247, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 403.0 (TID 284)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 109, boot = -138, init = 247, finish = 0\n",
      "[Executor task launch worker for task 0.0 in stage 403.0 (TID 284)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 403.0 (TID 284). 2538 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 403.0 (TID 284) in 116 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 403.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 403 (reduce at /tmp/ipykernel_14509/1633301326.py:71) finished in 0.124 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 201 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 403: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 201 finished: reduce at /tmp/ipykernel_14509/1633301326.py:71, took 0.125926 s\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1482 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 89\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 202 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 404 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 404 (MapPartitionsRDD[1482] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_242 stored as values in memory (estimated size 65.8 KiB, free 425.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_242_piece0 stored as bytes in memory (estimated size 22.3 KiB, free 425.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_242_piece0 in memory on 192.168.1.24:39111 (size: 22.3 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 242 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 404 (MapPartitionsRDD[1482] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 404.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 404.0 (TID 285) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 404.0 (TID 286) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 404.0 (TID 286)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 404.0 (TID 286)\n",
      "[Executor task launch worker for task 0.0 in stage 404.0 (TID 285)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 404.0 (TID 285)\n",
      "[Executor task launch worker for task 1.0 in stage 404.0 (TID 286)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 404.0 (TID 285)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 404.0 (TID 285)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 404.0 (TID 285). 2325 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 404.0 (TID 285) in 7 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 404.0 (TID 286)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 404.0 (TID 286). 2325 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 404.0 (TID 286) in 7 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 404.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 404 (count at NativeMethodAccessorImpl.java:0) finished in 0.009 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[broadcast-exchange-24] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.573602 ms\n",
      "[broadcast-exchange-24] INFO org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 203 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 406 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 405)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 406 (MapPartitionsRDD[1489] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_243 stored as values in memory (estimated size 101.4 KiB, free 425.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_243_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 425.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_243_piece0 in memory on 192.168.1.24:39111 (size: 37.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 243 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 406 (MapPartitionsRDD[1489] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 406.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 406.0 (TID 287) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 406.0 (TID 287)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 406.0 (TID 287)\n",
      "[Executor task launch worker for task 0.0 in stage 406.0 (TID 287)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (125.9 KiB) non-empty blocks including 2 (125.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 406.0 (TID 287)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating k=11: RMSE=1.8762435983891022, MAE=1.2494196868078415, MSE=3.5202900404960866\n",
      "Best k values: {'Best k for RMSE': 5, 'Best k for MAE': 5, 'Best k for MSE': 5, 'Best metrics': {'RMSE': 1.5366776795611328, 'MAE': 1.1251934247056476, 'MSE': 2.361378290861387}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 406.0 (TID 287)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.737302 ms\n",
      "[Executor task launch worker for task 0.0 in stage 406.0 (TID 287)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 406.0 (TID 287). 8120 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 406.0 (TID 287) in 29 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 406.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 406 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.033 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 203 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 406: Stage finished\n",
      "[broadcast-exchange-24] INFO org.apache.spark.scheduler.DAGScheduler - Job 203 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.033121 s\n",
      "[broadcast-exchange-24] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_244 stored as values in memory (estimated size 1025.5 KiB, free 424.4 MiB)\n",
      "[broadcast-exchange-24] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_244_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 424.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_244_piece0 in memory on 192.168.1.24:39111 (size: 2.3 KiB, free: 434.1 MiB)\n",
      "[broadcast-exchange-24] INFO org.apache.spark.SparkContext - Created broadcast 244 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1491 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 90\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 204 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 410 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 409)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 410 (MapPartitionsRDD[1491] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_245 stored as values in memory (estimated size 24.0 KiB, free 424.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_245_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 424.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_245_piece0 in memory on 192.168.1.24:39111 (size: 12.0 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 245 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 410 (MapPartitionsRDD[1491] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 410.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-11] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 410.0 (TID 288) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7422 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 410.0 (TID 288)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 410.0 (TID 288)\n",
      "[Executor task launch worker for task 0.0 in stage 410.0 (TID 288)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 410.0 (TID 288)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 410.0 (TID 288)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 82, boot = -169, init = 250, finish = 1\n",
      "[Executor task launch worker for task 0.0 in stage 410.0 (TID 288)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 410.0 (TID 288). 3162 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 410.0 (TID 288) in 88 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 410.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 410 (count at NativeMethodAccessorImpl.java:0) finished in 0.090 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 205 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 415 (count at NativeMethodAccessorImpl.java:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 414)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 415 (MapPartitionsRDD[1494] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_246 stored as values in memory (estimated size 12.5 KiB, free 424.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_246_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 424.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_246_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 246 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 415 (MapPartitionsRDD[1494] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 415.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-9] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 415.0 (TID 289) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 415.0 (TID 289)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 415.0 (TID 289)\n",
      "[Executor task launch worker for task 0.0 in stage 415.0 (TID 289)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 415.0 (TID 289)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 415.0 (TID 289)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 415.0 (TID 289). 3909 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 415.0 (TID 289) in 2 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 415.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 415 (count at NativeMethodAccessorImpl.java:0) finished in 0.005 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 205 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 415: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 205 finished: count at NativeMethodAccessorImpl.java:0, took 0.005857 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k_range = [3, 5, 7, 9, 11]\n",
    "\n",
    "# Find the best k\n",
    "best_k = find_best_k(train, test, k_range)\n",
    "print(\"Best k values:\", best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd0549",
   "metadata": {},
   "source": [
    "## KNN Classifier: \n",
    "\n",
    "Học sinh được chúng em xếp vào nhóm tốt bằng cách xét điểm thi qua 3 kì học (grade_1, grade_2, final_grade). Nếu cả 3 kì đều có điểm thi là khá trở lên, hoặc học sinh có tiến bộ/không bị rớt điểm trong quá trình thi và điểm thi cuối năm là điểm khá trở lên. Thì sẽ được xếp vào nhóm học tốt (is_good)\n",
    "     \n",
    "    (grade_1 + grade_2 + final_grade)/3 >= 14\n",
    "    hoặc\n",
    "    (final_grade >= grade_2 và grade_2 >= grade_1 và final_grade >= 14)\n",
    "    -->  is_good = 1\n",
    "\n",
    "\n",
    "#### Tham khảo:\n",
    "    3 học kì trong năm: https://en.wikipedia.org/wiki/Education_in_Portugal#Years_of_schooling\n",
    "    Phân loại học sinh: https://www.scholaro.com/db/Countries/Portugal/Grading-System\n",
    "   \n",
    "     \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41a5a0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.780039 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1503 (count at <unknown>:0) as input to shuffle 91\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 206 (count at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 416 (count at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 416 (MapPartitionsRDD[1503] at count at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_247 stored as values in memory (estimated size 39.3 KiB, free 424.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_247_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 424.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_247_piece0 in memory on 192.168.1.24:39111 (size: 16.3 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 247 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 416 (MapPartitionsRDD[1503] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 416.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 416.0 (TID 290) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 416.0 (TID 291) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 416.0 (TID 291)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 416.0 (TID 291)\n",
      "[Executor task launch worker for task 0.0 in stage 416.0 (TID 290)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 416.0 (TID 290)\n",
      "[Executor task launch worker for task 0.0 in stage 416.0 (TID 290)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 416.0 (TID 291)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 416.0 (TID 291)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.640271 ms\n",
      "[Executor task launch worker for task 0.0 in stage 416.0 (TID 290)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.387529 ms\n",
      "[Executor task launch worker for task 0.0 in stage 416.0 (TID 290)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 416.0 (TID 290). 2354 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 416.0 (TID 291)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 416.0 (TID 291). 2354 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 416.0 (TID 290) in 11 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 416.0 (TID 291) in 11 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 416.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 416 (count at <unknown>:0) finished in 0.014 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.817231 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 207 (count at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 418 (count at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 417)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 418 (MapPartitionsRDD[1506] at count at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_248 stored as values in memory (estimated size 12.5 KiB, free 424.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_248_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 424.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_248_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 248 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 418 (MapPartitionsRDD[1506] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 418.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 418.0 (TID 292) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 418.0 (TID 292)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 418.0 (TID 292)\n",
      "[Executor task launch worker for task 0.0 in stage 418.0 (TID 292)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 418.0 (TID 292)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 418.0 (TID 292)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.716508 ms\n",
      "[Executor task launch worker for task 0.0 in stage 418.0 (TID 292)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 418.0 (TID 292). 3952 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 418.0 (TID 292) in 6 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 418.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 418 (count at <unknown>:0) finished in 0.008 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 207 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 418: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 207 finished: count at <unknown>:0, took 0.008896 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số dòng: 1044\n",
      "Số cột: 40\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "|age|class_failures|school_support|family_support|extra_paid_classes|activities|higher_ed|internet_access|romantic_relationship|family_relationship|free_time|social|weekday_alcohol|health|absences|grade_1|grade_2|final_grade|subject_code| id|mother_education_is_5th_to_9th_grade|mother_education_is_none|mother_education_is_secondary_education|mother_education_is_primary_education_(4th_grade)|mother_education_is_higher_education|father_education_is_5th_to_9th_grade|father_education_is_none|father_education_is_secondary_education|father_education_is_primary_education_(4th_grade)|father_education_is_higher_education|reputation_school_choice|course_school_choice|other_school_choice|home_school_choice|father_guardian|mother_guardian|other_guardian|study_time_encoded|sex_is_M|address_type_is_Urban|\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "| 18|             0|             1|             0|                 0|         0|        1|              0|                    0|                  4|        3|     4|              1|     3|       6|      5|      6|          6|           1|  1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   1|                  0|                 0|              0|              1|             1|                 1|       0|                    1|\n",
      "| 17|             0|             0|             1|                 0|         0|        1|              1|                    0|                  5|        3|     3|              1|     3|       4|      5|      5|          6|           1|  2|                                   0|                       0|                                      0|                                                1|                                   0|                                   0|                       0|                                      0|                                                1|                                   0|                       0|                   1|                  0|                 0|              1|              0|             0|                 1|       0|                    1|\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+------+--------+-------+-------+-----------+------------+---+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- class_failures: integer (nullable = true)\n",
      " |-- school_support: integer (nullable = false)\n",
      " |-- family_support: integer (nullable = false)\n",
      " |-- extra_paid_classes: integer (nullable = false)\n",
      " |-- activities: integer (nullable = false)\n",
      " |-- higher_ed: integer (nullable = false)\n",
      " |-- internet_access: integer (nullable = false)\n",
      " |-- romantic_relationship: integer (nullable = false)\n",
      " |-- family_relationship: integer (nullable = true)\n",
      " |-- free_time: integer (nullable = true)\n",
      " |-- social: integer (nullable = true)\n",
      " |-- weekday_alcohol: integer (nullable = true)\n",
      " |-- health: integer (nullable = true)\n",
      " |-- absences: integer (nullable = true)\n",
      " |-- grade_1: integer (nullable = true)\n",
      " |-- grade_2: integer (nullable = true)\n",
      " |-- final_grade: integer (nullable = true)\n",
      " |-- subject_code: integer (nullable = false)\n",
      " |-- id: integer (nullable = false)\n",
      " |-- mother_education_is_5th_to_9th_grade: integer (nullable = true)\n",
      " |-- mother_education_is_none: integer (nullable = true)\n",
      " |-- mother_education_is_secondary_education: integer (nullable = true)\n",
      " |-- mother_education_is_primary_education_(4th_grade): integer (nullable = true)\n",
      " |-- mother_education_is_higher_education: integer (nullable = true)\n",
      " |-- father_education_is_5th_to_9th_grade: integer (nullable = true)\n",
      " |-- father_education_is_none: integer (nullable = true)\n",
      " |-- father_education_is_secondary_education: integer (nullable = true)\n",
      " |-- father_education_is_primary_education_(4th_grade): integer (nullable = true)\n",
      " |-- father_education_is_higher_education: integer (nullable = true)\n",
      " |-- reputation_school_choice: integer (nullable = true)\n",
      " |-- course_school_choice: integer (nullable = true)\n",
      " |-- other_school_choice: integer (nullable = true)\n",
      " |-- home_school_choice: integer (nullable = true)\n",
      " |-- father_guardian: integer (nullable = true)\n",
      " |-- mother_guardian: integer (nullable = true)\n",
      " |-- other_guardian: integer (nullable = true)\n",
      " |-- study_time_encoded: integer (nullable = false)\n",
      " |-- sex_is_M: integer (nullable = false)\n",
      " |-- address_type_is_Urban: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Thread-3] WARN org.apache.spark.sql.execution.window.WindowExec - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_247_piece0 on 192.168.1.24:39111 in memory (size: 16.3 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_241_piece0 on 192.168.1.24:39111 in memory (size: 12.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_248_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_242_piece0 on 192.168.1.24:39111 in memory (size: 22.3 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_244_piece0 on 192.168.1.24:39111 in memory (size: 2.3 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_245_piece0 on 192.168.1.24:39111 in memory (size: 12.0 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_246_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_243_piece0 on 192.168.1.24:39111 in memory (size: 37.4 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.7929 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.048487 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1518 (showString at <unknown>:0) as input to shuffle 92\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 208 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 420 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 419)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 419)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 419 (MapPartitionsRDD[1518] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_249 stored as values in memory (estimated size 78.8 KiB, free 425.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_249_piece0 stored as bytes in memory (estimated size 28.1 KiB, free 425.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_249_piece0 in memory on 192.168.1.24:39111 (size: 28.1 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 249 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 419 (MapPartitionsRDD[1518] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 419.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 419.0 (TID 293) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 419.0 (TID 294) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 419.0 (TID 293)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 419.0 (TID 293)\n",
      "[Executor task launch worker for task 1.0 in stage 419.0 (TID 294)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 419.0 (TID 294)\n",
      "[Executor task launch worker for task 0.0 in stage 419.0 (TID 293)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 419.0 (TID 294)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 419.0 (TID 293)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 419.0 (TID 293). 3654 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 419.0 (TID 293) in 9 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 419.0 (TID 294)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 419.0 (TID 294). 3654 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 419.0 (TID 294) in 8 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 419.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 419 (showString at <unknown>:0) finished in 0.011 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 420)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 420 (MapPartitionsRDD[1523] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_250 stored as values in memory (estimated size 84.1 KiB, free 425.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_250_piece0 stored as bytes in memory (estimated size 30.8 KiB, free 425.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_250_piece0 in memory on 192.168.1.24:39111 (size: 30.8 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 250 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 420 (MapPartitionsRDD[1523] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 420.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 420.0 (TID 295) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 420.0 (TID 295)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 420.0 (TID 295)\n",
      "[Executor task launch worker for task 0.0 in stage 420.0 (TID 295)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (1186.0 B) non-empty blocks including 2 (1186.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 420.0 (TID 295)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 420.0 (TID 295)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.436016 ms\n",
      "[Executor task launch worker for task 0.0 in stage 420.0 (TID 295)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 420.0 (TID 295). 4955 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 420.0 (TID 295) in 21 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 420.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 420 (showString at <unknown>:0) finished in 0.023 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 208 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 420: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 208 finished: showString at <unknown>:0, took 0.036876 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.698529 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Số dòng: {df.count()}\")\n",
    "print(f\"Số cột: {len(df.columns)}\")\n",
    "\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b4ce4d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.126245 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------+\n",
      "|grade_1|grade_2|final_grade|is_good|\n",
      "+-------+-------+-----------+-------+\n",
      "|      5|      6|          6|      0|\n",
      "|      5|      5|          6|      0|\n",
      "|      7|      8|         10|      0|\n",
      "|     15|     14|         15|      1|\n",
      "|      6|     10|         10|      0|\n",
      "|     15|     15|         15|      1|\n",
      "|     12|     12|         11|      0|\n",
      "|      6|      5|          6|      0|\n",
      "|     16|     18|         19|      1|\n",
      "|     14|     15|         15|      1|\n",
      "|     10|      8|          9|      0|\n",
      "|     10|     12|         12|      0|\n",
      "|     14|     14|         14|      1|\n",
      "|     10|     10|         11|      0|\n",
      "|     14|     16|         16|      1|\n",
      "|     14|     14|         14|      1|\n",
      "|     13|     14|         14|      1|\n",
      "|      8|     10|         10|      0|\n",
      "|      6|      5|          5|      0|\n",
      "|      8|     10|         10|      0|\n",
      "+-------+-------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.236448 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 209 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 421 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 421 (MapPartitionsRDD[1533] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_251 stored as values in memory (estimated size 47.9 KiB, free 425.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_251_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 425.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_251_piece0 in memory on 192.168.1.24:39111 (size: 19.2 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 251 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 421 (MapPartitionsRDD[1533] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 421.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 421.0 (TID 296) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 421.0 (TID 296)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 421.0 (TID 296)\n",
      "[Executor task launch worker for task 0.0 in stage 421.0 (TID 296)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 421.0 (TID 296)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.993748 ms\n",
      "[Executor task launch worker for task 0.0 in stage 421.0 (TID 296)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.681506 ms\n",
      "[Executor task launch worker for task 0.0 in stage 421.0 (TID 296)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 421.0 (TID 296)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 421.0 (TID 296)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 421.0 (TID 296). 2460 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 421.0 (TID 296) in 13 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 421.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 421 (showString at <unknown>:0) finished in 0.017 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 209 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 421: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 209 finished: showString at <unknown>:0, took 0.017451 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.387808 ms\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa luật phân loại\n",
    "\n",
    "condition1 = (col(\"grade_1\") + col(\"grade_2\") + col(\"final_grade\")) / 3 >= 14\n",
    "condition2 = (col(\"final_grade\") >= col(\"grade_2\")) & (col(\"grade_2\") >= col(\"grade_1\")) & (col(\"final_grade\") >= 14)\n",
    "\n",
    "\n",
    "df = df.withColumn(\"is_good\", when(condition1 | condition2, 1).otherwise(0))\n",
    "\n",
    "\n",
    "df.select(\"grade_1\", \"grade_2\", \"final_grade\", \"is_good\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "371922d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1542 (count at <unknown>:0) as input to shuffle 93\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 210 (count at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 422 (count at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 422 (MapPartitionsRDD[1542] at count at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_252 stored as values in memory (estimated size 39.3 KiB, free 425.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_252_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 425.4 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_252_piece0 in memory on 192.168.1.24:39111 (size: 16.3 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 252 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 422 (MapPartitionsRDD[1542] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 422.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 422.0 (TID 297) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-10] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 422.0 (TID 298) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 422.0 (TID 297)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 422.0 (TID 297)\n",
      "[Executor task launch worker for task 1.0 in stage 422.0 (TID 298)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 422.0 (TID 298)\n",
      "[Executor task launch worker for task 1.0 in stage 422.0 (TID 298)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 422.0 (TID 297)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 422.0 (TID 297)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 422.0 (TID 297). 2354 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 422.0 (TID 298)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 422.0 (TID 298). 2354 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 422.0 (TID 297) in 5 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 422.0 (TID 298) in 5 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 422.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 422 (count at <unknown>:0) finished in 0.008 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 211 (count at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 424 (count at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 423)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 424 (MapPartitionsRDD[1545] at count at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_253 stored as values in memory (estimated size 12.5 KiB, free 425.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_253_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 425.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_253_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 253 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 424 (MapPartitionsRDD[1545] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 424.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 424.0 (TID 299) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 424.0 (TID 299)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 424.0 (TID 299)\n",
      "[Executor task launch worker for task 0.0 in stage 424.0 (TID 299)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 424.0 (TID 299)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 424.0 (TID 299)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 424.0 (TID 299). 3952 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 424.0 (TID 299) in 3 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 424.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 424 (count at <unknown>:0) finished in 0.005 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 211 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 424: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 211 finished: count at <unknown>:0, took 0.005816 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.67012 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.21924 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.353428 ms\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 1556 (count at <unknown>:0) as input to shuffle 94\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got map stage job 212 (count at <unknown>:0) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 425 (count at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 425 (MapPartitionsRDD[1556] at count at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_254 stored as values in memory (estimated size 60.4 KiB, free 425.3 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_254_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 425.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_254_piece0 in memory on 192.168.1.24:39111 (size: 23.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 254 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 425 (MapPartitionsRDD[1556] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 425.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 425.0 (TID 300) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8359 bytes) \n",
      "[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 425.0 (TID 301) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8365 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 425.0 (TID 300)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 425.0 (TID 300)\n",
      "[Executor task launch worker for task 1.0 in stage 425.0 (TID 301)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 425.0 (TID 301)\n",
      "[Executor task launch worker for task 1.0 in stage 425.0 (TID 301)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 425.0 (TID 300)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 425.0 (TID 300)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.623155 ms\n",
      "[Executor task launch worker for task 1.0 in stage 425.0 (TID 301)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.618733 ms\n",
      "[Executor task launch worker for task 1.0 in stage 425.0 (TID 301)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.389995 ms\n",
      "[Executor task launch worker for task 0.0 in stage 425.0 (TID 300)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 425.0 (TID 300). 2578 bytes result sent to driver\n",
      "[Executor task launch worker for task 1.0 in stage 425.0 (TID 301)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 425.0 (TID 301). 2578 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 425.0 (TID 300) in 18 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 425.0 (TID 301) in 17 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 425.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 425 (count at <unknown>:0) finished in 0.030 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.694932 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: count at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 213 (count at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 427 (count at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 426)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 427 (MapPartitionsRDD[1559] at count at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_255 stored as values in memory (estimated size 12.5 KiB, free 425.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_255_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 425.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_255_piece0 in memory on 192.168.1.24:39111 (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 255 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 427 (MapPartitionsRDD[1559] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 427.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-8] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 427.0 (TID 302) (192.168.1.24, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 427.0 (TID 302)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 427.0 (TID 302)\n",
      "[Executor task launch worker for task 0.0 in stage 427.0 (TID 302)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "[Executor task launch worker for task 0.0 in stage 427.0 (TID 302)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "[Executor task launch worker for task 0.0 in stage 427.0 (TID 302)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.920152 ms\n",
      "[Executor task launch worker for task 0.0 in stage 427.0 (TID 302)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 427.0 (TID 302). 3952 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 427.0 (TID 302) in 7 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 427.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 427 (count at <unknown>:0) finished in 0.008 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 213 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 427: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 213 finished: count at <unknown>:0, took 0.009429 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of rows where is_good = 1: 0.27011494252873564 \n",
      "+-------+-------+-----------+-------+\n",
      "|grade_1|grade_2|final_grade|is_good|\n",
      "+-------+-------+-----------+-------+\n",
      "|     15|     14|         15|      1|\n",
      "|     15|     15|         15|      1|\n",
      "|     16|     18|         19|      1|\n",
      "|     14|     15|         15|      1|\n",
      "|     14|     14|         14|      1|\n",
      "|     14|     16|         16|      1|\n",
      "|     14|     14|         14|      1|\n",
      "|     13|     14|         14|      1|\n",
      "|     13|     14|         15|      1|\n",
      "|     12|     15|         15|      1|\n",
      "|     15|     15|         16|      1|\n",
      "|     15|     16|         15|      1|\n",
      "|     17|     16|         17|      1|\n",
      "|     17|     16|         16|      1|\n",
      "|     12|     14|         15|      1|\n",
      "|     15|     16|         18|      1|\n",
      "|     15|     16|         15|      1|\n",
      "|     19|     18|         18|      1|\n",
      "|     19|     19|         20|      1|\n",
      "|     15|     15|         14|      1|\n",
      "|     14|     15|         15|      1|\n",
      "|     14|     15|         15|      1|\n",
      "|     15|     16|         16|      1|\n",
      "|     16|     15|         15|      1|\n",
      "|     16|     16|         16|      1|\n",
      "|     13|     15|         15|      1|\n",
      "|     12|     12|         14|      1|\n",
      "|     15|     15|         15|      1|\n",
      "|     13|     14|         14|      1|\n",
      "|     16|     17|         18|      1|\n",
      "+-------+-------+-----------+-------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_253_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_250_piece0 on 192.168.1.24:39111 in memory (size: 30.8 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_251_piece0 on 192.168.1.24:39111 in memory (size: 19.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_254_piece0 on 192.168.1.24:39111 in memory (size: 23.9 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_255_piece0 on 192.168.1.24:39111 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_249_piece0 on 192.168.1.24:39111 in memory (size: 28.1 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_252_piece0 on 192.168.1.24:39111 in memory (size: 16.3 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.604057 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.507082 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 214 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 428 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 428 (MapPartitionsRDD[1569] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_256 stored as values in memory (estimated size 65.2 KiB, free 425.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_256_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 425.6 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_256_piece0 in memory on 192.168.1.24:39111 (size: 23.8 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 256 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 428 (MapPartitionsRDD[1569] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 428.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 428.0 (TID 303) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 428.0 (TID 303)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 428.0 (TID 303)\n",
      "[Executor task launch worker for task 0.0 in stage 428.0 (TID 303)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 428.0 (TID 303)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.764803 ms\n",
      "[Executor task launch worker for task 0.0 in stage 428.0 (TID 303)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 428.0 (TID 303)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 428.0 (TID 303)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 428.0 (TID 303). 2590 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 428.0 (TID 303) in 12 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 428.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 428 (showString at <unknown>:0) finished in 0.019 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 214 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 428: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 214 finished: showString at <unknown>:0, took 0.020072 s\n"
     ]
    }
   ],
   "source": [
    "# lọc lại các dòng có is_good = 1\n",
    "df_is_good = df.filter(df.is_good == 1)\n",
    "\n",
    "# Đếm tổng số dòng và các dòng có is_good = 1\n",
    "total_count = df.count()\n",
    "is_good_count = df_is_good.count()\n",
    "\n",
    "# Tính tỉ lệ (ratio)\n",
    "ratio = is_good_count / total_count\n",
    "\n",
    "print(f\"Ratio of rows where is_good = 1: {ratio} \")\n",
    "\n",
    "# Các dòng có is_good = 1\n",
    "df_is_good.select(\"grade_1\", \"grade_2\", \"final_grade\", \"is_good\").show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a1e9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classifier = df.drop(\"grade_1\", \"grade_2\", \"final_grade\", \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ad993d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.288597 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.170958 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 215 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 429 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 429 (MapPartitionsRDD[1579] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_257 stored as values in memory (estimated size 97.8 KiB, free 425.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_257_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 425.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_257_piece0 in memory on 192.168.1.24:39111 (size: 30.4 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 257 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 429 (MapPartitionsRDD[1579] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 429.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 429.0 (TID 304) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 429.0 (TID 304)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 429.0 (TID 304)\n",
      "[Executor task launch worker for task 0.0 in stage 429.0 (TID 304)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 429.0 (TID 304)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.958844 ms\n",
      "[Executor task launch worker for task 0.0 in stage 429.0 (TID 304)] INFO org.apache.spark.executor.Executor - 1 block locks were not released by task 0.0 in stage 429.0 (TID 304)\n",
      "[rdd_35_0]\n",
      "[Executor task launch worker for task 0.0 in stage 429.0 (TID 304)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 429.0 (TID 304). 3640 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 429.0 (TID 304) in 21 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 429.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 429 (showString at <unknown>:0) finished in 0.023 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 215 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 429: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 215 finished: showString at <unknown>:0, took 0.024228 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+------+--------+------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+-------+\n",
      "|age|class_failures|school_support|family_support|extra_paid_classes|activities|higher_ed|internet_access|romantic_relationship|family_relationship|free_time|social|weekday_alcohol|health|absences|subject_code|mother_education_is_5th_to_9th_grade|mother_education_is_none|mother_education_is_secondary_education|mother_education_is_primary_education_(4th_grade)|mother_education_is_higher_education|father_education_is_5th_to_9th_grade|father_education_is_none|father_education_is_secondary_education|father_education_is_primary_education_(4th_grade)|father_education_is_higher_education|reputation_school_choice|course_school_choice|other_school_choice|home_school_choice|father_guardian|mother_guardian|other_guardian|study_time_encoded|sex_is_M|address_type_is_Urban|is_good|\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+------+--------+------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+-------+\n",
      "| 18|             0|             1|             0|                 0|         0|        1|              0|                    0|                  4|        3|     4|              1|     3|       6|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   1|                  0|                 0|              0|              1|             1|                 1|       0|                    1|      0|\n",
      "| 17|             0|             0|             1|                 0|         0|        1|              1|                    0|                  5|        3|     3|              1|     3|       4|           1|                                   0|                       0|                                      0|                                                1|                                   0|                                   0|                       0|                                      0|                                                1|                                   0|                       0|                   1|                  0|                 0|              1|              0|             0|                 1|       0|                    1|      0|\n",
      "| 15|             3|             1|             0|                 1|         0|        1|              1|                    0|                  4|        3|     2|              2|     3|      10|           1|                                   0|                       0|                                      0|                                                1|                                   0|                                   0|                       0|                                      0|                                                1|                                   0|                       0|                   0|                  1|                 0|              0|              1|             1|                 1|       0|                    1|      0|\n",
      "| 15|             0|             0|             1|                 1|         1|        1|              1|                    1|                  3|        2|     2|              1|     5|       2|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   0|                  0|                 1|              0|              1|             1|                 2|       0|                    1|      1|\n",
      "| 16|             0|             0|             1|                 1|         0|        1|              0|                    0|                  4|        3|     2|              1|     5|       4|           1|                                   0|                       0|                                      1|                                                0|                                   0|                                   0|                       0|                                      1|                                                0|                                   0|                       0|                   0|                  0|                 1|              1|              0|             0|                 1|       0|                    1|      0|\n",
      "| 16|             0|             0|             1|                 1|         1|        1|              1|                    0|                  5|        4|     2|              1|     5|      10|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      1|                                                0|                                   0|                       1|                   0|                  0|                 0|              0|              1|             1|                 1|       1|                    1|      1|\n",
      "| 16|             0|             0|             0|                 0|         0|        1|              1|                    0|                  4|        4|     4|              1|     3|       0|           1|                                   1|                       0|                                      0|                                                0|                                   0|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   0|                  0|                 1|              0|              1|             1|                 1|       1|                    1|      0|\n",
      "| 17|             0|             1|             1|                 0|         0|        1|              0|                    0|                  4|        1|     4|              1|     1|       6|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   0|                  0|                 1|              0|              1|             1|                 1|       0|                    1|      0|\n",
      "| 15|             0|             0|             1|                 1|         0|        1|              1|                    0|                  4|        2|     2|              1|     1|       0|           1|                                   0|                       0|                                      1|                                                0|                                   0|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   0|                  0|                 1|              0|              1|             1|                 1|       1|                    1|      1|\n",
      "| 15|             0|             0|             1|                 1|         1|        1|              1|                    0|                  5|        5|     1|              1|     5|       0|           1|                                   0|                       0|                                      1|                                                0|                                   0|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   0|                  0|                 1|              0|              1|             1|                 1|       1|                    1|      1|\n",
      "| 15|             0|             0|             1|                 1|         0|        1|              1|                    0|                  3|        3|     3|              1|     2|       0|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       1|                   0|                  0|                 0|              0|              1|             1|                 1|       0|                    1|      0|\n",
      "| 15|             0|             0|             1|                 0|         1|        1|              1|                    0|                  5|        2|     2|              1|     4|       4|           1|                                   1|                       0|                                      0|                                                0|                                   0|                                   0|                       0|                                      0|                                                1|                                   0|                       1|                   0|                  0|                 0|              1|              0|             0|                 2|       0|                    1|      0|\n",
      "| 15|             0|             0|             1|                 1|         1|        1|              1|                    0|                  4|        3|     3|              1|     5|       2|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   1|                  0|                 0|              1|              0|             0|                 0|       1|                    1|      1|\n",
      "| 15|             0|             0|             1|                 1|         0|        1|              1|                    0|                  5|        4|     3|              1|     3|       2|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      1|                                                0|                                   0|                       0|                   1|                  0|                 0|              0|              1|             1|                 1|       1|                    1|      0|\n",
      "| 15|             0|             0|             1|                 0|         0|        1|              1|                    1|                  4|        5|     2|              1|     3|       0|           1|                                   1|                       0|                                      0|                                                0|                                   0|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   0|                  0|                 1|              0|              0|             1|                 2|       1|                    1|      1|\n",
      "| 16|             0|             0|             1|                 0|         0|        1|              1|                    0|                  4|        4|     4|              1|     2|       4|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       0|                   0|                  0|                 1|              0|              1|             1|                 0|       0|                    1|      1|\n",
      "| 16|             0|             0|             1|                 1|         1|        1|              1|                    0|                  3|        2|     3|              1|     2|       6|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      0|                                                0|                                   1|                       1|                   0|                  0|                 0|              0|              1|             1|                 2|       0|                    1|      1|\n",
      "| 16|             0|             1|             1|                 0|         1|        1|              0|                    0|                  5|        3|     2|              1|     4|       4|           1|                                   0|                       0|                                      1|                                                0|                                   0|                                   0|                       0|                                      1|                                                0|                                   0|                       1|                   0|                  0|                 0|              0|              1|             1|                 1|       0|                    1|      0|\n",
      "| 17|             3|             0|             1|                 0|         1|        1|              1|                    0|                  5|        5|     5|              2|     5|      16|           1|                                   0|                       0|                                      1|                                                0|                                   0|                                   1|                       0|                                      0|                                                0|                                   0|                       0|                   1|                  0|                 0|              0|              1|             1|                 0|       1|                    1|      0|\n",
      "| 16|             0|             0|             0|                 1|         1|        1|              1|                    0|                  3|        1|     3|              1|     5|       4|           1|                                   0|                       0|                                      0|                                                0|                                   1|                                   0|                       0|                                      1|                                                0|                                   0|                       0|                   0|                  0|                 1|              1|              0|             0|                 0|       1|                    1|      0|\n",
      "+---+--------------+--------------+--------------+------------------+----------+---------+---------------+---------------------+-------------------+---------+------+---------------+------+--------+------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------------------+------------------------+---------------------------------------+-------------------------------------------------+------------------------------------+------------------------+--------------------+-------------------+------------------+---------------+---------------+--------------+------------------+--------+---------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.726582 ms\n"
     ]
    }
   ],
   "source": [
    "df_classifier.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1230f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia tập dữ liệu với tỉ lệ train:test là 7:3\n",
    "train_data, test_data = df_classifier.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c26d4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_attribute = 'is_good'\n",
    "feature_columns = [column for column in df_classifier.columns if column != decision_attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "648a1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tính khoảng cách\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0\n",
    "    for column in feature_columns:\n",
    "        distance += (row1[column] - row2[column]) ** 2\n",
    "    return distance**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488a37b",
   "metadata": {},
   "source": [
    "Sử dụng hàm broadcast để broadcast dữ liệu huấn luyện (train_data.rdd.collect()) từ node chủ đến tất cả các node làm việc. Dữ liệu huấn luyện được thu thập thành một danh sách (list) để broadcast.\n",
    "\n",
    "Chuyển đổi DataFrame test_data thành một Resilient Distributed Dataset (RDD) để có thể thực hiện các phép biến đổi và hành động xử lý song song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6be78844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.360384 ms\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.583891 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_257_piece0 on 192.168.1.24:39111 in memory (size: 30.4 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_256_piece0 on 192.168.1.24:39111 in memory (size: 23.8 KiB, free: 434.2 MiB)\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.146629 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: collect at /tmp/ipykernel_14509/1984109000.py:2\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 216 (collect at /tmp/ipykernel_14509/1984109000.py:2) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 430 (collect at /tmp/ipykernel_14509/1984109000.py:2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 430 (MapPartitionsRDD[1592] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_258 stored as values in memory (estimated size 133.6 KiB, free 425.6 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_258_piece0 stored as bytes in memory (estimated size 43.2 KiB, free 425.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_258_piece0 in memory on 192.168.1.24:39111 (size: 43.2 KiB, free: 434.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 258 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 430 (MapPartitionsRDD[1592] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 430.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 430.0 (TID 305) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 430.0 (TID 306) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8376 bytes) \n",
      "[Executor task launch worker for task 1.0 in stage 430.0 (TID 306)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 430.0 (TID 306)\n",
      "[Executor task launch worker for task 0.0 in stage 430.0 (TID 305)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 430.0 (TID 305)\n",
      "[Executor task launch worker for task 1.0 in stage 430.0 (TID 306)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 430.0 (TID 305)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 430.0 (TID 306)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.846705 ms\n",
      "[Executor task launch worker for task 0.0 in stage 430.0 (TID 305)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.873949 ms\n",
      "[Executor task launch worker for task 1.0 in stage 430.0 (TID 306)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.364928 ms\n",
      "[Executor task launch worker for task 1.0 in stage 430.0 (TID 306)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.586396 ms\n",
      "[Executor task launch worker for task 1.0 in stage 430.0 (TID 306)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 1.916031 ms\n",
      "[Executor task launch worker for task 0.0 in stage 430.0 (TID 305)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 430.0 (TID 305). 55470 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 430.0 (TID 305) in 66 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 430.0 (TID 306)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 430.0 (TID 306). 67770 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 430.0 (TID 306) in 71 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 430.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 430 (collect at /tmp/ipykernel_14509/1984109000.py:2) finished in 0.074 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 216 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 430: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 216 finished: collect at /tmp/ipykernel_14509/1984109000.py:2, took 0.075431 s\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_259 stored as values in memory (estimated size 208.0 B, free 425.5 MiB)\n",
      "[Thread-3] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_259_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 425.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_259_piece0 in memory on 192.168.1.24:39111 (size: 27.3 KiB, free: 434.1 MiB)\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Created broadcast 259 from broadcast at NativeMethodAccessorImpl.java:0\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.824449 ms\n"
     ]
    }
   ],
   "source": [
    "# Broadcast train_data to all worker nodes\n",
    "broadcast_train_data = sc.broadcast(train_data.rdd.collect())\n",
    "\n",
    "# Convert the test_data DataFrame to an RDD for parallel processing\n",
    "test_data_rdd = test_data.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ceee8",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1 - Tính khoảng cách Euclidean và Tìm k Láng giềng Gần Nhất\n",
    "1. Tính khoảng cách Euclidean:\n",
    "+ Duyệt qua mỗi hàng trong tập kiểm thử (test_data_rdd).\n",
    "+ Với mỗi hàng kiểm thử, tính khoảng cách Euclidean đến mỗi hàng trong tập huấn luyện (train_data_broadcast.value).\n",
    "\n",
    "2. Lựa chọn k láng giềng gần nhất:\n",
    "\n",
    "+ Đối với mỗi hàng kiểm thử, tạo một danh sách các khoảng cách Euclidean đến các hàng trong tập huấn luyện cùng với nhãn \"is_good\" tương ứng.\n",
    "+ Sắp xếp danh sách này theo thứ tự tăng dần của khoảng cách Euclidean.\n",
    "+ Giữ lại nhãn \"is_good\" của mỗi hàng trong tập huấn luyện tương ứng.\n",
    "+ Lựa chọn k giá trị nhỏ nhất từ danh sách đã sắp xếp.\n",
    "\n",
    "3. Lưu trữ kết quả cho mỗi hàng kiểm thử:\n",
    "\n",
    "+ Lưu trữ nhãn \"is_good\" của k láng giềng gần nhất.\n",
    "+ Tạo tuples gồm nhãn \"is_good\" và khoảng cách Euclidean của k láng giềng gần nhất cho mỗi hàng kiểm thử.\n",
    "\n",
    "4. Kiểm thử với các giá trị k khác nhau:\n",
    "\n",
    "+ Thực hiện các bước 1-3 với các giá trị k khác nhau để kiểm thử sự ảnh hưởng của giá trị k đối với dự đoán của mô hình (tinh chỉnh siêu tham số - hyperparameter tuning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eac8c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Euclidean distance between two rows\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0\n",
    "    for column in feature_columns:\n",
    "        distance += (row1[column] - row2[column]) ** 2\n",
    "    return distance**0.5\n",
    "\n",
    "k = 10 \n",
    "\n",
    "# Function to find k neighbors for each test row\n",
    "def find_neighbors(iter, train_data_broadcast):\n",
    "    distances = []\n",
    "    for test_row in iter:\n",
    "        for train_row in train_data_broadcast.value:\n",
    "            euc_dist = euclidean_distance(test_row, train_row)\n",
    "            distances.append((euc_dist, train_row['is_good']))\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        yield distances[:k]\n",
    "\n",
    "# Use mapPartitions transformation to find neighbors for each test row in parallel\n",
    "test_neighbors_rdd = test_data_rdd.mapPartitions(lambda iter: find_neighbors(iter, broadcast_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b6a31",
   "metadata": {},
   "source": [
    "### Step 2 - Dự đoán\n",
    "\n",
    "Sử dụng mapPartitions để áp dụng hàm find_neighbors và tìm ra k láng giềng gần nhất cho mỗi hàng trong tập kiểm thử (test_data_rdd) dựa trên tập huấn luyện (broadcast_train_data). Kết quả được lưu trữ trong test_neighbors_rdd.\n",
    "\n",
    "Sau đó, sử dụng hàm predict_class để dự đoán lớp cho mỗi hàng trong tập kiểm thử dựa trên nhãn của k láng giềng gần nhất.\n",
    "\n",
    "Kết quả dự đoán được thu thập từ test_predictions_rdd bằng cách sử dụng collect() để có danh sách dự đoán cuối cùng cho tất cả các hàng trong tập kiểm thử, được lưu trữ trong biến test_predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3550e558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: collect at /tmp/ipykernel_14509/2497880046.py:14\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 217 (collect at /tmp/ipykernel_14509/2497880046.py:14) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 431 (collect at /tmp/ipykernel_14509/2497880046.py:14)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 431 (PythonRDD[1606] at collect at /tmp/ipykernel_14509/2497880046.py:14), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_260 stored as values in memory (estimated size 138.6 KiB, free 425.4 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_260_piece0 stored as bytes in memory (estimated size 46.6 KiB, free 425.3 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_260_piece0 in memory on 192.168.1.24:39111 (size: 46.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 260 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 431 (PythonRDD[1606] at collect at /tmp/ipykernel_14509/2497880046.py:14) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 431.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 431.0 (TID 307) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 431.0 (TID 308) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8376 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 431.0 (TID 307)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 431.0 (TID 307)\n",
      "[Executor task launch worker for task 1.0 in stage 431.0 (TID 308)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 431.0 (TID 308)\n",
      "[Executor task launch worker for task 1.0 in stage 431.0 (TID 308)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 431.0 (TID 307)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 0.0 in stage 431.0 (TID 307)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.811781 ms\n",
      "[Executor task launch worker for task 0.0 in stage 431.0 (TID 307)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 4383, boot = -1184, init = 1271, finish = 4296\n",
      "[Executor task launch worker for task 0.0 in stage 431.0 (TID 307)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 431.0 (TID 307). 2931 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 431.0 (TID 307) in 4403 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 431.0 (TID 308)] INFO org.apache.spark.api.python.PythonRunner - Times: total = 10245, boot = -1445, init = 1533, finish = 10157\n",
      "[Executor task launch worker for task 1.0 in stage 431.0 (TID 308)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 431.0 (TID 308). 3177 bytes result sent to driver\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 431.0 (TID 308) in 10265 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 431.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 431 (collect at /tmp/ipykernel_14509/2497880046.py:14) finished in 10.269 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 217 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 431: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 217 finished: collect at /tmp/ipykernel_14509/2497880046.py:14, took 10.270441 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use mapPartitions transformation to find neighbors for each test row in parallel\n",
    "test_neighbors_rdd = test_data_rdd.mapPartitions(lambda iter: find_neighbors(iter, broadcast_train_data))\n",
    "\n",
    "# Function to predict classes based on neighbors\n",
    "def predict_class(neighbors):\n",
    "    classes = [neighbor[1] for neighbor in neighbors]\n",
    "    counter = Counter(classes)\n",
    "    most_common_class = counter.most_common(1)[0][0]\n",
    "    return most_common_class\n",
    "\n",
    "# Use map transformation to predict classes for each test row in parallel\n",
    "test_predictions_rdd = test_neighbors_rdd.map(predict_class)\n",
    "\n",
    "test_predictions = test_predictions_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fd83b",
   "metadata": {},
   "source": [
    "### Step 3 - Đánh giá độ chính xác của mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d62ea3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.949709 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: collect at /tmp/ipykernel_14509/411968121.py:2\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 218 (collect at /tmp/ipykernel_14509/411968121.py:2) with 2 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 432 (collect at /tmp/ipykernel_14509/411968121.py:2)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 432 (MapPartitionsRDD[1619] at javaToPython at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_261 stored as values in memory (estimated size 125.4 KiB, free 425.2 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_261_piece0 stored as bytes in memory (estimated size 41.2 KiB, free 425.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_261_piece0 in memory on 192.168.1.24:39111 (size: 41.2 KiB, free: 434.0 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 261 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 432 (MapPartitionsRDD[1619] at javaToPython at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 432.0 with 2 tasks resource profile 0\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 432.0 (TID 309) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 8370 bytes) \n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 432.0 (TID 310) (192.168.1.24, executor driver, partition 1, PROCESS_LOCAL, 8376 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 432.0 (TID 309)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 432.0 (TID 309)\n",
      "[Executor task launch worker for task 1.0 in stage 432.0 (TID 310)] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 432.0 (TID 310)\n",
      "[Executor task launch worker for task 0.0 in stage 432.0 (TID 309)] INFO org.apache.spark.storage.BlockManager - Found block rdd_35_0 locally\n",
      "[Executor task launch worker for task 1.0 in stage 432.0 (TID 310)] INFO org.apache.spark.storage.BlockManager - Found block rdd_23_0 locally\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_258_piece0 on 192.168.1.24:39111 in memory (size: 43.2 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_260_piece0 on 192.168.1.24:39111 in memory (size: 46.6 KiB, free: 434.1 MiB)\n",
      "[Executor task launch worker for task 1.0 in stage 432.0 (TID 310)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.281414 ms\n",
      "[Executor task launch worker for task 0.0 in stage 432.0 (TID 309)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 432.0 (TID 309). 4953 bytes result sent to driver\n",
      "[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 432.0 (TID 309) in 23 ms on 192.168.1.24 (executor driver) (1/2)\n",
      "[Executor task launch worker for task 1.0 in stage 432.0 (TID 310)] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 432.0 (TID 310). 6610 bytes result sent to driver\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 432.0 (TID 310) in 24 ms on 192.168.1.24 (executor driver) (2/2)\n",
      "[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 432.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 432 (collect at /tmp/ipykernel_14509/411968121.py:2) finished in 0.027 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 218 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 432: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 218 finished: collect at /tmp/ipykernel_14509/411968121.py:2, took 0.028330 s\n"
     ]
    }
   ],
   "source": [
    "# Lấy nhãn của tập test để tiến hành đánh giá độ chính xác\n",
    "test_labels = [row['is_good'] for row in test_data.select('is_good').rdd.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b25d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def accuracy_score(y_pred, y_test):\n",
    "    correct_predictions = len([pred for pred, true_label in zip(y_pred, y_test) if pred == true_label])\n",
    "    total_predictions = len(y_pred)\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffdff140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for KNN test is: 63.81578947368421\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print accuracy\n",
    "accuracy_test = accuracy_score(test_predictions, test_labels)\n",
    "print('The accuracy score for KNN test is:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fc648",
   "metadata": {},
   "source": [
    "### Step 4 - In ra kết quả dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32ef0807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2.92442 ms\n",
      "[Thread-3] INFO org.apache.spark.SparkContext - Starting job: showString at <unknown>:0\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 219 (showString at <unknown>:0) with 1 output partitions\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 433 (showString at <unknown>:0)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 433 (MapPartitionsRDD[1626] at showString at <unknown>:0), which has no missing parents\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_262 stored as values in memory (estimated size 12.7 KiB, free 425.5 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_262_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 425.5 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_262_piece0 in memory on 192.168.1.24:39111 (size: 6.6 KiB, free: 434.1 MiB)\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 262 from broadcast at DAGScheduler.scala:1585\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 433 (MapPartitionsRDD[1626] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 433.0 with 1 tasks resource profile 0\n",
      "[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 433.0 (TID 311) (192.168.1.24, executor driver, partition 0, PROCESS_LOCAL, 7771 bytes) \n",
      "[Executor task launch worker for task 0.0 in stage 433.0 (TID 311)] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 433.0 (TID 311)\n",
      "[Executor task launch worker for task 0.0 in stage 433.0 (TID 311)] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3.075504 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-------------+\n",
      "|KNN Model Predicted Labels with k=10|Actual Labels|\n",
      "+------------------------------------+-------------+\n",
      "|                                   0|            0|\n",
      "|                                   0|            1|\n",
      "|                                   0|            1|\n",
      "|                                   0|            0|\n",
      "|                                   0|            1|\n",
      "|                                   0|            0|\n",
      "|                                   0|            1|\n",
      "|                                   0|            0|\n",
      "|                                   0|            1|\n",
      "|                                   0|            1|\n",
      "|                                   0|            1|\n",
      "|                                   0|            1|\n",
      "|                                   0|            0|\n",
      "|                                   0|            0|\n",
      "|                                   0|            1|\n",
      "|                                   1|            0|\n",
      "|                                   1|            1|\n",
      "|                                   1|            0|\n",
      "|                                   1|            0|\n",
      "|                                   1|            0|\n",
      "+------------------------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Executor task launch worker for task 0.0 in stage 433.0 (TID 311)] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 433.0 (TID 311). 1864 bytes result sent to driver\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 433.0 (TID 311) in 95 ms on 192.168.1.24 (executor driver) (1/1)\n",
      "[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 433.0, whose tasks have all completed, from pool \n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 433 (showString at <unknown>:0) finished in 0.098 s\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 219 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 433: Stage finished\n",
      "[Thread-3] INFO org.apache.spark.scheduler.DAGScheduler - Job 219 finished: showString at <unknown>:0, took 0.099472 s\n",
      "[Thread-3] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.244619 ms\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_233_piece0 on 192.168.1.24:39111 in memory (size: 2.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_262_piece0 on 192.168.1.24:39111 in memory (size: 6.6 KiB, free: 434.1 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_261_piece0 on 192.168.1.24:39111 in memory (size: 41.2 KiB, free: 434.2 MiB)\n",
      "[dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_240_piece0 on 192.168.1.24:39111 in memory (size: 2.6 KiB, free: 434.2 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Tạo DataFrame từ list của tuples\n",
    "results_data = [(predicted_label, actual_label) for predicted_label, actual_label in zip(test_predictions, test_labels)]\n",
    "df_results = spark.createDataFrame(results_data, [\"KNN Model Predicted Labels with k=10\", \"Actual Labels\"])\n",
    "\n",
    "# Hiển thị kết quả\n",
    "df_results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
